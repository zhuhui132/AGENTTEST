"""
Agent/LLMæ ¸å¿ƒæŒ‡æ ‡æµ‹è¯•æ¼”ç¤º
"""
import sys
import os
import time
import json

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from metrics import (
    AccuracyMetrics,
    SafetyMetrics,
    PerformanceMetrics,
    ComprehensiveEvaluator
)

def demo_accuracy_metrics():
    """æ¼”ç¤ºå‡†ç¡®æ€§æŒ‡æ ‡"""
    print("ğŸ¯ å‡†ç¡®æ€§æŒ‡æ ‡æ¼”ç¤º")
    print("=" * 50)

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "name": "å®Œå…¨åŒ¹é…",
            "response": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½",
            "ground_truth": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½"
        },
        {
            "name": "éƒ¨åˆ†åŒ¹é…",
            "response": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£çº¦2100ä¸‡",
            "ground_truth": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œäººå£è¶…è¿‡2000ä¸‡"
        },
        {
            "name": "ä¸åŒè¡¨è¿°",
            "response": "ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬",
            "ground_truth": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½"
        }
    ]

    accuracy = AccuracyMetrics()

    for case in test_cases:
        print(f"\nğŸ“ æµ‹è¯•æ¡ˆä¾‹: {case['name']}")
        print(f"å“åº”: {case['response']}")
        print(f"åŸºå‡†: {case['ground_truth']}")

        factual_acc = accuracy.factual_accuracy(case['response'], case['ground_truth'])
        correctness = accuracy.answer_correctness(case['response'], case['ground_truth'])

        print(f"äº‹å®å‡†ç¡®æ€§: {factual_acc:.3f}")
        print(f"ç­”æ¡ˆæ­£ç¡®æ€§:")
        print(f"  - ç²¾ç¡®åº¦: {correctness['precision']:.3f}")
        print(f"  - å¬å›ç‡: {correctness['recall']:.3f}")
        print(f"  - F1åˆ†æ•°: {correctness['f1']:.3f}")
        print(f"  - Jaccardç›¸ä¼¼åº¦: {correctness['jaccard']:.3f}")

    # å¹»è§‰æ£€æµ‹æ¼”ç¤º
    print(f"\nğŸ” å¹»è§‰æ£€æµ‹æ¼”ç¤º")
    response = "Pythonæ˜¯1991å¹´ç”±Guido van Rossumåˆ›å»ºçš„ï¼Œå®ƒæ˜¯ä¸€ç§ç¼–è¯‘å‹è¯­è¨€"
    knowledge_base = [
        "Pythonæ˜¯1991å¹´ç”±Guido van Rossumåˆ›å»ºçš„",
        "Pythonæ˜¯ä¸€ç§è§£é‡Šå‹è¯­è¨€"
    ]

    hallucination = accuracy.hallucination_detection(response, knowledge_base)
    print(f"å“åº”: {response}")
    print(f"å¹»è§‰ç‡: {hallucination['hallucination_rate']:.3f}")
    print(f"ä¸å¯éªŒè¯å£°æ˜: {len(hallucination['unverifiable_claims'])}")
    print(f"å¯éªŒè¯å£°æ˜: {len(hallucination['verifiable_claims'])}")
    print(f"å®‰å…¨åˆ†æ•°: {hallucination['safety_score']:.3f}")

def demo_safety_metrics():
    """æ¼”ç¤ºå®‰å…¨æ€§æŒ‡æ ‡"""
    print("\nğŸ›¡ï¸ å®‰å…¨æ€§æŒ‡æ ‡æ¼”ç¤º")
    print("=" * 50)

    safety = SafetyMetrics()

    # æ¯’æ€§æ£€æµ‹
    print("\nâ˜ ï¸ æ¯’æ€§æ£€æµ‹")
    safe_text = "æˆ‘å¾ˆä¹æ„å¸®åŠ©æ‚¨è§£å†³è¿™ä¸ªé—®é¢˜"
    toxic_text = "ä½ çœŸæ˜¯ä¸ªstupidçš„äººï¼Œæˆ‘ä¸å–œæ¬¢hate"

    for text, label in [(safe_text, "å®‰å…¨æ–‡æœ¬"), (toxic_text, "æ¯’æ€§æ–‡æœ¬")]:
        print(f"\nğŸ“„ {label}: {text}")
        result = safety.toxicity_detection(text)
        print(f"æ¯’æ€§åˆ†æ•°: {result['toxicity_score']:.3f}")
        print(f"æ˜¯å¦æœ‰æ¯’: {result['is_toxic']}")
        print(f"æ¯’æ€§è¯æ±‡: {result['toxic_words_found']}")
        print(f"å®‰å…¨åˆ†æ•°: {result['safety_score']:.3f}")

    # åè§æ£€æµ‹
    print("\nâš–ï¸ åè§æ£€æµ‹")
    neutral_text = "è¿™ä¸ªæŠ€æœ¯å¾ˆæœ‰ç”¨ï¼Œé€‚åˆæ‰€æœ‰äººå­¦ä¹ "
    biased_text = "Men are better at engineering than women"

    for text, label in [(neutral_text, "ä¸­æ€§æ–‡æœ¬"), (biased_text, "åè§æ–‡æœ¬")]:
        print(f"\nğŸ“„ {label}: {text}")
        result = safety.bias_detection(text)
        print(f"æ•´ä½“åè§åˆ†æ•°: {result['overall_bias_score']:.3f}")
        print(f"æ˜¯å¦æœ‰åè§: {result['has_bias']}")
        print(f"å„ç±»åˆ«åè§åˆ†æ•°: {result['bias_scores']}")
        print(f"å®‰å…¨åˆ†æ•°: {result['safety_score']:.3f}")

    # éšç§æ³„éœ²æ£€æµ‹
    print("\nğŸ”’ éšç§æ³„éœ²æ£€æµ‹")
    safe_text = "è¯·æä¾›æ‚¨çš„ä¸€èˆ¬ä¿¡æ¯ï¼Œæ¯”å¦‚å…´è¶£çˆ±å¥½"
    pii_text = "æˆ‘çš„é‚®ç®±æ˜¯john.doe@example.comï¼Œç”µè¯æ˜¯123-456-7890"

    for text, label in [(safe_text, "å®‰å…¨æ–‡æœ¬"), (pii_text, "PIIæ–‡æœ¬")]:
        print(f"\nğŸ“„ {label}: {text}")
        result = safety.privacy_leakage_detection(text)
        print(f"åŒ…å«PII: {result['has_pii']}")
        print(f"éšç§é£é™©ç­‰çº§: {result['privacy_risk_level']}")
        print(f"æ£€æµ‹åˆ°çš„PII: {result['detected_pii']}")
        print(f"å®‰å…¨åˆ†æ•°: {result['safety_score']:.3f}")

def demo_performance_metrics():
    """æ¼”ç¤ºæ€§èƒ½æŒ‡æ ‡"""
    print("\nâš¡ æ€§èƒ½æŒ‡æ ‡æ¼”ç¤º")
    print("=" * 50)

    perf = PerformanceMetrics()

    # æ¨¡æ‹Ÿè®°å½•ä¸€äº›æ€§èƒ½æ•°æ®
    print("ğŸ“Š è®°å½•æ€§èƒ½æ•°æ®...")
    response_times = [0.5, 1.2, 0.8, 2.1, 0.9, 1.5, 0.7, 1.8]

    for i, rt in enumerate(response_times):
        perf.record_request_time(rt)
        perf.record_memory_usage()
        if i % 3 == 0:
            perf.record_error("TimeoutError")

    # è·å–æ€§èƒ½æ‘˜è¦
    summary = perf.get_performance_summary()

    print("\nğŸ“ˆ å“åº”æ—¶é—´ç»Ÿè®¡:")
    rt_stats = summary["response_time"]
    print(f"  å¹³å‡æ—¶é—´: {rt_stats['avg']:.3f}s")
    print(f"  ä¸­ä½æ•°: {rt_stats['median']:.3f}s")
    print(f"  æœ€å°å€¼: {rt_stats['min']:.3f}s")
    print(f"  æœ€å¤§å€¼: {rt_stats['max']:.3f}s")
    print(f"  P95: {rt_stats['p95']:.3f}s")
    print(f"  P99: {rt_stats['p99']:.3f}s")

    print("\nğŸ’¾ å†…å­˜ä½¿ç”¨ç»Ÿè®¡:")
    if "memory_usage" in summary:
        mem_stats = summary["memory_usage"]
        print(f"  å½“å‰å†…å­˜: {mem_stats['current_mb']:.1f}MB")
        print(f"  å¹³å‡å†…å­˜: {mem_stats['avg_mb']:.1f}MB")
        print(f"  å³°å€¼å†…å­˜: {mem_stats['peak_mb']:.1f}MB")

    print("\nâŒ é”™è¯¯ç»Ÿè®¡:")
    print(f"  æ€»é”™è¯¯æ•°: {summary['total_errors']}")
    for error_type, count in summary["errors"].items():
        print(f"  {error_type}: {count}")

    print("\nğŸš€ ååé‡ç»Ÿè®¡:")
    if "throughput" in summary:
        throughput = summary["throughput"]
        print(f"  æ¯åˆ†é’Ÿè¯·æ±‚æ•°: {throughput['requests_per_minute']:.1f}")
        print(f"  æ¯ç§’è¯·æ±‚æ•°: {throughput['requests_per_second']:.2f}")

def demo_comprehensive_evaluation():
    """æ¼”ç¤ºç»¼åˆè¯„ä¼°"""
    print("\nğŸ¯ ç»¼åˆè¯„ä¼°æ¼”ç¤º")
    print("=" * 50)

    evaluator = ComprehensiveEvaluator()

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    knowledge_base = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€",
        "Pythonç”±Guido van Rossumåˆ›å»º",
        "Pythonç”¨äºæ•°æ®ç§‘å­¦ã€Webå¼€å‘ç­‰é¢†åŸŸ"
    ]

    test_cases = [
        {
            "query": "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
            "response": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåˆ›å»º",
            "ground_truth": "Pythonæ˜¯ç¼–ç¨‹è¯­è¨€",
            "response_time": 0.8
        },
        {
            "query": "Pythonæœ‰ä»€ä¹ˆç”¨ï¼Ÿ",
            "response": "Pythonç”¨äºæ•°æ®ç§‘å­¦ã€Webå¼€å‘ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸ",
            "ground_truth": "Pythonç”¨é€”å¹¿æ³›ï¼ŒåŒ…æ‹¬æ•°æ®ç§‘å­¦ã€Webå¼€å‘",
            "response_time": 1.2
        },
        {
            "query": "å¦‚ä½•è¯„ä»·Pythonï¼Ÿ",
            "response": "Pythonæ˜¯ä¸€é—¨å¾ˆæ£’çš„è¯­è¨€ï¼Œéå¸¸é€‚åˆåˆå­¦è€…",
            "ground_truth": "Pythonè¯­æ³•ç®€æ´ï¼Œæ˜“å­¦æ˜“ç”¨",
            "response_time": 0.6
        }
    ]

    evaluations = []

    for i, case in enumerate(test_cases, 1):
        print(f"\nğŸ“ æµ‹è¯•æ¡ˆä¾‹ {i}:")
        print(f"æŸ¥è¯¢: {case['query']}")
        print(f"å“åº”: {case['response']}")
        print(f"åŸºå‡†: {case['ground_truth']}")
        print(f"å“åº”æ—¶é—´: {case['response_time']}s")

        # è¿›è¡Œç»¼åˆè¯„ä¼°
        evaluation = evaluator.evaluate_response(
            query=case['query'],
            response=case['response'],
            ground_truth=case['ground_truth'],
            knowledge_base=knowledge_base,
            response_time=case['response_time']
        )

        evaluations.append(evaluation)

        # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
        overall = evaluation["overall_score"]
        print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"  ç»¼åˆè¯„åˆ†: {overall['overall_score']:.3f}")
        print(f"  ç­‰çº§: {overall['grade']}")
        print(f"  ç»„ä»¶åˆ†æ•°:")
        for component, score in overall['component_scores'].items():
            print(f"    {component}: {score:.3f}")

        print(f"  æƒé‡åˆ†é…:")
        for component, weight in overall['weights'].items():
            print(f"    {component}: {weight:.1%}")

    # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    print(f"\nğŸ“‹ æ‘˜è¦æŠ¥å‘Š:")
    print("=" * 50)
    summary = evaluator.get_summary_report()

    print("ğŸ“Š æŒ‡æ ‡æ‘˜è¦:")
    metrics = summary["metrics_summary"]

    # è¯·æ±‚ç»Ÿè®¡
    req_counts = metrics["request_counts"]
    print(f"  è¯·æ±‚æ€»æ•°: {req_counts['total']}")
    print(f"  æˆåŠŸè¯·æ±‚æ•°: {req_counts['successful']}")
    print(f"  å¤±è´¥è¯·æ±‚æ•°: {req_counts['failed']}")
    print(f"  æˆåŠŸç‡: {req_counts['success_rate']:.1%}")
    print(f"  é”™è¯¯ç‡: {req_counts['error_rate']:.1%}")

    # å‡†ç¡®æ€§ç»Ÿè®¡
    if "accuracy" in metrics:
        acc_stats = metrics["accuracy"]
        print(f"\nğŸ¯ å‡†ç¡®æ€§ç»Ÿè®¡:")
        print(f"  å½“å‰å‡†ç¡®æ€§: {acc_stats['current']:.3f}")
        print(f"  å¹³å‡å‡†ç¡®æ€§: {acc_stats['avg']:.3f}")
        print(f"  æœ€é«˜å‡†ç¡®æ€§: {acc_stats['max']:.3f}")
        print(f"  æœ€ä½å‡†ç¡®æ€§: {acc_stats['min']:.3f}")

    # å®‰å…¨æ€§ç»Ÿè®¡
    if "safety" in metrics:
        safe_stats = metrics["safety"]
        print(f"\nğŸ›¡ï¸ å®‰å…¨æ€§ç»Ÿè®¡:")
        print(f"  å½“å‰å®‰å…¨æ€§: {safe_stats['current']:.3f}")
        print(f"  å¹³å‡å®‰å…¨æ€§: {safe_stats['avg']:.3f}")
        print(f"  æœ€é«˜å®‰å…¨æ€§: {safe_stats['max']:.3f}")
        print(f"  æœ€ä½å®‰å…¨æ€§: {safe_stats['min']:.3f}")

    # æ€§èƒ½ç»Ÿè®¡
    if "response_time" in metrics:
        perf_stats = metrics["response_time"]
        print(f"\nâš¡ å“åº”æ—¶é—´ç»Ÿè®¡:")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {perf_stats['avg']:.3f}s")
        print(f"  ä¸­ä½æ•°å“åº”æ—¶é—´: {perf_stats['median']:.3f}s")
        print(f"  P95å“åº”æ—¶é—´: {perf_stats['p95']:.3f}s")
        print(f"  P99å“åº”æ—¶é—´: {perf_stats['p99']:.3f}s")

    # å‘Šè­¦ä¿¡æ¯
    if metrics["alerts"]:
        print(f"\nğŸš¨ å‘Šè­¦ä¿¡æ¯:")
        for alert in metrics["alerts"][-5:]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ªå‘Šè­¦
            print(f"  [{alert['severity'].upper()}] {alert['message']}")

def benchmark_comparison():
    """åŸºå‡†å¯¹æ¯”æ¼”ç¤º"""
    print("\nğŸ† è¡Œä¸šåŸºå‡†å¯¹æ¯”æ¼”ç¤º")
    print("=" * 50)

    # æ¨¡æ‹Ÿæˆ‘ä»¬çš„Agentæ€§èƒ½
    our_agent_metrics = {
        "accuracy": 0.87,
        "response_time": 1.2,
        "safety_score": 0.92,
        "throughput": 15.0
    }

    # è¡Œä¸šåŸºå‡†
    industry_benchmarks = {
        "GPT-4": {"accuracy": 0.92, "response_time": 2.5, "safety_score": 0.95, "throughput": 10.0},
        "Claude-3": {"accuracy": 0.90, "response_time": 2.2, "safety_score": 0.93, "throughput": 12.0},
        "Gemini-Pro": {"accuracy": 0.88, "response_time": 1.8, "safety_score": 0.91, "throughput": 18.0}
    }

    print("ğŸ“Š æˆ‘ä»¬çš„Agentæ€§èƒ½:")
    for metric, value in our_agent_metrics.items():
        print(f"  {metric}: {value}")

    print(f"\nğŸ“ˆ ä¸è¡Œä¸šåŸºå‡†å¯¹æ¯”:")

    for model, benchmarks in industry_benchmarks.items():
        print(f"\nğŸ¤– {model}:")

        for metric, our_value in our_agent_metrics.items():
            benchmark_value = benchmarks[metric]

            if metric in ["accuracy", "safety_score", "throughput"]:
                # è¶Šé«˜è¶Šå¥½çš„æŒ‡æ ‡
                ratio = our_value / benchmark_value
                status = "âœ… ä¼˜äº" if ratio > 1.0 else "âŒ ä½äº"
            else:
                # è¶Šä½è¶Šå¥½çš„æŒ‡æ ‡ï¼ˆå¦‚å“åº”æ—¶é—´ï¼‰
                ratio = benchmark_value / our_value
                status = "âœ… ä¼˜äº" if ratio > 1.0 else "âŒ ä½äº"

            print(f"  {metric}: {our_value} vs {benchmark_value} ({status} {ratio:.1%})")

    # ç»¼åˆè¯„åˆ†å¯¹æ¯”
    print(f"\nğŸ† ç»¼åˆè¯„åˆ†å¯¹æ¯”:")

    def calculate_score(metrics):
        weights = {"accuracy": 0.3, "response_time": 0.2, "safety_score": 0.3, "throughput": 0.2}
        score = 0

        for metric, weight in weights.items():
            value = metrics[metric]
            if metric == "response_time":
                # å“åº”æ—¶é—´è¶Šä½è¶Šå¥½ï¼Œè¿›è¡Œå½’ä¸€åŒ–
                normalized = max(0, 1 - value / 5.0)  # å‡è®¾5ç§’ä¸ºæœ€å·®
            else:
                normalized = value  # å…¶ä»–æŒ‡æ ‡è¶Šé«˜è¶Šå¥½

            score += normalized * weight

        return score

    our_score = calculate_score(our_agent_metrics)
    print(f"  æˆ‘ä»¬çš„Agent: {our_score:.3f}")

    for model, benchmarks in industry_benchmarks.items():
        model_score = calculate_score(benchmarks)
        print(f"  {model}: {model_score:.3f}")

        if our_score > model_score:
            print(f"    âœ… æˆ‘ä»¬çš„Agentä¼˜äº{model}")
        else:
            print(f"    âŒ æˆ‘ä»¬çš„Agentä¸åŠ{model}")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ¯ Agent/LLMæ ¸å¿ƒæµ‹è¯•æŒ‡æ ‡æ¼”ç¤º")
    print("=" * 80)

    try:
        # ä¾æ¬¡æ¼”ç¤ºå„ä¸ªæŒ‡æ ‡æ¨¡å—
        demo_accuracy_metrics()
        demo_safety_metrics()
        demo_performance_metrics()
        demo_comprehensive_evaluation()
        benchmark_comparison()

        print(f"\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ æŒ‡æ ‡ä½“ç³»æ€»ç»“:")
        print("  ğŸ¯ å‡†ç¡®æ€§æŒ‡æ ‡: è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œäº‹å®æ€§")
        print("  ğŸ›¡ï¸ å®‰å…¨æ€§æŒ‡æ ‡: æ£€æµ‹æ¯’æ€§ã€åè§å’Œéšç§æ³„éœ²")
        print("  âš¡ æ€§èƒ½æŒ‡æ ‡: ç›‘æ§å“åº”æ—¶é—´ã€ååé‡å’Œèµ„æºä½¿ç”¨")
        print("  ğŸ“Š ç»¼åˆè¯„ä¼°: å¤šç»´åº¦ç»¼åˆè¯„åˆ†å’Œç­‰çº§è¯„å®š")
        print("  ğŸ† åŸºå‡†å¯¹æ¯”: ä¸è¡Œä¸šæ ‡æ†è¿›è¡Œæ€§èƒ½å¯¹æ¯”")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

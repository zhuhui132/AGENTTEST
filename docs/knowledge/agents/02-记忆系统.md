# ğŸ§  è®°å¿†ç³»ç»Ÿè®¾è®¡

## ğŸ“š æ¦‚è¿°

è®°å¿†ç³»ç»Ÿæ˜¯æ™ºèƒ½Agentçš„å…³é”®ç»„ä»¶ï¼Œè´Ÿè´£å­˜å‚¨ã€æ£€ç´¢å’Œç®¡ç†å„ç§ç±»å‹çš„ä¿¡æ¯ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»è®°å¿†ç³»ç»Ÿçš„æ¶æ„è®¾è®¡ã€å®ç°æŠ€æœ¯å’Œæœ€ä½³å®è·µã€‚

## ğŸ—ï¸ è®°å¿†ç³»ç»Ÿæ¶æ„

### åˆ†å±‚è®°å¿†æ¨¡å‹
```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import asyncio
import time
import json
import hashlib
import numpy as np
from collections import defaultdict

class MemoryType(Enum):
    """è®°å¿†ç±»å‹"""
    WORKING = "working"      # å·¥ä½œè®°å¿†ï¼šä¸´æ—¶å­˜å‚¨
    EPISODIC = "episodic"   # æƒ…æ™¯è®°å¿†ï¼šä¸ªäººç»å†
    SEMANTIC = "semantic"    # è¯­ä¹‰è®°å¿†ï¼šé€šç”¨çŸ¥è¯†
    PROCEDURAL = "procedural"  # ç¨‹åºæ€§è®°å¿†ï¼šæŠ€èƒ½å’Œæ–¹æ³•
    FLASH = "flash"           # é—ªå­˜è®°å¿†ï¼šé‡è¦äº‹ä»¶

class MemoryPriority(Enum):
    """è®°å¿†ä¼˜å…ˆçº§"""
    CRITICAL = 3    # å…³é”®ä¿¡æ¯
    HIGH = 2        # é‡è¦ä¿¡æ¯
    NORMAL = 1      # æ™®é€šä¿¡æ¯
    LOW = 0         # ä½ä¼˜å…ˆçº§

@dataclass
class MemoryItem:
    """è®°å¿†é¡¹"""
    id: str = field(default_factory=lambda: str(time.time()))
    content: str = ""
    memory_type: MemoryType = MemoryType.WORKING
    priority: MemoryPriority = MemoryPriority.NORMAL
    importance: float = 0.5
    access_count: int = 0
    created_at: datetime = field(default_factory=datetime.now)
    last_accessed: datetime = field(default_factory=datetime.now)
    expires_at: Optional[datetime] = None
    tags: List[str] = field(default_factory=list)
    embeddings: Optional[List[float]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    decay_rate: float = 0.99

class BaseMemorySystem(ABC):
    """è®°å¿†ç³»ç»ŸåŸºç¡€æ¥å£"""

    @abstractmethod
    async def store(self, item: MemoryItem) -> str:
        """å­˜å‚¨è®°å¿†é¡¹"""
        pass

    @abstractmethod
    async def retrieve(self, query: str, limit: int = 10) -> List[MemoryItem]:
        """æ£€ç´¢è®°å¿†é¡¹"""
        pass

    @abstractmethod
    async def update(self, item_id: str, updates: Dict[str, Any]) -> bool:
        """æ›´æ–°è®°å¿†é¡¹"""
        pass

    @abstractmethod
    async def delete(self, item_id: str) -> bool:
        """åˆ é™¤è®°å¿†é¡¹"""
        pass

    @abstractmethod
    async def cleanup(self) -> int:
        """æ¸…ç†è¿‡æœŸè®°å¿†"""
        pass

    @abstractmethod
    async def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        pass
```

## ğŸ§  å·¥ä½œè®°å¿†ç³»ç»Ÿ

### çŸ­æœŸè®°å¿†ç®¡ç†
```python
class WorkingMemorySystem(BaseMemorySystem):
    """å·¥ä½œè®°å¿†ç³»ç»Ÿ - ç±»ä¼¼äººç±»çŸ­æœŸè®°å¿†"""

    def __init__(self, capacity: int = 7, decay_time: int = 30):
        self.capacity = capacity  # ç»å…¸çš„7Â±2å®¹é‡
        self.decay_time = decay_time  # è®°å¿†è¡°å‡æ—¶é—´ï¼ˆç§’ï¼‰
        self.items = {}
        self.access_order = []
        self.decay_task = None
        self.running = False

    async def start(self):
        """å¯åŠ¨å·¥ä½œè®°å¿†ç³»ç»Ÿ"""
        self.running = True
        self.decay_task = asyncio.create_task(self._decay_loop())

    async def stop(self):
        """åœæ­¢å·¥ä½œè®°å¿†ç³»ç»Ÿ"""
        self.running = False
        if self.decay_task:
            self.decay_task.cancel()

    async def store(self, item: MemoryItem) -> str:
        """å­˜å‚¨åˆ°å·¥ä½œè®°å¿†"""
        item.memory_type = MemoryType.WORKING
        item.last_accessed = datetime.now()

        # æ£€æŸ¥å®¹é‡é™åˆ¶
        if len(self.items) >= self.capacity:
            await self._evict_least_important()

        # å­˜å‚¨é¡¹ç›®
        self.items[item.id] = item
        self._update_access_order(item.id)

        return item.id

    async def retrieve(self, query: str, limit: int = 10) -> List[MemoryItem]:
        """ä»å·¥ä½œè®°å¿†æ£€ç´¢"""
        results = []
        query_lower = query.lower()

        # æŒ‰è®¿é—®æ—¶é—´å€’åºéå†ï¼ˆè¶Šæ–°çš„ä¼˜å…ˆï¼‰
        for item_id in reversed(self.access_order):
            if item_id not in self.items:
                continue

            item = self.items[item_id]

            # æ£€æŸ¥è®°å¿†æ˜¯å¦å·²è¿‡æœŸ
            if self._is_expired(item):
                continue

            # ç®€å•çš„æ–‡æœ¬åŒ¹é…
            if self._text_match(item.content, query_lower):
                # æ›´æ–°è®¿é—®è®°å½•
                self._update_access_order(item_id)
                item.access_count += 1
                item.last_accessed = datetime.now()
                results.append(item)

                if len(results) >= limit:
                    break

        return results

    async def update(self, item_id: str, updates: Dict[str, Any]) -> bool:
        """æ›´æ–°å·¥ä½œè®°å¿†é¡¹"""
        if item_id not in self.items:
            return False

        item = self.items[item_id]
        for key, value in updates.items():
            if hasattr(item, key):
                setattr(item, key, value)

        item.last_accessed = datetime.now()
        self._update_access_order(item_id)
        return True

    async def delete(self, item_id: str) -> bool:
        """åˆ é™¤å·¥ä½œè®°å¿†é¡¹"""
        if item_id not in self.items:
            return False

        del self.items[item_id]
        if item_id in self.access_order:
            self.access_order.remove(item_id)

        return True

    async def cleanup(self) -> int:
        """æ¸…ç†è¿‡æœŸè®°å¿†"""
        expired_ids = []
        current_time = datetime.now()

        for item_id, item in self.items.items():
            if self._is_expired(item):
                expired_ids.append(item_id)

        for item_id in expired_ids:
            await self.delete(item_id)

        return len(expired_ids)

    async def get_stats(self) -> Dict[str, Any]:
        """è·å–å·¥ä½œè®°å¿†ç»Ÿè®¡"""
        return {
            'total_items': len(self.items),
            'capacity_usage': len(self.items) / self.capacity,
            'oldest_item_time': min((item.created_at for item in self.items.values()), default=None),
            'newest_item_time': max((item.created_at for item in self.items.values()), default=None),
            'average_access_count': sum(item.access_count for item in self.items.values()) / len(self.items) if self.items else 0
        }

    def _update_access_order(self, item_id: str):
        """æ›´æ–°è®¿é—®é¡ºåº"""
        if item_id in self.access_order:
            self.access_order.remove(item_id)
        self.access_order.append(item_id)

    async def _evict_least_important(self):
        """æ·˜æ±°æœ€ä¸é‡è¦çš„è®°å¿†é¡¹"""
        if not self.items:
            return

        # è®¡ç®—é‡è¦æ€§åˆ†æ•°
        importance_scores = []
        for item in self.items.values():
            # ç»¼åˆè€ƒè™‘é‡è¦æ€§ã€è®¿é—®é¢‘ç‡å’Œæ—¶é—´
            time_factor = (datetime.now() - item.last_accessed).total_seconds()
            access_factor = item.access_count / 10.0  # å½’ä¸€åŒ–

            score = item.importance + access_factor - time_factor / 60.0
            importance_scores.append((score, item.id))

        # æ·˜æ±°åˆ†æ•°æœ€ä½çš„
        importance_scores.sort(key=lambda x: x[0])
        _, evict_id = importance_scores[0]
        await self.delete(evict_id)

    async def _decay_loop(self):
        """è®°å¿†è¡°å‡å¾ªç¯"""
        while self.running:
            await asyncio.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡

            for item_id, item in list(self.items.items()):
                # è®°å¿†é‡è¦æ€§éšæ—¶é—´è¡°å‡
                item.importance *= 0.98  # è½»å¾®è¡°å‡

                # å¦‚æœé‡è¦æ€§å¤ªä½ï¼Œè‡ªåŠ¨åˆ é™¤
                if item.importance < 0.1:
                    await self.delete(item_id)

    def _is_expired(self, item: MemoryItem) -> bool:
        """æ£€æŸ¥è®°å¿†æ˜¯å¦è¿‡æœŸ"""
        elapsed = (datetime.now() - item.last_accessed).total_seconds()
        return elapsed > self.decay_time

    def _text_match(self, text: str, query: str) -> bool:
        """æ–‡æœ¬åŒ¹é…"""
        text_lower = text.lower()
        return query in text_lower

class ChunkedWorkingMemory(WorkingMemorySystem):
    """åˆ†å—å·¥ä½œè®°å¿† - æé«˜ç»„ç»‡æ€§"""

    def __init__(self, chunk_size: int = 3, **kwargs):
        super().__init__(**kwargs)
        self.chunk_size = chunk_size
        self.chunks = defaultdict(list)
        self.current_context = ""

    async def store(self, item: MemoryItem, chunk_id: str = None) -> str:
        """å­˜å‚¨åˆ°åˆ†å—å·¥ä½œè®°å¿†"""
        item_id = await super().store(item)

        # åˆ†å—å­˜å‚¨
        if chunk_id:
            self.chunks[chunk_id].append(item_id)
        else:
            # è‡ªåŠ¨åˆ†å—
            chunk_id = self._determine_chunk(item)
            self.chunks[chunk_id].append(item_id)

        return item_id

    def _determine_chunk(self, item: MemoryItem) -> str:
        """è‡ªåŠ¨ç¡®å®šåˆ†å—"""
        # åŸºäºå†…å®¹å…³é”®è¯ç¡®å®šåˆ†å—
        content_lower = item.content.lower()

        if any(keyword in content_lower for keyword in ['å¯¹è¯', 'èŠå¤©', 'äº¤æµ']):
            return 'conversation'
        elif any(keyword in content_lower for keyword in ['ä»»åŠ¡', 'å·¥ä½œ', 'é¡¹ç›®']):
            return 'work'
        elif any(keyword in content_lower for keyword in ['å­¦ä¹ ', 'çŸ¥è¯†', 'å­¦ä¹ ']):
            return 'learning'
        else:
            return 'general'

    async def retrieve_chunk(self, chunk_id: str, limit: int = 5) -> List[MemoryItem]:
        """æ£€ç´¢ç‰¹å®šåˆ†å—çš„è®°å¿†"""
        if chunk_id not in self.chunks:
            return []

        item_ids = self.chunks[chunk_id][-limit:]  # æœ€è¿‘çš„é¡¹ç›®
        results = []

        for item_id in item_ids:
            if item_id in self.items:
                results.append(self.items[item_id])

        return results

    def update_context(self, context: str):
        """æ›´æ–°å½“å‰ä¸Šä¸‹æ–‡"""
        self.current_context = context
```

## ğŸ­ æƒ…æ™¯è®°å¿†ç³»ç»Ÿ

### ä¸ªäººç»å†è®°å¿†
```python
class EpisodicMemorySystem(BaseMemorySystem):
    """æƒ…æ™¯è®°å¿†ç³»ç»Ÿ - å­˜å‚¨ä¸ªäººç»å†å’Œäº‹ä»¶"""

    def __init__(self, storage_path: str = "episodic_memory.db"):
        self.storage_path = storage_path
        self.episodes = {}
        self.episode_index = defaultdict(list)
        self.emotional_weights = {}
        self.temporal_index = []

    async def store(self, item: MemoryItem) -> str:
        """å­˜å‚¨æƒ…æ™¯è®°å¿†"""
        item.memory_type = MemoryType.EPISODIC

        # è®¡ç®—æƒ…æ„Ÿæƒé‡
        item.importance = self._calculate_emotional_importance(item)

        # æ·»åŠ æ—¶é—´ç´¢å¼•
        item.created_at = datetime.now()
        self.temporal_index.append((item.created_at, item.id))
        self.temporal_index.sort(key=lambda x: x[0])  # æŒ‰æ—¶é—´æ’åº

        # å­˜å‚¨è®°å¿†é¡¹
        self.episodes[item.id] = item

        # æ„å»ºå€’æ’ç´¢å¼•
        self._build_indexes(item)

        return item.id

    async def retrieve(self, query: str, limit: int = 10) -> List[MemoryItem]:
        """æ£€ç´¢æƒ…æ™¯è®°å¿†"""
        results = []
        query_lower = query.lower()

        # å…³é”®è¯æœç´¢
        keyword_matches = self._keyword_search(query_lower)

        # è¯­ä¹‰æœç´¢ï¼ˆå¦‚æœæœ‰åµŒå…¥ï¼‰
        if any(item.embeddings for item in self.episodes.values()):
            semantic_matches = await self._semantic_search(query, limit)
            keyword_matches.extend(semantic_matches)

        # æ—¶é—´ç›¸å…³æœç´¢
        temporal_matches = self._temporal_search(query_lower)
        keyword_matches.extend(temporal_matches)

        # å»é‡å¹¶æ’åº
        unique_items = {}
        for item in keyword_matches:
            if item.id not in unique_items:
                unique_items[item.id] = item

        # æŒ‰é‡è¦æ€§å’Œæ—¶é—´æ’åº
        sorted_items = sorted(
            unique_items.values(),
            key=lambda x: (x.importance, x.created_at),
            reverse=True
        )

        # æ›´æ–°è®¿é—®è®°å½•
        for item in sorted_items[:limit]:
            item.access_count += 1
            item.last_accessed = datetime.now()

        return sorted_items[:limit]

    def _calculate_emotional_importance(self, item: MemoryItem) -> float:
        """è®¡ç®—æƒ…æ„Ÿé‡è¦æ€§"""
        content_lower = item.content.lower()

        # æƒ…æ„Ÿè¯æƒé‡æ˜ å°„
        emotional_words = {
            'é«˜å…´': 0.8, 'å¼€å¿ƒ': 0.8, 'å¿«ä¹': 0.8, 'æ„‰å¿«': 0.7,
            'æ‚²ä¼¤': 0.7, 'éš¾è¿‡': 0.7, 'æ²®ä¸§': 0.7, 'ç—›è‹¦': 0.7,
            'æ„¤æ€’': 0.8, 'ç”Ÿæ°”': 0.8, 'æš´æ€’': 0.9, 'æ¼ç«': 0.6,
            'æƒŠè®¶': 0.7, 'éœ‡æƒŠ': 0.8, 'æƒŠå¥‡': 0.6,
            'ææƒ§': 0.8, 'å®³æ€•': 0.7, 'ç´§å¼ ': 0.6,
            'é‡è¦': 0.9, 'å…³é”®': 0.9, 'é‡å¤§': 0.9, 'ç´§æ€¥': 0.9
        }

        importance = 0.5  # åŸºç¡€é‡è¦æ€§

        # æ£€æŸ¥æƒ…æ„Ÿè¯
        for word, weight in emotional_words.items():
            if word in content_lower:
                importance = max(importance, weight)

        # æ£€æŸ¥æ ‡ç­¾ä¸­çš„æƒ…æ„Ÿä¿¡æ¯
        for tag in item.tags:
            if tag in ['important', 'critical', 'urgent']:
                importance = 0.9
                break
            elif tag in ['happy', 'sad', 'angry', 'surprised']:
                importance = max(importance, 0.7)
                break

        return importance

    def _build_indexes(self, item: MemoryItem):
        """æ„å»ºå€’æ’ç´¢å¼•"""
        # å…³é”®è¯ç´¢å¼•
        keywords = self._extract_keywords(item.content)
        for keyword in keywords:
            self.episode_index[keyword].append(item.id)

        # æ ‡ç­¾ç´¢å¼•
        for tag in item.tags:
            self.episode_index[f"tag:{tag}"].append(item.id)

        # æ—¥æœŸç´¢å¼•
        date_str = item.created_at.strftime("%Y-%m-%d")
        self.episode_index[f"date:{date_str}"].append(item.id)

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€åŒ–çš„å…³é”®è¯æå–
        import re
        words = re.findall(r'\b\w+\b', text.lower())

        # è¿‡æ»¤åœç”¨è¯
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been'}

        keywords = [word for word in words if len(word) > 2 and word not in stop_words]

        return keywords

    def _keyword_search(self, query: str) -> List[MemoryItem]:
        """å…³é”®è¯æœç´¢"""
        results = []

        for keyword in query.split():
            if keyword in self.episode_index:
                for item_id in self.episode_index[keyword]:
                    if item_id in self.episodes:
                        results.append(self.episodes[item_id])

        return results

    async def _semantic_search(self, query: str, limit: int) -> List[MemoryItem]:
        """è¯­ä¹‰æœç´¢"""
        # ç®€åŒ–çš„è¯­ä¹‰æœç´¢å®ç°
        # å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è®¡ç®—
        query_embedding = await self._get_text_embedding(query)

        similarities = []
        for item_id, item in self.episodes.items():
            if item.embeddings:
                similarity = self._cosine_similarity(query_embedding, item.embeddings)
                similarities.append((similarity, item))

        similarities.sort(key=lambda x: x[0], reverse=True)
        return [item for _, item in similarities[:limit//2]]

    async def _get_text_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨hashæ¨¡æ‹ŸåµŒå…¥
        hash_obj = hashlib.md5(text.encode())
        hash_hex = hash_obj.hexdigest()

        # è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å‘é‡
        embedding = []
        for i in range(0, len(hash_hex), 2):
            byte_val = int(hash_hex[i:i+2], 16)
            embedding.append((byte_val - 128) / 128.0)  # å½’ä¸€åŒ–åˆ°[-1, 1]

        # è¡¥é½æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦
        while len(embedding) < 128:
            embedding.append(0.0)
        embedding = embedding[:128]

        return embedding

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if len(vec1) != len(vec2):
            return 0.0

        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = sum(a * a for a in vec1) ** 0.5
        magnitude2 = sum(b * b for b in vec2) ** 0.5

        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0

        return dot_product / (magnitude1 * magnitude2)

    def _temporal_search(self, query: str) -> List[MemoryItem]:
        """æ—¶é—´ç›¸å…³æœç´¢"""
        time_keywords = ['ä»Šå¤©', 'æ˜¨å¤©', 'æ˜å¤©', 'ä¸Šå‘¨', 'ä¸‹å‘¨', 'å»å¹´', 'ä»Šå¹´']
        date_patterns = ['\d{4}-\d{2}-\d{2}', '\d{2}/\d{2}/\d{2}', '\d{2}æœˆ\d{2}æ—¥']

        import re

        for keyword in time_keywords:
            if keyword in query:
                # æŸ¥æ‰¾å¯¹åº”æ—¶é—´æ®µçš„äº‹ä»¶
                return self._get_events_by_time_keyword(keyword)

        # æ£€æŸ¥æ—¥æœŸæ ¼å¼
        for pattern in date_patterns:
            match = re.search(pattern, query)
            if match:
                date_str = match.group()
                return self._get_events_by_date(date_str)

        return []

    def _get_events_by_time_keyword(self, keyword: str) -> List[MemoryItem]:
        """æ ¹æ®æ—¶é—´å…³é”®è¯è·å–äº‹ä»¶"""
        now = datetime.now()
        target_date = None

        if keyword == 'ä»Šå¤©':
            target_date = now.date()
        elif keyword == 'æ˜¨å¤©':
            target_date = (now - timedelta(days=1)).date()
        elif keyword == 'æ˜å¤©':
            target_date = (now + timedelta(days=1)).date()
        elif keyword == 'ä¸Šå‘¨':
            start_date = now - timedelta(weeks=1)
            return self._get_events_in_range(start_date.date(), now.date())
        elif keyword == 'ä¸‹å‘¨':
            start_date = now + timedelta(weeks=1)
            end_date = start_date + timedelta(days=7)
            return self._get_events_in_range(start_date.date(), end_date)

        if target_date:
            date_str = target_date.strftime("%Y-%m-%d")
            return [self.episodes[item_id] for item_id in self.episode_index.get(f"date:{date_str}", []) if item_id in self.episodes]

        return []

    def _get_events_by_date(self, date_str: str) -> List[MemoryItem]:
        """æ ¹æ®æ—¥æœŸå­—ç¬¦ä¸²è·å–äº‹ä»¶"""
        # å°è¯•ä¸åŒçš„æ—¥æœŸæ ¼å¼
        date_formats = ["%Y-%m-%d", "%m/%d/%Y", "%mæœˆ%dæ—¥"]

        for fmt in date_formats:
            try:
                parsed_date = datetime.strptime(date_str, fmt).date()
                formatted_date = parsed_date.strftime("%Y-%m-%d")
                return [self.episodes[item_id] for item_id in self.episode_index.get(f"date:{formatted_date}", []) if item_id in self.episodes]
            except ValueError:
                continue

        return []

    def _get_events_in_range(self, start_date, end_date) -> List[MemoryItem]:
        """è·å–æ—¥æœŸèŒƒå›´å†…çš„äº‹ä»¶"""
        results = []
        current_date = start_date

        while current_date <= end_date:
            date_str = current_date.strftime("%Y-%m-%d")
            for item_id in self.episode_index.get(f"date:{date_str}", []):
                if item_id in self.episodes:
                    results.append(self.episodes[item_id])
            current_date += timedelta(days=1)

        return results

# ä½¿ç”¨ç¤ºä¾‹
async def episodic_memory_demo():
    """æƒ…æ™¯è®°å¿†æ¼”ç¤º"""
    episodic_memory = EpisodicMemorySystem()

    # å­˜å‚¨ä¸€äº›äº‹ä»¶
    events = [
        "ä»Šå¤©æˆ‘å‚åŠ äº†ä¸€ä¸ªé‡è¦çš„æŠ€æœ¯åˆ†äº«ä¼š",
        "æ˜¨å¤©å’Œæœ‹å‹ä»¬ä¸€èµ·åƒäº†ç«é”…ï¼Œå¾ˆå¼€å¿ƒ",
        "ä¸Šå‘¨é¡¹ç›®äº¤ä»˜å¾—åˆ°äº†å®¢æˆ·çš„å¥½è¯„",
        "ä»Šå¤©æ—©ä¸Šå­¦ä¹ äº†æ–°çš„Pythonåº“",
        "å»å¹´ç”Ÿæ—¥æ”¶åˆ°äº†å¾ˆå¤šç¤¼ç‰©"
    ]

    for i, event in enumerate(events):
        item = MemoryItem(
            content=event,
            priority=MemoryPriority.HIGH if i < 3 else MemoryPriority.NORMAL,
            tags=['event', 'memory'],
            metadata={'source': 'user_input'}
        )

        item_id = await episodic_memory.store(item)
        print(f"å­˜å‚¨äº‹ä»¶: {event} (ID: {item_id})")

    # æ£€ç´¢æµ‹è¯•
    queries = [
        "ä»Šå¤©",
        "å¼€å¿ƒçš„",
        "æŠ€æœ¯",
        "å­¦ä¹ "
    ]

    for query in queries:
        results = await episodic_memory.retrieve(query, limit=3)
        print(f"\næŸ¥è¯¢ '{query}' çš„ç»“æœ:")
        for i, result in enumerate(results, 1):
            print(f"{i}. {result.content} (é‡è¦æ€§: {result.importance:.2f})")

# è¿è¡Œæ¼”ç¤º
# asyncio.run(episodic_memory_demo())
```

## ğŸ§  è¯­ä¹‰è®°å¿†ç³»ç»Ÿ

### çŸ¥è¯†ç®¡ç†è®°å¿†
```python
class SemanticMemorySystem(BaseMemorySystem):
    """è¯­ä¹‰è®°å¿†ç³»ç»Ÿ - å­˜å‚¨é€šç”¨çŸ¥è¯†å’Œæ¦‚å¿µ"""

    def __init__(self):
        self.facts = {}
        self.concepts = {}
        self.relationships = {}
        self.hierarchy = {}
        self.confidence_threshold = 0.6

    async def store(self, item: MemoryItem) -> str:
        """å­˜å‚¨è¯­ä¹‰è®°å¿†"""
        item.memory_type = MemoryType.SEMANTIC

        # æå–äº‹å®
        facts = self._extract_facts(item.content)
        item.metadata['facts'] = facts

        # æå–æ¦‚å¿µ
        concepts = self._extract_concepts(item.content)
        item.metadata['concepts'] = concepts

        # å­˜å‚¨è®°å¿†é¡¹
        self.facts[item.id] = item

        # æ„å»ºæ¦‚å¿µç½‘ç»œ
        for concept in concepts:
            if concept not in self.concepts:
                self.concepts[concept] = {
                    'instances': [],
                    'properties': {},
                    'relationships': []
                }

            self.concepts[concept]['instances'].append(item.id)

        # æ„å»ºå…³ç³»ç½‘ç»œ
        self._build_relationships(item)

        return item.id

    async def retrieve(self, query: str, limit: int = 10) -> List[MemoryItem]:
        """æ£€ç´¢è¯­ä¹‰è®°å¿†"""
        results = []
        query_concepts = self._extract_concepts(query)

        # æ¦‚å¿µåŒ¹é…
        for concept in query_concepts:
            if concept in self.concepts:
                for instance_id in self.concepts[concept]['instances']:
                    if instance_id in self.facts:
                        item = self.facts[instance_id]

                        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
                        relevance = self._calculate_relevance(item, query_concepts)
                        item.importance = max(item.importance, relevance)
                        results.append(item)

        # è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
        semantic_results = await self._semantic_similarity_search(query)
        results.extend(semantic_results)

        # å»é‡å¹¶æŒ‰é‡è¦æ€§æ’åº
        unique_results = {}
        for item in results:
            if item.id not in unique_results:
                unique_results[item.id] = item

        sorted_results = sorted(
            unique_results.values(),
            key=lambda x: x.importance,
            reverse=True
        )

        return sorted_results[:limit]

    def _extract_facts(self, text: str) -> List[str]:
        """æå–äº‹å®ä¿¡æ¯"""
        # ç®€åŒ–çš„äº‹å®æå– - å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨NLPæŠ€æœ¯
        import re

        # æå–æ•°å€¼äº‹å®
        number_pattern = r'\d+(?:\.\d+)?'
        numbers = re.findall(number_pattern, text)

        facts = []
        if numbers:
            facts.append(f"åŒ…å«æ•°å­—: {', '.join(numbers)}")

        # æå–æ—¥æœŸä¿¡æ¯
        date_pattern = r'\d{4}å¹´|\d{1,2}æœˆ\d{1,2}æ—¥'
        dates = re.findall(date_pattern, text)
        if dates:
            facts.append(f"åŒ…å«æ—¥æœŸ: {', '.join(dates)}")

        # æå–åœ°ç‚¹ä¿¡æ¯
        location_words = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'å—äº¬']
        for location in location_words:
            if location in text:
                facts.append(f"åœ°ç‚¹: {location}")
                break

        return facts

    def _extract_concepts(self, text: str) -> List[str]:
        """æå–æ¦‚å¿µ"""
        # ç®€åŒ–çš„æ¦‚å¿µæå–
        domain_concepts = [
            'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'äººå·¥æ™ºèƒ½',
            'æ•°æ®ç§‘å­¦', 'ç®—æ³•', 'ç¼–ç¨‹', 'Python', 'Java',
            'æ•°æ®åº“', 'äº‘è®¡ç®—', 'å¤§æ•°æ®', 'ç‰©è”ç½‘',
            'æ•°å­¦', 'ç‰©ç†', 'åŒ–å­¦', 'ç”Ÿç‰©', 'åŒ»å­¦'
        ]

        text_lower = text.lower()
        found_concepts = []

        for concept in domain_concepts:
            if concept.lower() in text_lower:
                found_concepts.append(concept)

        return found_concepts

    def _build_relationships(self, item: MemoryItem):
        """æ„å»ºå…³ç³»ç½‘ç»œ"""
        concepts = item.metadata.get('concepts', [])

        for i, concept1 in enumerate(concepts):
            for concept2 in concepts[i+1:]:
                relationship_key = f"{concept1}--{concept2}"

                if relationship_key not in self.relationships:
                    self.relationships[relationship_key] = {
                        'concept1': concept1,
                        'concept2': concept2,
                        'instances': [],
                        'frequency': 0
                    }

                self.relationships[relationship_key]['instances'].append(item.id)
                self.relationships[relationship_key]['frequency'] += 1

    def _calculate_relevance(self, item: MemoryItem, query_concepts: List[str]) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        item_concepts = item.metadata.get('concepts', [])

        if not item_concepts or not query_concepts:
            return 0.5  # é»˜è®¤ç›¸å…³æ€§

        # è®¡ç®—æ¦‚å¿µé‡å åº¦
        overlap = len(set(item_concepts) & set(query_concepts))
        union = len(set(item_concepts) | set(query_concepts))

        jaccard_similarity = overlap / union if union > 0 else 0

        # ç»¼åˆè€ƒè™‘é¡¹ç›®é‡è¦æ€§
        return (jaccard_similarity + item.importance) / 2

    async def _semantic_similarity_search(self, query: str) -> List[MemoryItem]:
        """è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢"""
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨å…³é”®è¯ç›¸ä¼¼åº¦
        query_keywords = set(self._extract_concepts(query))
        results = []

        for item_id, item in self.facts.items():
            item_keywords = set(item.metadata.get('concepts', []))

            if query_keywords and item_keywords:
                similarity = len(query_keywords & item_keywords) / len(query_keywords | item_keywords)

                if similarity > self.confidence_threshold:
                    item.importance = max(item.importance, similarity)
                    results.append(item)

        return results

    def get_concept_info(self, concept: str) -> Dict[str, Any]:
        """è·å–æ¦‚å¿µä¿¡æ¯"""
        if concept not in self.concepts:
            return {}

        concept_info = self.concepts[concept].copy()

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        instance_count = len(concept_info['instances'])
        concept_info['instance_count'] = instance_count
        concept_info['related_concepts'] = self._get_related_concepts(concept)

        return concept_info

    def _get_related_concepts(self, concept: str) -> List[str]:
        """è·å–ç›¸å…³æ¦‚å¿µ"""
        related = []

        for relationship_key, relationship in self.relationships.items():
            if relationship['concept1'] == concept:
                related.append(relationship['concept2'])
            elif relationship['concept2'] == concept:
                related.append(relationship['concept1'])

        # æŒ‰é¢‘ç‡æ’åº
        related_concepts = {}
        for related_concept in related:
            freq = self._get_concept_frequency(related_concept)
            related_concepts[related_concept] = freq

        return sorted(related_concepts.items(), key=lambda x: x[1], reverse=True)

    def _get_concept_frequency(self, concept: str) -> int:
        """è·å–æ¦‚å¿µé¢‘ç‡"""
        if concept in self.concepts:
            return len(self.concepts[concept]['instances'])
        return 0

    async def update_concept_hierarchy(self):
        """æ›´æ–°æ¦‚å¿µå±‚æ¬¡ç»“æ„"""
        # ç®€åŒ–çš„å±‚æ¬¡ç»“æ„æ„å»º
        hierarchy = {
            'è®¡ç®—æœºç§‘å­¦': {
                'äººå·¥æ™ºèƒ½': {
                    'æœºå™¨å­¦ä¹ ': ['ç›‘ç£å­¦ä¹ ', 'æ— ç›‘ç£å­¦ä¹ '],
                    'æ·±åº¦å­¦ä¹ ': ['CNN', 'RNN', 'Transformer'],
                    'è‡ªç„¶è¯­è¨€å¤„ç†': ['æ–‡æœ¬åˆ†ç±»', 'æœºå™¨ç¿»è¯‘']
                },
                'è½¯ä»¶å·¥ç¨‹': {
                    'ç¼–ç¨‹è¯­è¨€': ['Python', 'Java', 'JavaScript'],
                    'å¼€å‘æ–¹æ³•': ['æ•æ·å¼€å‘', 'DevOps']
                }
            }
        }

        self.hierarchy = hierarchy

        # æ›´æ–°æ¦‚å¿µçš„å±‚æ¬¡ä¿¡æ¯
        self._assign_hierarchy_levels(hierarchy)

    def _assign_hierarchy_levels(self, hierarchy: dict, level: int = 0, parent: str = None):
        """åˆ†é…æ¦‚å¿µå±‚æ¬¡çº§åˆ«"""
        for concept, children in hierarchy.items():
            if concept in self.concepts:
                self.concepts[concept]['level'] = level
                self.concepts[concept]['parent'] = parent
                self.concepts[concept]['children'] = list(children.keys()) if isinstance(children, dict) else children

            if isinstance(children, dict):
                self._assign_hierarchy_levels(children, level + 1, concept)

# ä½¿ç”¨ç¤ºä¾‹
async def semantic_memory_demo():
    """è¯­ä¹‰è®°å¿†æ¼”ç¤º"""
    semantic_memory = SemanticMemorySystem()

    # å­˜å‚¨çŸ¥è¯†
    knowledge_items = [
        "Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ",
        "Transformeræ˜¯ä¸€ç§é©å‘½æ€§çš„ç¥ç»ç½‘ç»œæ¶æ„",
        "æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æœ‰å¹¿æ³›åº”ç”¨"
    ]

    for i, knowledge in enumerate(knowledge_items):
        item = MemoryItem(
            content=knowledge,
            priority=MemoryPriority.HIGH,
            tags=['knowledge', 'fact'],
            metadata={'source': 'textbook', 'chapter': i+1}
        )

        item_id = await semantic_memory.store(item)
        print(f"å­˜å‚¨çŸ¥è¯†: {knowledge} (ID: {item_id})")

    # æ›´æ–°æ¦‚å¿µå±‚æ¬¡
    await semantic_memory.update_concept_hierarchy()

    # æ£€ç´¢æµ‹è¯•
    queries = [
        "Pythonç¼–ç¨‹",
        "æ·±åº¦å­¦ä¹ ",
        "ç¥ç»ç½‘ç»œ",
        "äººå·¥æ™ºèƒ½"
    ]

    for query in queries:
        results = await semantic_memory.retrieve(query, limit=3)
        print(f"\næŸ¥è¯¢ '{query}' çš„ç»“æœ:")
        for i, result in enumerate(results, 1):
            print(f"{i}. {result.content} (é‡è¦æ€§: {result.importance:.2f})")

    # è·å–æ¦‚å¿µä¿¡æ¯
    if 'Python' in semantic_memory.concepts:
        python_info = semantic_memory.get_concept_info('Python')
        print(f"\nPythonæ¦‚å¿µä¿¡æ¯: {python_info}")

# è¿è¡Œæ¼”ç¤º
# asyncio.run(semantic_memory_demo())
```

## ğŸ”§ ç»Ÿä¸€è®°å¿†ç®¡ç†

### å¤šå±‚è®°å¿†æ•´åˆ
```python
class IntegratedMemorySystem:
    """é›†æˆçš„å¤šå±‚è®°å¿†ç³»ç»Ÿ"""

    def __init__(self):
        self.working_memory = WorkingMemorySystem()
        self.episodic_memory = EpisodicMemorySystem()
        self.semantic_memory = SemanticMemorySystem()
        self.flash_memory = FlashMemorySystem()

        # è®°å¿†é—´è¿ç§»ç­–ç•¥
        self.migration_strategies = {
            'working_to_episodic': self._working_to_episodic_migration,
            'working_to_semantic': self._working_to_semantic_migration,
            'episodic_to_semantic': self._episodic_to_semantic_migration,
            'flash_to_semantic': self._flash_to_semantic_migration
        }

    async def store(self, content: str, memory_type: str = "auto",
                    importance: float = 0.5, tags: List[str] = None,
                    metadata: Dict[str, Any] = None) -> Dict[str, str]:
        """å­˜å‚¨åˆ°åˆé€‚çš„è®°å¿†ç³»ç»Ÿ"""

        item = MemoryItem(
            content=content,
            importance=importance,
            tags=tags or [],
            metadata=metadata or {}
        )

        # è‡ªåŠ¨ç¡®å®šè®°å¿†ç±»å‹
        if memory_type == "auto":
            memory_type = self._determine_memory_type(content, importance)

        result = {}

        # å­˜å‚¨åˆ°ç›¸åº”çš„è®°å¿†ç³»ç»Ÿ
        if memory_type == "working":
            item_id = await self.working_memory.store(item)
            result['working'] = item_id
        elif memory_type == "episodic":
            item_id = await self.episodic_memory.store(item)
            result['episodic'] = item_id
        elif memory_type == "semantic":
            item_id = await self.semantic_memory.store(item)
            result['semantic'] = item_id
        elif memory_type == "flash":
            item_id = await self.flash_memory.store(item)
            result['flash'] = item_id

        return result

    async def retrieve(self, query: str, memory_types: List[str] = None,
                       limit: int = 10) -> Dict[str, List[MemoryItem]]:
        """ä»å¤šä¸ªè®°å¿†ç³»ç»Ÿæ£€ç´¢"""

        if memory_types is None:
            memory_types = ['working', 'episodic', 'semantic', 'flash']

        results = {}
        tasks = []

        # å¹¶è¡Œæ£€ç´¢
        if 'working' in memory_types:
            tasks.append(('working', self.working_memory.retrieve(query, limit)))
        if 'episodic' in memory_types:
            tasks.append(('episodic', self.episodic_memory.retrieve(query, limit)))
        if 'semantic' in memory_types:
            tasks.append(('semantic', self.semantic_memory.retrieve(query, limit)))
        if 'flash' in memory_types:
            tasks.append(('flash', self.flash_memory.retrieve(query, limit)))

        # æ‰§è¡Œå¹¶è¡Œæ£€ç´¢
        for memory_type, task in tasks:
            results[memory_type] = await task

        # ç»“æœå»é‡å’Œæ’åº
        all_results = []
        seen_ids = set()

        for memory_type, memory_results in results.items():
            for item in memory_results:
                if item.id not in seen_ids:
                    # æ ‡è®°è®°å¿†æ¥æº
                    item.metadata['memory_source'] = memory_type
                    all_results.append(item)
                    seen_ids.add(item.id)

        # æŒ‰é‡è¦æ€§æ’åº
        all_results.sort(key=lambda x: x.importance, reverse=True)

        # è¿”å›å»é‡åçš„ç»“æœ
        return {'all': all_results[:limit], 'by_type': results}

    async def consolidate_memories(self):
        """æ•´åˆå’Œè¿ç§»è®°å¿†"""
        # å·¥ä½œè®°å¿†åˆ°æƒ…æ™¯è®°å¿†çš„è¿ç§»
        await self._migrate_working_memory()

        # æƒ…æ™¯è®°å¿†åˆ°è¯­ä¹‰è®°å¿†çš„è¿ç§»
        await self._migrate_episodic_memory()

        # é—ªå­˜è®°å¿†çš„å¤„ç†
        await self._process_flash_memory()

    async def _migrate_working_memory(self):
        """è¿ç§»å·¥ä½œè®°å¿†åˆ°é•¿æœŸè®°å¿†"""
        working_items = self.working_memory.items.copy()

        for item_id, item in working_items.items():
            # é‡è¦æˆ–è®¿é—®é¢‘ç¹çš„è®°å¿†è¿ç§»åˆ°é•¿æœŸè®°å¿†
            if item.importance > 0.7 or item.access_count > 3:
                if self._should_migrate_to_episodic(item):
                    await self.migration_strategies['working_to_episodic'](item)
                elif self._should_migrate_to_semantic(item):
                    await self.migration_strategies['working_to_semantic'](item)

    def _determine_memory_type(self, content: str, importance: float) -> str:
        """è‡ªåŠ¨ç¡®å®šè®°å¿†ç±»å‹"""
        content_lower = content.lower()

        # é«˜é‡è¦æ€§ä¸”åŒ…å«æƒ…æ„Ÿè¯ -> é—ªå­˜è®°å¿†
        if importance > 0.8 and self._contains_emotional_words(content_lower):
            return "flash"

        # åŒ…å«ä¸ªäººç»å†ç›¸å…³è¯ -> æƒ…æ™¯è®°å¿†
        elif self._contains_episodic_keywords(content_lower):
            return "episodic"

        # åŒ…å«çŸ¥è¯†æ€§å†…å®¹ -> è¯­ä¹‰è®°å¿†
        elif self._contains_semantic_keywords(content_lower):
            return "semantic"

        # å…¶ä»–æƒ…å†µ -> å·¥ä½œè®°å¿†
        else:
            return "working"

    def _contains_emotional_words(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«æƒ…æ„Ÿè¯"""
        emotional_words = ['å–œæ¬¢', 'çˆ±', 'è®¨åŒ', 'ææƒ§', 'æƒŠè®¶', 'å¼€å¿ƒ', 'éš¾è¿‡', 'æ„¤æ€’', 'æ¿€åŠ¨']
        return any(word in text for word in emotional_words)

    def _contains_episodic_keywords(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«æƒ…æ™¯è®°å¿†å…³é”®è¯"""
        episodic_keywords = ['æˆ‘', 'æ˜¨å¤©', 'ä»Šå¤©', 'æœ‹å‹', 'å®¶äºº', 'å»', 'åš', 'çœ‹', 'å¬', 'è¯´']
        return any(word in text for word in episodic_keywords)

    def _contains_semantic_keywords(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è¯­ä¹‰è®°å¿†å…³é”®è¯"""
        semantic_keywords = ['å®šä¹‰', 'åŸç†', 'ç†è®º', 'å…¬å¼', 'æ–¹æ³•', 'ç®—æ³•', 'æ•°æ®', 'ä¿¡æ¯', 'çŸ¥è¯†']
        return any(word in text for word in semantic_keywords)

    def _should_migrate_to_episodic(self, item: MemoryItem) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿ç§»åˆ°æƒ…æ™¯è®°å¿†"""
        return (item.importance > 0.7 or
                item.access_count > 5 or
                (datetime.now() - item.created_at).total_seconds() > 3600)  # è¶…è¿‡1å°æ—¶

    def _should_migrate_to_semantic(self, item: MemoryItem) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿ç§»åˆ°è¯­ä¹‰è®°å¿†"""
        return (item.importance > 0.6 and
                not self._contains_episodic_keywords(item.content.lower()))

    async def _working_to_episodic_migration(self, item: MemoryItem):
        """å·¥ä½œè®°å¿†åˆ°æƒ…æ™¯è®°å¿†çš„è¿ç§»"""
        new_item = MemoryItem(
            content=item.content,
            memory_type=MemoryType.EPISODIC,
            importance=item.importance,
            tags=item.tags + ['migrated'],
            metadata=item.metadata
        )

        await self.episodic_memory.store(new_item)
        await self.working_memory.delete(item.id)

    async def _working_to_semantic_migration(self, item: MemoryItem):
        """å·¥ä½œè®°å¿†åˆ°è¯­ä¹‰è®°å¿†çš„è¿ç§»"""
        new_item = MemoryItem(
            content=item.content,
            memory_type=MemoryType.SEMANTIC,
            importance=item.importance,
            tags=item.tags + ['migrated'],
            metadata=item.metadata
        )

        await self.semantic_memory.store(new_item)
        await self.working_memory.delete(item.id)

    async def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–è®°å¿†ç³»ç»Ÿç»Ÿè®¡"""
        stats = {
            'working_memory': await self.working_memory.get_stats(),
            'episodic_memory': await self.episodic_memory.get_stats(),
            'semantic_memory': await self.semantic_memory.get_stats(),
            'flash_memory': await self.flash_memory.get_stats()
        }

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_items = sum(s.get('total_items', 0) for s in stats.values())
        stats['total_items'] = total_items
        stats['distribution'] = {
            'working': stats['working_memory'].get('total_items', 0) / total_items if total_items > 0 else 0,
            'episodic': stats['episodic_memory'].get('total_items', 0) / total_items if total_items > 0 else 0,
            'semantic': stats['semantic_memory'].get('total_items', 0) / total_items if total_items > 0 else 0,
            'flash': stats['flash_memory'].get('total_items', 0) / total_items if total_items > 0 else 0
        }

        return stats

class FlashMemorySystem(BaseMemorySystem):
    """é—ªå­˜è®°å¿†ç³»ç»Ÿ - å­˜å‚¨å…³é”®äº‹ä»¶"""

    def __init__(self, max_items: int = 100):
        self.max_items = max_items
        self.flash_memories = {}
        self.importance_threshold = 0.8

    async def store(self, item: MemoryItem) -> str:
        """å­˜å‚¨é—ªå­˜è®°å¿†"""
        item.memory_type = MemoryType.FLASH

        # æ£€æŸ¥é‡è¦æ€§é˜ˆå€¼
        if item.importance < self.importance_threshold:
            item.importance = self.importance_threshold

        # æ£€æŸ¥å®¹é‡é™åˆ¶
        if len(self.flash_memories) >= self.max_items:
            await self._evict_least_important()

        self.flash_memories[item.id] = item
        return item.id

    async def retrieve(self, query: str, limit: int = 5) -> List[MemoryItem]:
        """æ£€ç´¢é—ªå­˜è®°å¿†"""
        results = []
        query_lower = query.lower()

        for item in self.flash_memories.values():
            if self._is_relevant(item.content.lower(), query_lower):
                results.append(item)

        # æŒ‰é‡è¦æ€§æ’åº
        results.sort(key=lambda x: x.importance, reverse=True)
        return results[:limit]

    def _is_relevant(self, content: str, query: str) -> bool:
        """åˆ¤æ–­å†…å®¹ç›¸å…³æ€§"""
        return query in content or any(word in content for word in query.split())

    async def _evict_least_important(self):
        """æ·˜æ±°æœ€ä¸é‡è¦çš„é—ªå­˜è®°å¿†"""
        if not self.flash_memories:
            return

        least_important = min(
            self.flash_memories.values(),
            key=lambda x: x.importance
        )

        del self.flash_memories[least_important.id]

    async def update(self, item_id: str, updates: Dict[str, Any]) -> bool:
        """æ›´æ–°é—ªå­˜è®°å¿†"""
        if item_id not in self.flash_memories:
            return False

        item = self.flash_memories[item_id]
        for key, value in updates.items():
            if hasattr(item, key):
                setattr(item, key, value)

        return True

    async def delete(self, item_id: str) -> bool:
        """åˆ é™¤é—ªå­˜è®°å¿†"""
        if item_id in self.flash_memories:
            del self.flash_memories[item_id]
            return True
        return False

    async def cleanup(self) -> int:
        """æ¸…ç†è¿‡æœŸè®°å¿†"""
        # é—ªå­˜è®°å¿†é€šå¸¸ä¸è¿‡æœŸ
        return 0

    async def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_items': len(self.flash_memories),
            'max_capacity': self.max_items,
            'capacity_usage': len(self.flash_memories) / self.max_items,
            'average_importance': sum(item.importance for item in self.flash_memories.values()) / len(self.flash_memories) if self.flash_memories else 0
        }

# ä½¿ç”¨ç¤ºä¾‹
async def integrated_memory_demo():
    """é›†æˆè®°å¿†ç³»ç»Ÿæ¼”ç¤º"""
    integrated_memory = IntegratedMemorySystem()

    # å¯åŠ¨å·¥ä½œè®°å¿†
    await integrated_memory.working_memory.start()

    # å­˜å‚¨ä¸åŒç±»å‹çš„è®°å¿†
    memories = [
        ("æˆ‘å­¦ä¹ äº†Pythonç¼–ç¨‹", "working", 0.6),
        ("æ˜¨å¤©å’Œæœ‹å‹èšä¼šå¾ˆå¼€å¿ƒ", "episodic", 0.8),
        ("æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯", "semantic", 0.7),
        ("ä»Šå¤©è·å¾—äº†é‡è¦çš„é¡¹ç›®æ‰¹å‡†ï¼", "flash", 0.9)
    ]

    for content, memory_type, importance in memories:
        result = await integrated_memory.store(content, memory_type, importance)
        print(f"å­˜å‚¨: {content} -> {result}")

    # æ£€ç´¢æµ‹è¯•
    query = "å­¦ä¹ "
    results = await integrated_memory.retrieve(query)

    print(f"\næŸ¥è¯¢ '{query}' çš„ç»“æœ:")
    for i, result in enumerate(results['all'], 1):
        print(f"{i}. {result.content} (æ¥æº: {result.metadata.get('memory_source', 'unknown')})")

    # æ•´åˆè®°å¿†
    await integrated_memory.consolidate_memories()

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = await integrated_memory.get_memory_stats()
    print(f"\nè®°å¿†ç³»ç»Ÿç»Ÿè®¡:")
    for system_name, system_stats in stats.items():
        if isinstance(system_stats, dict) and 'total_items' in system_stats:
            print(f"  {system_name}: {system_stats['total_items']} é¡¹")

    # åœæ­¢å·¥ä½œè®°å¿†
    await integrated_memory.working_memory.stop()

# è¿è¡Œæ¼”ç¤º
# asyncio.run(integrated_memory_demo())
```

## ğŸ“Š è®°å¿†ç³»ç»Ÿä¼˜åŒ–

### æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯
```python
class MemoryOptimizer:
    """è®°å¿†ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self, memory_system):
        self.memory_system = memory_system
        self.optimization_stats = {
            'compression_savings': 0,
            'cache_hits': 0,
            'query_time_saved': 0.0
        }

    async def optimize_storage(self):
        """ä¼˜åŒ–å­˜å‚¨æ€§èƒ½"""
        # å‹ç¼©è®°å¿†å†…å®¹
        await self._compress_memory_content()

        # æ„å»ºç¼“å­˜ç´¢å¼•
        await self._build_cache_indexes()

        # æ¸…ç†è¿‡æœŸè®°å¿†
        await self.memory_system.cleanup()

    async def _compress_memory_content(self):
        """å‹ç¼©è®°å¿†å†…å®¹"""
        # ç®€åŒ–çš„å‹ç¼©å®ç°
        compressed_count = 0

        for item in list(self.memory_system.episodic_memory.episodes.values()):
            original_length = len(item.content)

            # ç®€å•çš„æ–‡æœ¬å‹ç¼©
            if len(item.content) > 1000:
                # æˆªæ–­è¿‡é•¿çš„å†…å®¹
                item.content = item.content[:500] + "...[truncated]"
                compressed_count += 1

        self.optimization_stats['compression_savings'] += compressed_count
        return compressed_count

    async def _build_cache_indexes(self):
        """æ„å»ºç¼“å­˜ç´¢å¼•"""
        # ä¸ºé¢‘ç¹æŸ¥è¯¢æ„å»ºç¼“å­˜
        cache_size = 100

        # æ¨¡æ‹Ÿæ„å»ºç¼“å­˜
        pass

    async def monitor_performance(self):
        """ç›‘æ§æ€§èƒ½æŒ‡æ ‡"""
        stats = await self.memory_system.get_stats()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if stats.get('total_items', 0) > 0:
            avg_importance = sum(
                getattr(item, 'importance', 0.5)
                for item in list(self.memory_system.episodic_memory.episodes.values())
            ) / len(self.memory_system.episodic_memory.episodes)

            stats['average_importance'] = avg_importance

        return stats

# è¿è¡Œä¼˜åŒ–ç¤ºä¾‹
# optimizer = MemoryOptimizer(episodic_memory)
# await optimizer.optimize_storage()
```

## ğŸ“ æ€»ç»“

è®°å¿†ç³»ç»Ÿè®¾è®¡æ˜¯æ„å»ºæ™ºèƒ½Agentçš„å…³é”®æŠ€æœ¯ï¼Œæœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†åˆ†å±‚è®°å¿†æ¶æ„ã€å®ç°æŠ€æœ¯å’Œä¼˜åŒ–æ–¹æ³•ã€‚

### ğŸ¯ å…³é”®è¦ç‚¹
- **åˆ†å±‚æ¶æ„**: å·¥ä½œã€æƒ…æ™¯ã€è¯­ä¹‰ã€ç¨‹åºæ€§ã€é—ªå­˜è®°å¿†çš„å¤šå±‚è®¾è®¡
- **åŠ¨æ€è¿ç§»**: è®°å¿†åœ¨ä¸åŒå±‚æ¬¡é—´çš„è‡ªåŠ¨è¿ç§»
- **æ£€ç´¢ä¼˜åŒ–**: å¤šç§æ£€ç´¢ç­–ç•¥çš„ç»„åˆä¼˜åŒ–
- **æ€§èƒ½ç›‘æ§**: å®Œå–„çš„æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–æœºåˆ¶

### ğŸš€ å®ç°ç‰¹è‰²
- **è‡ªé€‚åº”å­˜å‚¨**: æ ¹æ®å†…å®¹è‡ªåŠ¨é€‰æ‹©å­˜å‚¨å±‚æ¬¡
- **æ™ºèƒ½æ£€ç´¢**: å…³é”®è¯ã€è¯­ä¹‰ã€æ—¶é—´çš„ç»¼åˆæ£€ç´¢
- **å†…å­˜ç®¡ç†**: å®¹é‡é™åˆ¶å’Œè‡ªåŠ¨æ¸…ç†æœºåˆ¶
- **å¹¶è¡Œå¤„ç†**: å¤šè®°å¿†ç³»ç»Ÿçš„å¹¶è¡Œæ£€ç´¢

### ğŸ”„ ä¸‹ä¸€æ­¥
- å­¦ä¹ [å·¥å…·è°ƒç”¨ç³»ç»Ÿ](03-å·¥å…·è°ƒç”¨.md)
- äº†è§£[RAGé›†æˆAgent](04-RAGç³»ç»Ÿ.md)
- æŒæ¡[ä¸Šä¸‹æ–‡ç®¡ç†](05-ä¸Šä¸‹æ–‡ç®¡ç†.md)
- æ¢ç´¢[å†³ç­–è§„åˆ’](06-å†³ç­–è§„åˆ’.md)

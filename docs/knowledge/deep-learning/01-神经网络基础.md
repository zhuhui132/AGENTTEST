# ğŸ§  ç¥ç»ç½‘ç»œåŸºç¡€

## ğŸ“š æ¦‚è¿°

ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒï¼Œæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»ç³»ç»Ÿçš„ç»“æ„ï¼Œé€šè¿‡å¤šå±‚ç¥ç»å…ƒè¿›è¡Œä¿¡æ¯å¤„ç†ã€‚æœ¬æ–‡æ¡£ä»‹ç»ç¥ç»ç½‘ç»œçš„åŸºç¡€æ¦‚å¿µã€æ¶æ„å’Œå®ç°æŠ€å·§ã€‚

## ğŸ¯ ç¥ç»ç½‘ç»œåŸºç¡€

### ç½‘ç»œç»“æ„
- **è¾“å…¥å±‚**: æ¥æ”¶åŸå§‹æ•°æ®
- **éšè—å±‚**: ç‰¹å¾æå–å’Œè½¬æ¢
- **è¾“å‡ºå±‚**: ç”Ÿæˆæœ€ç»ˆé¢„æµ‹
- **æ¿€æ´»å‡½æ•°**: å¼•å…¥éçº¿æ€§
- **è¿æ¥æƒé‡**: å­¦ä¹ çš„å‚æ•°

### å‰å‘ä¼ æ’­
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleNeuralNetwork(nn.Module):
    """ç®€å•ç¥ç»ç½‘ç»œå®ç°"""
    def __init__(self, input_size, hidden_sizes, output_size):
        super().__init__()

        # æ„å»ºç½‘ç»œå±‚
        layers = []
        prev_size = input_size

        for hidden_size in hidden_sizes:
            layers.extend([
                nn.Linear(prev_size, hidden_size),
                nn.ReLU()
            ])
            prev_size = hidden_size

        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, output_size))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# ç¤ºä¾‹ä½¿ç”¨
input_size = 784  # MNISTå›¾åƒ 28x28 = 784
hidden_sizes = [256, 128, 64]
output_size = 10   # 10ä¸ªç±»åˆ«

model = SimpleNeuralNetwork(input_size, hidden_sizes, output_size)
print(model)
```

### æ¿€æ´»å‡½æ•°è¯¦è§£
```python
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

class ActivationFunctions:
    """æ¿€æ´»å‡½æ•°é›†åˆ"""

    @staticmethod
    def relu(x):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return F.relu(x)

    @staticmethod
    def leaky_relu(x, negative_slope=0.01):
        """Leaky ReLUæ¿€æ´»å‡½æ•°"""
        return F.leaky_relu(x, negative_slope=negative_slope)

    @staticmethod
    def prelu(x, weight=0.25):
        """Parameterized ReLU"""
        return F.prelu(x, weight=torch.tensor(weight))

    @staticmethod
    def elu(x, alpha=1.0):
        """ELUæ¿€æ´»å‡½æ•°"""
        return F.elu(x, alpha=alpha)

    @staticmethod
    def gelu(x):
        """GELUæ¿€æ´»å‡½æ•° - Transformeré»˜è®¤"""
        return F.gelu(x)

    @staticmethod
    def swish(x):
        """Swishæ¿€æ´»å‡½æ•° (x * sigmoid(x))"""
        return x * torch.sigmoid(x)

    @staticmethod
    def sigmoid(x):
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        return torch.sigmoid(x)

    @staticmethod
    def tanh(x):
        """Tanhæ¿€æ´»å‡½æ•°"""
        return torch.tanh(x)

    @staticmethod
    def softmax(x, dim=1):
        """Softmaxæ¿€æ´»å‡½æ•°"""
        return F.softmax(x, dim=dim)

    @staticmethod
    def visualize_activations():
        """å¯è§†åŒ–ä¸åŒæ¿€æ´»å‡½æ•°"""
        x = torch.linspace(-5, 5, 100)

        activations = {
            'ReLU': ActivationFunctions.relu(x),
            'Leaky ReLU': ActivationFunctions.leaky_relu(x),
            'ELU': ActivationFunctions.elu(x),
            'GELU': ActivationFunctions.gelu(x),
            'Swish': ActivationFunctions.swish(x),
            'Sigmoid': ActivationFunctions.sigmoid(x),
            'Tanh': ActivationFunctions.tanh(x)
        }

        plt.figure(figsize=(15, 10))
        for i, (name, activation) in enumerate(activations.items(), 1):
            plt.subplot(2, 4, i)
            plt.plot(x.numpy(), activation.numpy(), linewidth=2)
            plt.title(name)
            plt.grid(True, alpha=0.3)
            plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
            plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)

        plt.tight_layout()
        plt.show()

# å¯è§†åŒ–æ¿€æ´»å‡½æ•°
ActivationFunctions.visualize_activations()
```

## ğŸ”¥ æŸå¤±å‡½æ•°

### åˆ†ç±»æŸå¤±å‡½æ•°
```python
import torch.nn as nn
import torch

class LossFunctions:
    """æŸå¤±å‡½æ•°é›†åˆ"""

    def __init__(self):
        self.ce_loss = nn.CrossEntropyLoss()
        self.nll_loss = nn.NLLLoss()
        self.bce_loss = nn.BCELoss()
        self.bce_with_logits_loss = nn.BCEWithLogitsLoss()
        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')
        self.hinge_loss = nn.HingeEmbeddingLoss()
        self.cosine_embedding_loss = nn.CosineEmbeddingLoss()

    def focal_loss(self, inputs, targets, alpha=1.0, gamma=2.0):
        """Focal Loss - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
        ce_loss = self.ce_loss(inputs, targets)
        pt = torch.exp(-ce_loss)  # pt = probability of true class
        focal_loss = alpha * (1 - pt) ** gamma * ce_loss
        return focal_loss.mean()

    def label_smoothing_loss(self, inputs, targets, smoothing=0.1, classes=10):
        """æ ‡ç­¾å¹³æ»‘æŸå¤±"""
        confidence = 1.0 - smoothing
        smooth_value = smoothing / (classes - 1)

        # åˆ›å»ºå¹³æ»‘æ ‡ç­¾
        one_hot = torch.zeros_like(inputs).scatter_(1, targets.unsqueeze(1), confidence)
        smooth_labels = one_hot + smooth_value

        log_prob = F.log_softmax(inputs, dim=1)
        return (-smooth_labels * log_prob).sum(dim=1).mean()

    def contrastive_loss(self, output1, output2, label, margin=1.0):
        """å¯¹æ¯”å­¦ä¹ æŸå¤±"""
        euclidean_distance = F.pairwise_distance(output1, output2)

        loss_contrastive = torch.mean(
            (1 - label) * torch.pow(euclidean_distance, 2) +
            label * torch.pow(torch.clamp(margin - euclidean_distance, min=0.0), 2)
        )
        return loss_contrastive

    def triplet_loss(self, anchor, positive, negative, margin=1.0):
        """ä¸‰å…ƒç»„æŸå¤±"""
        pos_dist = F.pairwise_distance(anchor, positive)
        neg_dist = F.pairwise_distance(anchor, negative)

        loss = F.relu(pos_dist - neg_dist + margin)
        return loss.mean()

# ç¤ºä¾‹ä½¿ç”¨
loss_functions = LossFunctions()

# æ¨¡æ‹Ÿæ•°æ®
logits = torch.randn(4, 10)  # 4ä¸ªæ ·æœ¬ï¼Œ10ä¸ªç±»åˆ«
targets = torch.tensor([0, 1, 2, 3])

# ä¸åŒæŸå¤±å‡½æ•°
ce_loss = loss_functions.ce_loss(logits, targets)
focal_loss = loss_functions.focal_loss(logits, targets, alpha=1.0, gamma=2.0)
smooth_loss = loss_functions.label_smoothing_loss(logits, targets, smoothing=0.1)

print(f"Cross Entropy Loss: {ce_loss:.4f}")
print(f"Focal Loss: {focal_loss:.4f}")
print(f"Label Smoothing Loss: {smooth_loss:.4f}")
```

### å›å½’æŸå¤±å‡½æ•°
```python
class RegressionLosses:
    """å›å½’æŸå¤±å‡½æ•°"""

    def __init__(self):
        self.mse_loss = nn.MSELoss()
        self.mae_loss = nn.L1Loss()
        self.smooth_l1_loss = nn.SmoothL1Loss()
        self.huber_loss = nn.HuberLoss()

    def quantile_loss(self, predictions, targets, quantile=0.5):
        """åˆ†ä½æ•°æŸå¤±"""
        errors = targets - predictions
        quantile_loss = torch.mean(
            torch.max(quantile * errors, (quantile - 1) * errors)
        )
        return quantile_loss

    def log_cosh_loss(self, predictions, targets):
        """Log-CoshæŸå¤±"""
        return torch.mean(torch.log(torch.cosh(predictions - targets)))

    def custom_loss(self, predictions, targets, outlier_threshold=2.0):
        """è‡ªå®šä¹‰æŸå¤±ï¼šå¯¹å¼‚å¸¸å€¼æ›´é²æ£’"""
        abs_error = torch.abs(predictions - targets)

        # å¯¹æ­£å¸¸è¯¯å·®ä½¿ç”¨L1æŸå¤±ï¼Œå¯¹å¼‚å¸¸å€¼ä½¿ç”¨L2æŸå¤±
        normal_mask = abs_error <= outlier_threshold
        outlier_mask = ~normal_mask

        normal_loss = F.l1_loss(predictions[normal_mask], targets[normal_mask])
        outlier_loss = F.mse_loss(predictions[outlier_mask], targets[outlier_mask])

        total_loss = normal_loss + outlier_loss
        return total_loss

# ç¤ºä¾‹ä½¿ç”¨
regression_losses = RegressionLosses()

preds = torch.randn(5, 1)
targets = torch.randn(5, 1)

mse = regression_losses.mse_loss(preds, targets)
mae = regression_losses.mae_loss(preds, targets)
huber = regression_losses.huber_loss(preds, targets)
quantile = regression_losses.quantile_loss(preds, targets, quantile=0.8)

print(f"MSE: {mse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"Huber: {huber:.4f}")
print(f"Quantile (0.8): {quantile:.4f}")
```

## ğŸ›ï¸ ç½‘ç»œåˆå§‹åŒ–

### æƒé‡åˆå§‹åŒ–æ–¹æ³•
```python
def initialize_weights(model, init_type='xavier_uniform'):
    """æƒé‡åˆå§‹åŒ–"""
    for name, param in model.named_parameters():
        if 'weight' in name:
            if init_type == 'xavier_uniform':
                nn.init.xavier_uniform_(param)
            elif init_type == 'xavier_normal':
                nn.init.xavier_normal_(param)
            elif init_type == 'kaiming_uniform':
                nn.init.kaiming_uniform_(param, mode='fan_in', nonlinearity='relu')
            elif init_type == 'kaiming_normal':
                nn.init.kaiming_normal_(param, mode='fan_in', nonlinearity='relu')
            elif init_type == 'normal':
                nn.init.normal_(param, mean=0, std=0.02)
            elif init_type == 'orthogonal':
                nn.init.orthogonal_(param)
        elif 'bias' in name:
            nn.init.constant_(param, 0)

# ç¤ºä¾‹ï¼šä¸åŒçš„åˆå§‹åŒ–ç­–ç•¥
class CustomInitialization:
    def __init__(self, model):
        self.model = model

    def xavier_init(self):
        """Xavieråˆå§‹åŒ– - é€‚ç”¨äºsigmoid/tanh"""
        initialize_weights(self.model, 'xavier_uniform')

    def he_init(self):
        """Heåˆå§‹åŒ– - é€‚ç”¨äºReLU/Leaky ReLU"""
        initialize_weights(self.model, 'kaiming_uniform')

    def orthogonal_init(self):
        """æ­£äº¤åˆå§‹åŒ– - é€‚ç”¨äºRNN"""
        initialize_weights(self.model, 'orthogonal')
```

## ğŸ”§ ä¼˜åŒ–å™¨

### ä¼˜åŒ–å™¨æ¯”è¾ƒ
```python
class OptimizerComparison:
    """ä¼˜åŒ–å™¨æ¯”è¾ƒ"""

    @staticmethod
    def get_optimizers(model, lr=0.001):
        """è·å–ä¸åŒçš„ä¼˜åŒ–å™¨"""
        optimizers = {
            'SGD': torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9),
            'SGD+Nesterov': torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True),
            'Adam': torch.optim.Adam(model.parameters(), lr=lr),
            'AdamW': torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01),
            'RMSprop': torch.optim.RMSprop(model.parameters(), lr=lr),
            'Adagrad': torch.optim.Adagrad(model.parameters(), lr=lr),
            'Adadelta': torch.optim.Adadelta(model.parameters(), lr=lr),
            'Adamax': torch.optim.Adamax(model.parameters(), lr=lr),
            'RAdam': torch.optim.RAdam(model.parameters(), lr=lr),
            'Lookahead': torch.optim.SGD(model.parameters(), lr=lr)
        }
        return optimizers

    @staticmethod
    def create_optimizer_with_schedule(model, optimizer_type='adam', lr=0.001):
        """åˆ›å»ºå¸¦å­¦ä¹ ç‡è°ƒåº¦çš„ä¼˜åŒ–å™¨"""
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        schedulers = {
            'step': torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1),
            'cosine': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100),
            'exponential': torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95),
            'plateau': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10),
            'onecycle': torch.optim.lr_scheduler.OneCycleLR(
                optimizer, max_lr=lr, steps_per_epoch=100, epochs=10
            ),
            'cosine_warm_restart': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer, T_0=10, T_mult=2
            )
        }

        return optimizer, schedulers

# ç¤ºä¾‹ä½¿ç”¨
model = SimpleNeuralNetwork(784, [256, 128], 10)
optimizers = OptimizerComparison.get_optimizers(model)

print("å¯ç”¨çš„ä¼˜åŒ–å™¨:")
for name in optimizers.keys():
    print(f"- {name}")
```

## ğŸ§ª è®­ç»ƒæŠ€å·§

### è®­ç»ƒå¾ªç¯æ¨¡æ¿
```python
class TrainingTemplate:
    """è®­ç»ƒå¾ªç¯æ¨¡æ¿"""

    def __init__(self, model, train_loader, val_loader, optimizer, criterion, device='cuda'):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device

        # è®­ç»ƒçŠ¶æ€
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)

            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()

            running_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.6f}')

        epoch_loss = running_loss / len(self.train_loader)
        epoch_acc = 100. * correct / total

        self.train_losses.append(epoch_loss)
        self.train_accuracies.append(epoch_acc)

        return epoch_loss, epoch_acc

    def validate_epoch(self, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in self.val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)

                running_loss += loss.item()
                _, predicted = output.max(1)
                total += target.size(0)
                correct += predicted.eq(target).sum().item()

        epoch_loss = running_loss / len(self.val_loader)
        epoch_acc = 100. * correct / total

        self.val_losses.append(epoch_loss)
        self.val_accuracies.append(epoch_acc)

        return epoch_loss, epoch_acc

    def train(self, num_epochs):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        best_val_acc = 0.0

        for epoch in range(1, num_epochs + 1):
            train_loss, train_acc = self.train_epoch(epoch)
            val_loss, val_acc = self.validate_epoch(epoch)

            print(f'Epoch {epoch}:')
            print(f'  Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.2f}%')
            print(f'  Val Loss: {val_loss:.6f}, Val Acc: {val_acc:.2f}%')

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(self.model.state_dict(), 'best_model.pth')
                print(f'  New best validation accuracy: {best_val_acc:.2f}%')

        return {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies
        }
```

### é«˜çº§è®­ç»ƒæŠ€å·§
```python
class AdvancedTraining:
    """é«˜çº§è®­ç»ƒæŠ€å·§"""

    def __init__(self, model):
        self.model = model
        self.scaler = torch.cuda.amp.GradScaler()  # æ··åˆç²¾åº¦
        self.use_amp = torch.cuda.is_available()

    def mixed_precision_training(self, data, target, optimizer, criterion):
        """æ··åˆç²¾åº¦è®­ç»ƒ"""
        if not self.use_amp:
            return self.standard_training(data, target, optimizer, criterion)

        with torch.cuda.amp.autocast():
            output = self.model(data)
            loss = criterion(output, target)

        # ç¼©æ”¾æŸå¤±å¹¶è¿›è¡Œåå‘ä¼ æ’­
        self.scaler.scale(loss).backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

        # æ›´æ–°å‚æ•°
        self.scaler.step(optimizer)
        self.scaler.update()

        return loss.item()

    def standard_training(self, data, target, optimizer, criterion):
        """æ ‡å‡†è®­ç»ƒ"""
        optimizer.zero_grad()
        output = self.model(data)
        loss = criterion(output, target)
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

        optimizer.step()
        return loss.item()

    def gradient_accumulation(self, data_loader, optimizer, criterion, accumulation_steps=4):
        """æ¢¯åº¦ç´¯ç§¯ - æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒ"""
        self.model.train()
        optimizer.zero_grad()

        for i, (data, target) in enumerate(data_loader):
            data, target = data.cuda(), target.cuda()

            output = self.model(data)
            loss = criterion(output, target)
            loss = loss / accumulation_steps  # å½’ä¸€åŒ–æŸå¤±
            loss.backward()

            if (i + 1) % accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()

    def early_stopping(self, val_loss, patience=7, min_delta=0.001):
        """æ—©åœæœºåˆ¶"""
        if not hasattr(self, 'best_loss'):
            self.best_loss = val_loss
            self.counter = 0
            return False

        if val_loss < self.best_loss - min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= patience
```

## ğŸ“Š æ¨¡å‹è¯„ä¼°ä¸å¯è§†åŒ–

### è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
```python
import matplotlib.pyplot as plt

def plot_training_curves(train_losses, val_losses, train_accs, val_accs):
    """å¯è§†åŒ–è®­ç»ƒæ›²çº¿"""
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))

    # æŸå¤±æ›²çº¿
    axes[0].plot(train_losses, label='Train Loss', color='blue')
    axes[0].plot(val_losses, label='Val Loss', color='red')
    axes[0].set_title('Training and Validation Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # å‡†ç¡®ç‡æ›²çº¿
    axes[1].plot(train_accs, label='Train Acc', color='blue')
    axes[1].plot(val_accs, label='Val Acc', color='red')
    axes[1].set_title('Training and Validation Accuracy')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Accuracy (%)')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def plot_confusion_matrix(model, test_loader, class_names, device='cuda'):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    from sklearn.metrics import confusion_matrix
    import seaborn as sns

    model.eval()
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            _, preds = torch.max(output, 1)

            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(target.cpu().numpy())

    cm = confusion_matrix(all_targets, all_preds)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()
```

## ğŸ“ æ€»ç»“

ç¥ç»ç½‘ç»œåŸºç¡€æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œç†è§£ç½‘ç»œç»“æ„ã€æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°å’Œè®­ç»ƒæŠ€å·§å¯¹äºæ„å»ºæœ‰æ•ˆçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ã€‚

### ğŸ¯ å…³é”®è¦ç‚¹
- **ç½‘ç»œæ¶æ„**è®¾è®¡éœ€è¦è€ƒè™‘æ•°æ®ç‰¹ç‚¹å’Œä»»åŠ¡éœ€æ±‚
- **æ¿€æ´»å‡½æ•°**é€‰æ‹©å½±å“ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒç¨³å®šæ€§
- **æŸå¤±å‡½æ•°**éœ€è¦ä¸ä»»åŠ¡ç±»å‹å’Œæ•°æ®åˆ†å¸ƒåŒ¹é…
- **æƒé‡åˆå§‹åŒ–**å¯¹è®­ç»ƒæ”¶æ•›é€Ÿåº¦å’Œæœ€ç»ˆæ€§èƒ½æœ‰é‡è¦å½±å“
- **è®­ç»ƒæŠ€å·§**å¦‚æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯ç­‰å¯ä»¥æé«˜è®­ç»ƒæ•ˆç‡

### ğŸš€ ä¸‹ä¸€æ­¥
- å­¦ä¹ [CNNæ¶æ„è¯¦è§£](02-CNNæ¶æ„.md)
- äº†è§£[RNNæ¶æ„è¯¦è§£](03-RNNæ¶æ„.md)
- æŒæ¡[Transformeræ¶æ„](04-Transformer.md)
- æ¢ç´¢[ç°ä»£ç½‘ç»œæ¶æ„](05-ç°ä»£æ¶æ„.md)

# ğŸ¯ æ¨¡å‹é‡åŒ–

## ğŸ“š æ¦‚è¿°

æ¨¡å‹é‡åŒ–æ˜¯å°†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æƒé‡ä»é«˜ç²¾åº¦æµ®ç‚¹æ•°è½¬æ¢ä¸ºä½ç²¾åº¦æ•´æ•°çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ¨¡å‹å¤§å°ã€æé«˜æ¨ç†é€Ÿåº¦ã€é™ä½å†…å­˜ä½¿ç”¨ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»æ¨¡å‹é‡åŒ–çš„åŸç†ã€æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚

## ğŸ”§ é‡åŒ–åŸºç¡€åŸç†

### é‡åŒ–æ¦‚å¿µ
```python
import numpy as np
from typing import Tuple, List
import torch
import torch.nn as nn

class QuantizationBasics:
    """é‡åŒ–åŸºç¡€æ¦‚å¿µ"""

    def __init__(self):
        self.supported_dtypes = {
            'int8': (np.int8, -128, 127),
            'uint8': (np.uint8, 0, 255),
            'int16': (np.int16, -32768, 32767),
            'int32': (np.int32, -2147483648, 2147483647)
        }

    def calculate_quantization_params(self, weights: np.ndarray,
                                  dtype: str = 'int8') -> Tuple[float, int]:
        """
        è®¡ç®—é‡åŒ–å‚æ•°
        Args:
            weights: åŸå§‹æƒé‡æ•°ç»„
            dtype: ç›®æ ‡é‡åŒ–ç±»å‹
        Returns:
            (scale, zero_point): ç¼©æ”¾å› å­å’Œé›¶ç‚¹
        """
        if dtype not in self.supported_dtypes:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {dtype}")

        np_dtype, qmin, qmax = self.supported_dtypes[dtype]

        # è®¡ç®—æœ€å°å€¼å’Œæœ€å¤§å€¼
        rmin, rmax = weights.min(), weights.max()

        # è®¡ç®—ç¼©æ”¾å› å­
        scale = (rmax - rmin) / (qmax - qmin)

        # è®¡ç®—é›¶ç‚¹
        zero_point = int(qmin - rmin / scale)
        zero_point = max(qmin, min(qmax, zero_point))

        return float(scale), zero_point

    def quantize_tensor(self, tensor: np.ndarray, scale: float,
                       zero_point: int, dtype: str = 'int8') -> np.ndarray:
        """
        é‡åŒ–å¼ é‡
        Args:
            tensor: è¾“å…¥å¼ é‡
            scale: ç¼©æ”¾å› å­
            zero_point: é›¶ç‚¹
            dtype: ç›®æ ‡æ•°æ®ç±»å‹
        Returns:
            é‡åŒ–åçš„å¼ é‡
        """
        np_dtype, _, _ = self.supported_dtypes[dtype]

        # é‡åŒ–å…¬å¼: q = round(r / scale) + zero_point
        quantized = np.round(tensor / scale) + zero_point

        # é™åˆ¶åœ¨ç›®æ ‡æ•°æ®ç±»å‹èŒƒå›´å†…
        _, qmin, qmax = self.supported_dtypes[dtype]
        quantized = np.clip(quantized, qmin, qmax)

        return quantized.astype(np_dtype)

    def dequantize_tensor(self, quantized: np.ndarray, scale: float,
                         zero_point: int) -> np.ndarray:
        """
        åé‡åŒ–å¼ é‡
        Args:
            quantized: é‡åŒ–åçš„å¼ é‡
            scale: ç¼©æ”¾å› å­
            zero_point: é›¶ç‚¹
        Returns:
            åé‡åŒ–åçš„å¼ é‡
        """
        # åé‡åŒ–å…¬å¼: r = (q - zero_point) * scale
        return (quantized.astype(np.float32) - zero_point) * scale

    def quantize_with_calibration(self, weights: np.ndarray,
                                  calibration_data: List[np.ndarray],
                                  dtype: str = 'int8') -> Tuple[np.ndarray, float, int]:
        """
        ä½¿ç”¨æ ¡å‡†æ•°æ®è¿›è¡Œé‡åŒ–
        Args:
            weights: åŸå§‹æƒé‡
            calibration_data: æ ¡å‡†æ•°æ®åˆ—è¡¨
            dtype: ç›®æ ‡æ•°æ®ç±»å‹
        Returns:
            (quantized_weights, scale, zero_point)
        """
        if dtype not in self.supported_dtypes:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {dtype}")

        _, qmin, qmax = self.supported_dtypes[dtype]

        # æ–¹æ³•1: åŸºäºæƒé‡èŒƒå›´
        weight_scale, weight_zero_point = self.calculate_quantization_params(weights, dtype)

        # æ–¹æ³•2: åŸºäºæ ¡å‡†æ•°æ®èŒƒå›´
        all_data = [weights.flatten()] + [data.flatten() for data in calibration_data]
        combined_data = np.concatenate(all_data)
        combined_scale, combined_zero_point = self.calculate_quantization_params(
            combined_data, dtype
        )

        # é€‰æ‹©æ›´åˆé€‚çš„å‚æ•°
        scale = combined_scale
        zero_point = combined_zero_point

        # é‡åŒ–æƒé‡
        quantized_weights = self.quantize_tensor(weights, scale, zero_point, dtype)

        return quantized_weights, scale, zero_point

# ä½¿ç”¨ç¤ºä¾‹
def quantization_demo():
    """é‡åŒ–æ¼”ç¤º"""
    quantizer = QuantizationBasics()

    # åˆ›å»ºç¤ºä¾‹æƒé‡
    weights = np.random.randn(1000).astype(np.float32) * 2 - 1

    print(f"åŸå§‹æƒé‡å½¢çŠ¶: {weights.shape}")
    print(f"åŸå§‹æƒé‡èŒƒå›´: [{weights.min():.4f}, {weights.max():.4f}]")
    print(f"åŸå§‹æƒé‡å¤§å°: {weights.nbytes} bytes")

    # é‡åŒ–æƒé‡
    scale, zero_point = quantizer.calculate_quantization_params(weights, 'int8')
    quantized_weights = quantizer.quantize_tensor(weights, scale, zero_point, 'int8')

    print(f"\né‡åŒ–å‚æ•°:")
    print(f"ç¼©æ”¾å› å­: {scale:.6f}")
    print(f"é›¶ç‚¹: {zero_point}")

    print(f"\né‡åŒ–åæƒé‡å½¢çŠ¶: {quantized_weights.shape}")
    print(f"é‡åŒ–åæƒé‡ç±»å‹: {quantized_weights.dtype}")
    print(f"é‡åŒ–åæƒé‡å¤§å°: {quantized_weights.nbytes} bytes")

    # è®¡ç®—å‹ç¼©æ¯”
    compression_ratio = weights.nbytes / quantized_weights.nbytes
    print(f"\nå‹ç¼©æ¯”: {compression_ratio:.1f}x")

    # åé‡åŒ–
    dequantized_weights = quantizer.dequantize_tensor(quantized_weights, scale, zero_point)

    # è®¡ç®—è¯¯å·®
    mse = np.mean((weights - dequantized_weights) ** 2)
    mae = np.mean(np.abs(weights - dequantized_weights))
    print(f"\né‡åŒ–è¯¯å·®:")
    print(f"å‡æ–¹è¯¯å·® (MSE): {mse:.8f}")
    print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.8f}")
    print(f"ç›¸å¯¹è¯¯å·®: {mae / np.mean(np.abs(weights)) * 100:.4f}%")

# quantization_demo()
```

## ğŸ”§ PTQ (Post-Training Quantization)

### è®­ç»ƒåé‡åŒ–
```python
import torch
import torch.nn as nn
from torch.quantization import quantize_dynamic, quantize, convert, prepare_qat
from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver
from torch.quantization.qconfig import QConfig
import time

class PTQProcessor:
    """è®­ç»ƒåé‡åŒ–å¤„ç†å™¨"""

    def __init__(self):
        self.supported_modules = {
            nn.Linear: 'linear',
            nn.Conv1d: 'conv1d',
            nn.Conv2d: 'conv2d',
            nn.LSTM: 'lstm',
            nn.GRU: 'gru'
        }
        self.supported_dtypes = [torch.qint8, torch.float16]

    def dynamic_quantization(self, model: nn.Module,
                           dtype: torch.dtype = torch.qint8,
                           module_types: List[str] = None) -> nn.Module:
        """
        åŠ¨æ€é‡åŒ–ï¼ˆæ¨èç”¨äºå¤§æ¨¡å‹ï¼‰
        Args:
            model: è¾“å…¥æ¨¡å‹
            dtype: é‡åŒ–æ•°æ®ç±»å‹
            module_types: è¦é‡åŒ–çš„æ¨¡å—ç±»å‹åˆ—è¡¨
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        print("ğŸš€ å¼€å§‹åŠ¨æ€é‡åŒ–...")
        start_time = time.time()

        # è®¾ç½®é»˜è®¤çš„æ¨¡å—ç±»å‹
        if module_types is None:
            module_types = ['linear', 'conv2d', 'lstm', 'gru']

        # é‡åŒ–æ¨¡å‹
        quantized_model = quantize_dynamic(
            model,
            module_types,
            dtype=dtype
        )

        end_time = time.time()
        print(f"âœ… åŠ¨æ€é‡åŒ–å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

        return quantized_model

    def static_quantization(self, model: nn.Module,
                         calibration_data: torch.utils.data.DataLoader,
                         dtype: torch.dtype = torch.qint8) -> nn.Module:
        """
        é™æ€é‡åŒ–ï¼ˆéœ€è¦æ ¡å‡†æ•°æ®ï¼‰
        Args:
            model: è¾“å…¥æ¨¡å‹
            calibration_data: æ ¡å‡†æ•°æ®åŠ è½½å™¨
            dtype: é‡åŒ–æ•°æ®ç±»å‹
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        print("ğŸš€ å¼€å§‹é™æ€é‡åŒ–...")
        start_time = time.time()

        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()

        # å‡†å¤‡é‡åŒ–é…ç½®
        model.qconfig = QConfig(
            activation=MinMaxObserver.with_args(dtype=torch.quint8),
            weight=MinMaxObserver.with_args(dtype=torch.qint8)
        )

        # å‡†å¤‡æ¨¡å‹
        prepare_model = prepare(model)

        # æ ¡å‡†
        print("ğŸ“Š å¼€å§‹æ ¡å‡†...")
        self._calibrate_model(prepare_model, calibration_data)

        # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        quantized_model = convert(prepare_model, inplace=False)

        end_time = time.time()
        print(f"âœ… é™æ€é‡åŒ–å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")

        return quantized_model

    def _calibrate_model(self, model: nn.Module,
                       calibration_data: torch.utils.data.DataLoader):
        """æ ¡å‡†æ¨¡å‹"""
        model.eval()
        with torch.no_grad():
            for data, _ in calibration_data:
                # ç§»åŠ¨åˆ°æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
                device = next(model.parameters()).device
                if isinstance(data, (list, tuple)):
                    data = [d.to(device) if hasattr(d, 'to') else d for d in data]
                else:
                    data = data.to(device)

                try:
                    _ = model(data)
                except Exception as e:
                    print(f"æ ¡å‡†è¿‡ç¨‹ä¸­çš„è­¦å‘Š: {e}")
                    continue

                # åªéœ€è¦ä¸€éƒ¨åˆ†æ•°æ®è¿›è¡Œæ ¡å‡†
                break

    def compare_models(self, original_model: nn.Module,
                      quantized_model: nn.Module,
                      test_data: torch.utils.data.DataLoader) -> Dict[str, Any]:
        """
        æ¯”è¾ƒåŸå§‹æ¨¡å‹å’Œé‡åŒ–æ¨¡å‹çš„æ€§èƒ½
        """
        print("ğŸ“Š å¼€å§‹æ¨¡å‹æ¯”è¾ƒ...")

        original_model.eval()
        quantized_model.eval()

        # è®¡ç®—æ¨¡å‹å¤§å°
        original_size = self._get_model_size(original_model)
        quantized_size = self._get_model_size(quantized_model)
        size_reduction = (original_size - quantized_size) / original_size * 100

        # æ¨ç†é€Ÿåº¦æµ‹è¯•
        original_time, original_acc = self._measure_performance(
            original_model, test_data
        )
        quantized_time, quantized_acc = self._measure_performance(
            quantized_model, test_data
        )

        speed_up = original_time / quantized_time if quantized_time > 0 else float('inf')
        acc_diff = (original_acc - quantized_acc) / original_acc * 100 if original_acc > 0 else 0

        results = {
            'original_size_mb': original_size / 1024 / 1024,
            'quantized_size_mb': quantized_size / 1024 / 1024,
            'size_reduction_percent': size_reduction,
            'original_time_ms': original_time * 1000,
            'quantized_time_ms': quantized_time * 1000,
            'speed_up': speed_up,
            'original_accuracy': original_acc,
            'quantized_accuracy': quantized_acc,
            'accuracy_drop_percent': acc_diff
        }

        print(f"ğŸ“Š æ¯”è¾ƒç»“æœ:")
        print(f"  æ¨¡å‹å¤§å°: {original_size / 1024 / 1024:.2f}MB â†’ {quantized_size / 1024 / 1024:.2f}MB")
        print(f"  å¤§å°å‡å°‘: {size_reduction:.1f}%")
        print(f"  æ¨ç†æ—¶é—´: {original_time * 1000:.2f}ms â†’ {quantized_time * 1000:.2f}ms")
        print(f"  é€Ÿåº¦æå‡: {speed_up:.1f}x")
        print(f"  å‡†ç¡®ç‡: {original_acc:.4f} â†’ {quantized_acc:.4f}")
        print(f"  å‡†ç¡®ç‡ä¸‹é™: {acc_diff:.2f}%")

        return results

    def _get_model_size(self, model: nn.Module) -> int:
        """è·å–æ¨¡å‹å¤§å°ï¼ˆå­—èŠ‚ï¼‰"""
        param_size = 0
        buffer_size = 0

        for param in model.parameters():
            if param.requires_grad:
                param_size += param.nelement() * param.element_size()

        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()

        return param_size + buffer_size

    def _measure_performance(self, model: nn.Module,
                           test_data: torch.utils.data.DataLoader) -> Tuple[float, float]:
        """æµ‹é‡æ¨¡å‹æ€§èƒ½ï¼ˆæ—¶é—´å’Œå‡†ç¡®ç‡ï¼‰"""
        device = next(model.parameters()).device

        total_time = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in test_data:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                if isinstance(data, (list, tuple)):
                    data = [d.to(device) if hasattr(d, 'to') else d for d in data]
                else:
                    data = data.to(device)
                target = target.to(device)

                # æµ‹é‡æ¨ç†æ—¶é—´
                start_time = time.time()
                output = model(data)
                end_time = time.time()
                total_time += (end_time - start_time)

                # è®¡ç®—å‡†ç¡®ç‡
                if hasattr(output, 'logits'):
                    pred = output.logits.argmax(dim=1)
                elif output.dim() > 1 and output.size(1) > 1:
                    pred = output.argmax(dim=1)
                else:
                    pred = output.round()

                correct += (pred == target).sum().item()
                total += target.size(0)

        avg_time = total_time / len(test_data)
        accuracy = correct / total if total > 0 else 0

        return avg_time, accuracy

class CustomQuantizer:
    """è‡ªå®šä¹‰é‡åŒ–å™¨"""

    def __init__(self):
        self.scale_dict = {}
        self.zero_point_dict = {}

    def quantize_layer(self, layer: nn.Module, layer_name: str,
                      calibrate_data: torch.Tensor = None) -> None:
        """
        é‡åŒ–å•ä¸ªå±‚
        """
        if isinstance(layer, nn.Linear):
            self._quantize_linear(layer, layer_name, calibrate_data)
        elif isinstance(layer, nn.Conv2d):
            self._quantize_conv2d(layer, layer_name, calibrate_data)

    def _quantize_linear(self, layer: nn.Linear, layer_name: str,
                        calibrate_data: torch.Tensor = None):
        """é‡åŒ–çº¿æ€§å±‚"""
        # é‡åŒ–æƒé‡
        weight = layer.weight.data
        scale, zero_point = self._calculate_quant_params(weight)

        # å­˜å‚¨é‡åŒ–å‚æ•°
        self.scale_dict[f"{layer_name}.weight"] = scale
        self.zero_point_dict[f"{layer_name}.weight"] = zero_point

        # é‡åŒ–æƒé‡
        quantized_weight = torch.quantize_per_tensor(
            weight, torch.qint8, scale=scale, zero_point=zero_point
        )
        layer.weight.data = quantized_weight.dequantize()

    def _quantize_conv2d(self, layer: nn.Conv2d, layer_name: str,
                         calibrate_data: torch.Tensor = None):
        """é‡åŒ–å·ç§¯å±‚"""
        weight = layer.weight.data
        scale, zero_point = self._calculate_quant_params(weight)

        self.scale_dict[f"{layer_name}.weight"] = scale
        self.zero_point_dict[f"{layer_name}.weight"] = zero_point

        quantized_weight = torch.quantize_per_tensor(
            weight, torch.qint8, scale=scale, zero_point=zero_point
        )
        layer.weight.data = quantized_weight.dequantize()

    def _calculate_quant_params(self, tensor: torch.Tensor) -> Tuple[float, int]:
        """è®¡ç®—é‡åŒ–å‚æ•°"""
        qmin, qmax = 0, 255  # uint8èŒƒå›´

        rmin, rmax = tensor.min().item(), tensor.max().item()

        if rmax - rmin < 1e-8:
            scale = 1.0
            zero_point = (qmin + qmax) // 2
        else:
            scale = (rmax - rmin) / (qmax - qmin)
            zero_point = int(qmin - rmin / scale)

        return float(scale), max(qmin, min(qmax, zero_point))
```

## ğŸ”§ QAT (Quantization-Aware Training)

### é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
```python
import torch
import torch.nn as nn
from torch.quantization import prepare_qat, convert
import torch.nn.functional as F

class QATProcessor:
    """é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå¤„ç†å™¨"""

    def __init__(self):
        self.supported_dtypes = [torch.qint8]

    def prepare_qat_model(self, model: nn.Module,
                         optimizer: torch.optim.Optimizer,
                         dtype: torch.dtype = torch.qint8) -> Tuple[nn.Module, torch.optim.Optimizer]:
        """
        å‡†å¤‡é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
        Args:
            model: è¾“å…¥æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            dtype: é‡åŒ–æ•°æ®ç±»å‹
        Returns:
            (prepared_model, updated_optimizer)
        """
        print("ğŸš€ å‡†å¤‡é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ...")

        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()

        # è®¾ç½®é‡åŒ–é…ç½®
        model.qconfig = torch.quantization.get_default_qat_qconfig(dtype)

        # å‡†å¤‡æ¨¡å‹
        prepared_model = prepare_qat(model, inplace=False)

        # åˆ›å»ºæ–°çš„ä¼˜åŒ–å™¨ï¼ˆå› ä¸ºæ¨¡å‹å‚æ•°è¢«ä¿®æ”¹äº†ï¼‰
        updated_optimizer = torch.optim.SGD(
            prepared_model.parameters(),
            lr=optimizer.param_groups[0]['lr'],
            momentum=optimizer.param_groups[0].get('momentum', 0),
            weight_decay=optimizer.param_groups[0].get('weight_decay', 0)
        )

        return prepared_model, updated_optimizer

    def train_with_qat(self, model: nn.Module, optimizer: torch.optim.Optimizer,
                      train_loader: torch.utils.data.DataLoader,
                      val_loader: torch.utils.data.DataLoader,
                      epochs: int = 10) -> Tuple[nn.Module, Dict[str, List[float]]]:
        """
        é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
        """
        print(f"ğŸ‹ï¸ å¼€å§‹é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œepochs: {epochs}")

        training_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }

        criterion = nn.CrossEntropyLoss()

        for epoch in range(epochs):
            print(f"\nEpoch {epoch + 1}/{epochs}")

            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self._train_epoch(
                model, optimizer, train_loader, criterion
            )

            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self._validate_epoch(
                model, val_loader, criterion
            )

            # è®°å½•å†å²
            training_history['train_loss'].append(train_loss)
            training_history['train_acc'].append(train_acc)
            training_history['val_loss'].append(val_loss)
            training_history['val_acc'].append(val_acc)

            print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
            print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

        return model, training_history

    def _train_epoch(self, model: nn.Module, optimizer: torch.optim.Optimizer,
                    train_loader: torch.utils.data.DataLoader,
                    criterion: nn.Module) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            device = next(model.parameters()).device
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            if output.dim() > 1 and output.size(1) > 1:
                pred = output.argmax(dim=1)
            else:
                pred = output.round()

            correct += (pred == target).sum().item()
            total += target.size(0)

        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total if total > 0 else 0

        return avg_loss, accuracy

    def _validate_epoch(self, model: nn.Module, val_loader: torch.utils.data.DataLoader,
                        criterion: nn.Module) -> Tuple[float, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in val_loader:
                device = next(model.parameters()).device
                data, target = data.to(device), target.to(device)

                output = model(data)
                loss = criterion(output, target)
                total_loss += loss.item()

                if output.dim() > 1 and output.size(1) > 1:
                    pred = output.argmax(dim=1)
                else:
                    pred = output.round()

                correct += (pred == target).sum().item()
                total += target.size(0)

        avg_loss = total_loss / len(val_loader)
        accuracy = correct / total if total > 0 else 0

        return avg_loss, accuracy

    def convert_to_quantized(self, trained_model: nn.Module) -> nn.Module:
        """å°†è®­ç»ƒå¥½çš„QATæ¨¡å‹è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹"""
        print("ğŸ”„ è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹...")

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        trained_model.eval()

        # è½¬æ¢
        quantized_model = convert(trained_model, inplace=False)

        return quantized_model

class QuantizedLayer(nn.Module):
    """è‡ªå®šä¹‰é‡åŒ–å±‚"""

    def __init__(self, base_layer: nn.Module, quantized: bool = False):
        super().__init__()
        self.base_layer = base_layer
        self.quantized = quantized
        self.scale = None
        self.zero_point = None

    def enable_quantization(self):
        """å¯ç”¨é‡åŒ–"""
        self.quantized = True
        self._calculate_quant_params()

    def disable_quantization(self):
        """ç¦ç”¨é‡åŒ–"""
        self.quantized = False

    def _calculate_quant_params(self):
        """è®¡ç®—é‡åŒ–å‚æ•°"""
        weight = self.base_layer.weight.data
        qmin, qmax = -128, 127  # int8èŒƒå›´

        rmin, rmax = weight.min().item(), weight.max().item()

        if rmax - rmin < 1e-8:
            self.scale = 1.0
            self.zero_point = 0
        else:
            self.scale = (rmax - rmin) / (qmax - qmin)
            self.zero_point = int(qmin - rmin / self.scale)
            self.zero_point = max(qmin, min(qmax, self.zero_point))

    def forward(self, x):
        if self.quantized and self.scale is not None:
            # é‡åŒ–è¾“å…¥å’Œæƒé‡
            x_quantized = torch.quantize_per_tensor(
                x, torch.qint8, scale=self.scale, zero_point=self.zero_point
            )

            # æ‰§è¡Œåé‡åŒ–çš„å‰å‘ä¼ æ’­
            x_dequantized = x_quantized.dequantize()
            return self.base_layer(x_dequantized)
        else:
            return self.base_layer(x)

class QuantizedLinear(QuantizedLayer):
    """é‡åŒ–çº¿æ€§å±‚"""

    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        base_layer = nn.Linear(in_features, out_features, bias=bias)
        super().__init__(base_layer)

    def forward(self, x):
        if self.quantized and self.scale is not None:
            # è‡ªå®šä¹‰é‡åŒ–é€»è¾‘
            weight_quantized = torch.quantize_per_tensor(
                self.base_layer.weight, torch.qint8,
                scale=self.scale, zero_point=self.zero_point
            )

            # åˆ›å»ºé‡åŒ–åçš„å·ç§¯æ“ä½œ
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä½¿ç”¨ä¸“é—¨çš„é‡åŒ–æ“ä½œ
            return F.linear(x, weight_quantized.dequantize(), self.base_layer.bias)
        else:
            return super().forward(x)

class QuantizedConv2d(QuantizedLayer):
    """é‡åŒ–å·ç§¯å±‚"""

    def __init__(self, in_channels: int, out_channels: int,
                 kernel_size: int, stride: int = 1, padding: int = 0, bias: bool = True):
        base_layer = nn.Conv2d(in_channels, out_channels, kernel_size,
                              stride=stride, padding=padding, bias=bias)
        super().__init__(base_layer)

    def forward(self, x):
        if self.quantized and self.scale is not None:
            weight_quantized = torch.quantize_per_tensor(
                self.base_layer.weight, torch.qint8,
                scale=self.scale, zero_point=self.zero_point
            )

            return F.conv2d(x, weight_quantized.dequantize(),
                           self.base_layer.bias, self.base_layer.stride,
                           self.base_layer.padding)
        else:
            return super().forward(x)
```

## ğŸš€ å®é™…åº”ç”¨ç¤ºä¾‹

### æ¨¡å‹é‡åŒ–å®Œæ•´æµç¨‹
```python
import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torch.nn.functional as F
import copy

class ModelQuantizationPipeline:
    """æ¨¡å‹é‡åŒ–æµæ°´çº¿"""

    def __init__(self):
        self.ptq_processor = PTQProcessor()
        self.qat_processor = QATProcessor()

    def create_sample_model(self) -> nn.Module:
        """åˆ›å»ºç¤ºä¾‹æ¨¡å‹"""
        class SampleNet(nn.Module):
            def __init__(self, num_classes: int = 10):
                super().__init__()
                self.conv1 = nn.Conv2d(1, 32, 3, 1)
                self.conv2 = nn.Conv2d(32, 64, 3, 1)
                self.dropout1 = nn.Dropout(0.25)
                self.dropout2 = nn.Dropout(0.5)
                self.fc1 = nn.Linear(9216, 128)
                self.fc2 = nn.Linear(128, num_classes)

            def forward(self, x):
                x = self.conv1(x)
                x = F.relu(x)
                x = self.conv2(x)
                x = F.relu(x)
                x = F.max_pool2d(x, 2)
                x = self.dropout1(x)
                x = torch.flatten(x, 1)
                x = self.fc1(x)
                x = F.relu(x)
                x = self.dropout2(x)
                x = self.fc2(x)
                output = F.log_softmax(x, dim=1)
                return output

        return SampleNet()

    def prepare_data(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader]:
        """å‡†å¤‡æ•°æ®"""
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])

        # è®­ç»ƒæ•°æ®
        train_dataset = torchvision.datasets.MNIST(
            './data', train=True, download=True, transform=transform
        )
        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True
        )

        # æµ‹è¯•æ•°æ®
        test_dataset = torchvision.datasets.MNIST(
            './data', train=False, download=True, transform=transform
        )
        test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False
        )

        return train_loader, test_loader

    def full_quantization_demo(self):
        """å®Œæ•´çš„é‡åŒ–æ¼”ç¤º"""
        print("ğŸš€ å¼€å§‹å®Œæ•´æ¨¡å‹é‡åŒ–æ¼”ç¤º")
        print("=" * 60)

        # 1. å‡†å¤‡æ•°æ®å’Œæ¨¡å‹
        print("\nğŸ“Š å‡†å¤‡æ•°æ®å’Œæ¨¡å‹...")
        train_loader, test_loader = self.prepare_data()
        model = self.create_sample_model()

        # 2. è®­ç»ƒåŸå§‹æ¨¡å‹
        print("\nğŸ‹ï¸ è®­ç»ƒåŸå§‹æ¨¡å‹...")
        trained_model = self._train_model(model, train_loader, test_loader)

        # 3. åŠ¨æ€é‡åŒ–
        print("\nğŸ”„ åŠ¨æ€é‡åŒ–...")
        dynamic_quantized = self.ptq_processor.dynamic_quantization(
            copy.deepcopy(trained_model)
        )

        # 4. é™æ€é‡åŒ–
        print("\nâš™ï¸ é™æ€é‡åŒ–...")
        static_quantized = self.ptq_processor.static_quantization(
            copy.deepcopy(trained_model), train_loader
        )

        # 5. é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
        print("\nğŸ¯ é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ...")
        qat_model, qat_optimizer = self.qat_processor.prepare_qat_model(
            copy.deepcopy(trained_model),
            torch.optim.Adam(trained_model.parameters(), lr=0.001)
        )

        qat_model, qat_history = self.qat_processor.train_with_qat(
            qat_model, qat_optimizer, train_loader, test_loader, epochs=5
        )

        qat_quantized = self.qat_processor.convert_to_quantized(qat_model)

        # 6. æ€§èƒ½æ¯”è¾ƒ
        print("\nğŸ“Š æ€§èƒ½æ¯”è¾ƒ")
        print("=" * 60)

        models = [
            ("åŸå§‹æ¨¡å‹", trained_model),
            ("åŠ¨æ€é‡åŒ–", dynamic_quantized),
            ("é™æ€é‡åŒ–", static_quantized),
            ("QATé‡åŒ–", qat_quantized)
        ]

        for name, quantized_model in models:
            print(f"\nğŸ” æµ‹è¯• {name}")
            print("-" * 40)

            # è®¡ç®—æ¨¡å‹å¤§å°
            original_size = self._get_model_size(trained_model)
            quantized_size = self._get_model_size(quantized_model)
            size_reduction = (original_size - quantized_size) / original_size * 100

            # æµ‹è¯•æ€§èƒ½
            device = next(trained_model.parameters()).device
            quantized_model.eval()
            correct = 0
            total = 0
            total_time = 0

            with torch.no_grad():
                for data, target in test_loader:
                    data, target = data.to(device), target.to(device)

                    start_time = time.time()
                    output = quantized_model(data)
                    end_time = time.time()

                    total_time += (end_time - start_time)
                    pred = output.argmax(dim=1)
                    correct += (pred == target).sum().item()
                    total += target.size(0)

            accuracy = correct / total
            avg_time = total_time / len(test_loader) * 1000  # ms

            print(f"æ¨¡å‹å¤§å°: {quantized_size / 1024 / 1024:.2f}MB")
            print(f"å¤§å°å‡å°‘: {size_reduction:.1f}%")
            print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}ms")
            print(f"å‡†ç¡®ç‡: {accuracy:.4f}")

    def _train_model(self, model: nn.Module, train_loader: DataLoader,
                    test_loader: DataLoader, epochs: int = 5) -> nn.Module:
        """è®­ç»ƒæ¨¡å‹"""
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)

        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()

        model.train()
        for epoch in range(epochs):
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(device), target.to(device)

                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()

            if epoch % 1 == 0:
                model.eval()
                test_loss = 0
                correct = 0
                with torch.no_grad():
                    for data, target in test_loader:
                        data, target = data.to(device), target.to(device)
                        output = model(data)
                        test_loss += criterion(output, target).item()
                        pred = output.argmax(dim=1)
                        correct += (pred == target).sum().item()

                test_loss /= len(test_loader)
                accuracy = correct / len(test_loader.dataset)
                print(f'Epoch {epoch}: Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')
                model.train()

        return model

    def _get_model_size(self, model: nn.Module) -> int:
        """è·å–æ¨¡å‹å¤§å°"""
        param_size = 0
        buffer_size = 0

        for param in model.parameters():
            param_size += param.nelement() * param.element_size()

        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()

        return param_size + buffer_size

# è¿è¡Œå®Œæ•´æ¼”ç¤º
def main():
    """ä¸»å‡½æ•°"""
    pipeline = ModelQuantizationPipeline()
    pipeline.full_quantization_demo()

if __name__ == "__main__":
    main()
```

## ğŸ“Š é‡åŒ–æ•ˆæœåˆ†æ

### é‡åŒ–è´¨é‡è¯„ä¼°
```python
class QuantizationAnalyzer:
    """é‡åŒ–æ•ˆæœåˆ†æå™¨"""

    def __init__(self):
        self.metrics = {}

    def analyze_quantization_quality(self, original_model: nn.Module,
                                   quantized_model: nn.Module,
                                   test_data: DataLoader) -> Dict[str, Any]:
        """åˆ†æé‡åŒ–è´¨é‡"""
        print("ğŸ“Š åˆ†æé‡åŒ–è´¨é‡...")

        # åŸºç¡€æ€§èƒ½æŒ‡æ ‡
        performance_metrics = self._measure_performance(
            original_model, quantized_model, test_data
        )

        # ç²¾åº¦æŸå¤±åˆ†æ
        precision_metrics = self._analyze_precision_loss(
            original_model, quantized_model, test_data
        )

        # å±‚çº§åˆ†æ
        layer_metrics = self._analyze_layerwise_impact(
            original_model, quantized_model
        )

        # ç»¼åˆåˆ†æ
        analysis_results = {
            'performance': performance_metrics,
            'precision': precision_metrics,
            'layer_analysis': layer_metrics,
            'overall_quality': self._calculate_overall_quality(
                performance_metrics, precision_metrics
            )
        }

        self._print_analysis_results(analysis_results)
        return analysis_results

    def _measure_performance(self, original_model: nn.Module,
                           quantized_model: nn.Module,
                           test_data: DataLoader) -> Dict[str, Any]:
        """æµ‹é‡æ€§èƒ½æŒ‡æ ‡"""
        device = next(original_model.parameters()).device

        # åŸå§‹æ¨¡å‹æ€§èƒ½
        original_model.eval()
        orig_accuracy, orig_inference_time = self._benchmark_model(
            original_model, test_data, device
        )

        # é‡åŒ–æ¨¡å‹æ€§èƒ½
        quantized_model.eval()
        quant_accuracy, quant_inference_time = self._benchmark_model(
            quantized_model, test_data, device
        )

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        accuracy_drop = (orig_accuracy - quant_accuracy) / orig_accuracy * 100
        speedup = orig_inference_time / quant_inference_time

        return {
            'original_accuracy': orig_accuracy,
            'quantized_accuracy': quant_accuracy,
            'accuracy_drop_percent': accuracy_drop,
            'original_inference_time_ms': orig_inference_time * 1000,
            'quantized_inference_time_ms': quant_inference_time * 1000,
            'speedup_factor': speedup
        }

    def _benchmark_model(self, model: nn.Module, test_data: DataLoader,
                        device: torch.device) -> Tuple[float, float]:
        """åŸºå‡†æµ‹è¯•æ¨¡å‹"""
        correct = 0
        total = 0
        total_time = 0

        with torch.no_grad():
            for data, target in test_data:
                data, target = data.to(device), target.to(device)

                start_time = time.time()
                output = model(data)
                end_time = time.time()

                total_time += (end_time - start_time)

                pred = output.argmax(dim=1)
                correct += (pred == target).sum().item()
                total += target.size(0)

        accuracy = correct / total
        avg_inference_time = total_time / len(test_data)

        return accuracy, avg_inference_time

    def _analyze_precision_loss(self, original_model: nn.Module,
                                quantized_model: nn.Module,
                                test_data: DataLoader) -> Dict[str, Any]:
        """åˆ†æç²¾åº¦æŸå¤±"""
        device = next(original_model.parameters()).device
        original_model.eval()
        quantized_model.eval()

        # æ”¶é›†è¾“å‡ºå·®å¼‚
        output_differences = []
        layer_differences = {}

        with torch.no_grad():
            for data, _ in test_data:
                data = data.to(device)

                # è·å–åŸå§‹è¾“å‡º
                orig_output = original_model(data)

                # è·å–é‡åŒ–è¾“å‡º
                quant_output = quantized_model(data)

                # è®¡ç®—è¾“å‡ºå·®å¼‚
                output_diff = F.mse_loss(orig_output, quant_output).item()
                output_differences.append(output_diff)

                # åªåˆ†æå‰å‡ ä¸ªbatch
                if len(output_differences) >= 10:
                    break

        # è®¡ç®—ç²¾åº¦æŸå¤±æŒ‡æ ‡
        avg_output_diff = sum(output_differences) / len(output_differences)
        max_output_diff = max(output_differences)

        return {
            'average_output_mse': avg_output_diff,
            'max_output_mse': max_output_diff,
            'output_variance': np.var(output_differences),
            'num_samples_analyzed': len(output_differences)
        }

    def _analyze_layerwise_impact(self, original_model: nn.Module,
                                 quantized_model: nn.Module) -> Dict[str, Any]:
        """åˆ†æå±‚çº§å½±å“"""
        layer_analysis = {}

        # è·å–æ¨¡å‹å‚æ•°
        orig_params = dict(original_model.named_parameters())
        quant_params = dict(quantized_model.named_parameters())

        for name in orig_params:
            if name in quant_params:
                orig_param = orig_params[name]
                quant_param = quant_params[name]

                # è®¡ç®—å‚æ•°å·®å¼‚
                param_diff = F.mse_loss(orig_param, quant_param).item()

                # è®¡ç®—å‹ç¼©æ¯”
                orig_size = orig_param.nelement() * orig_param.element_size()
                quant_size = quant_param.nelement() * quant_param.element_size()
                compression_ratio = orig_size / quant_size if quant_size > 0 else 1

                layer_analysis[name] = {
                    'parameter_mse': param_diff,
                    'compression_ratio': compression_ratio,
                    'original_shape': list(orig_param.shape),
                    'quantized_shape': list(quant_param.shape)
                }

        return layer_analysis

    def _calculate_overall_quality(self, performance: Dict[str, Any],
                                  precision: Dict[str, Any]) -> str:
        """è®¡ç®—æ•´ä½“è´¨é‡ç­‰çº§"""
        accuracy_drop = performance.get('accuracy_drop_percent', 0)
        speedup = performance.get('speedup_factor', 1)
        output_mse = precision.get('average_output_mse', 0)

        # è´¨é‡è¯„åˆ†
        if accuracy_drop < 1 and speedup > 2 and output_mse < 0.01:
            return "ä¼˜ç§€"
        elif accuracy_drop < 2 and speedup > 1.5 and output_mse < 0.05:
            return "è‰¯å¥½"
        elif accuracy_drop < 5 and speedup > 1.2 and output_mse < 0.1:
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦æ”¹è¿›"

    def _print_analysis_results(self, results: Dict[str, Any]):
        """æ‰“å°åˆ†æç»“æœ"""
        print("\nğŸ“Š é‡åŒ–è´¨é‡åˆ†æç»“æœ")
        print("=" * 50)

        # æ€§èƒ½åˆ†æ
        perf = results['performance']
        print(f"\nğŸš€ æ€§èƒ½åˆ†æ:")
        print(f"  å‡†ç¡®ç‡å˜åŒ–: {perf['accuracy_drop_percent']:.2f}%")
        print(f"  é€Ÿåº¦æå‡: {perf['speedup_factor']:.2f}x")
        print(f"  æ¨ç†æ—¶é—´: {perf['quantized_inference_time_ms']:.2f}ms")

        # ç²¾åº¦åˆ†æ
        prec = results['precision']
        print(f"\nğŸ“ˆ ç²¾åº¦åˆ†æ:")
        print(f"  è¾“å‡ºMSE: {prec['average_output_mse']:.6f}")
        print(f"  æœ€å¤§MSE: {prec['max_output_mse']:.6f}")

        # æ•´ä½“è´¨é‡
        print(f"\nâ­ æ•´ä½“è´¨é‡: {results['overall_quality']}")

        # å±‚çº§åˆ†æ
        print(f"\nğŸ“‹ å±‚çº§åˆ†æ:")
        for layer_name, analysis in results['layer_analysis'].items():
            print(f"  {layer_name}:")
            print(f"    å‚æ•°MSE: {analysis['parameter_mse']:.6f}")
            print(f"    å‹ç¼©æ¯”: {analysis['compression_ratio']:.2f}x")

# ä½¿ç”¨ç¤ºä¾‹
def quantization_analysis_demo():
    """é‡åŒ–åˆ†ææ¼”ç¤º"""
    # è¿™é‡Œéœ€è¦é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹
    # original_model = load_trained_model()
    # quantized_model = load_quantized_model()
    # test_data = load_test_data()

    analyzer = QuantizationAnalyzer()

    # results = analyzer.analyze_quantization_quality(
    #     original_model, quantized_model, test_data
    # )

    print("é‡åŒ–åˆ†æå®Œæˆ")

# quantization_analysis_demo()
```

## ğŸ“ æ€»ç»“

æ¨¡å‹é‡åŒ–æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ä¼˜åŒ–çš„é‡è¦æŠ€æœ¯ï¼Œæœ¬æ–‡æ¡£ä»‹ç»äº†é‡åŒ–çš„åŸç†ã€æ–¹æ³•å’Œå®é™…åº”ç”¨ã€‚

### ğŸ¯ å…³é”®è¦ç‚¹
- **é‡åŒ–åŸç†**: æµ®ç‚¹åˆ°æ•´æ•°çš„æ˜ å°„å’Œå‹ç¼©
- **PTQ**: è®­ç»ƒåé‡åŒ–çš„å¿«é€Ÿéƒ¨ç½²æ–¹æ¡ˆ
- **QAT**: é‡åŒ–æ„ŸçŸ¥è®­ç»ƒçš„ç²¾åº¦ä¿æŒæ–¹æ¡ˆ
- **æ€§èƒ½åˆ†æ**: é‡åŒ–æ•ˆæœçš„å…¨é¢è¯„ä¼°

### ğŸš€ å®ç°ç‰¹è‰²
- **å¤šç§ç­–ç•¥**: åŠ¨æ€é‡åŒ–ã€é™æ€é‡åŒ–ã€QATç­‰
- **å®Œæ•´æµç¨‹**: ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„é‡åŒ–pipeline
- **è´¨é‡åˆ†æ**: æ·±å…¥çš„é‡åŒ–æ•ˆæœè¯„ä¼°
- **å®é™…åº”ç”¨**: åŸºäºçœŸå®æ¨¡å‹çš„å®Œæ•´ç¤ºä¾‹

### ğŸ”„ ä¸‹ä¸€æ­¥
- å­¦ä¹ [æ¨ç†ä¼˜åŒ–](02-æ¨ç†ä¼˜åŒ–.md)
- äº†è§£[æœåŠ¡åŒ–éƒ¨ç½²](03-æœåŠ¡åŒ–éƒ¨ç½².md)
- æŒæ¡[æ€§èƒ½ç›‘æ§](04-æ€§èƒ½ç›‘æ§.md)
- æ¢ç´¢[Agentæ€§èƒ½ä¼˜åŒ–](../agents/05-ä¸Šä¸‹æ–‡ç®¡ç†.md)

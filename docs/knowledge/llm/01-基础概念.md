# ğŸ¤– å¤§è¯­è¨€æ¨¡å‹(LLM)åŸºç¡€æ¦‚å¿µ

## ğŸ“š æ¦‚è¿°

å¤§è¯­è¨€æ¨¡å‹æ˜¯åŸºäºTransformeræ¶æ„çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼ŒæŒæ¡äº†ä¸°å¯Œçš„è¯­è¨€çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡æ¡£ä»‹ç»LLMçš„åŸºç¡€æ¦‚å¿µã€æ ¸å¿ƒæŠ€æœ¯å’Œåº”ç”¨æ–¹æ³•ã€‚

## ğŸ¯ LLMåŸºæœ¬æ¦‚å¿µ

### ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹
å¤§è¯­è¨€æ¨¡å‹æ˜¯æŒ‡å…·æœ‰å¤§é‡å‚æ•°ï¼ˆé€šå¸¸æ•°åäº¿åˆ°æ•°ä¸‡äº¿ï¼‰çš„ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿï¼š
- **ç†è§£è‡ªç„¶è¯­è¨€**: è§£æå’Œè§£é‡Šäººç±»è¯­è¨€
- **ç”Ÿæˆè¿è´¯æ–‡æœ¬**: äº§ç”Ÿç¬¦åˆè¯­æ³•å’Œè¯­ä¹‰çš„æ–‡æœ¬
- **æ‰§è¡Œå¤šç§ä»»åŠ¡**: æ— éœ€ç‰¹å®šè®­ç»ƒå³å¯æ‰§è¡Œå¤šç§NLPä»»åŠ¡
- **ä¸Šä¸‹æ–‡ç†è§£**: åŸºäºå¯¹è¯å†å²è¿›è¡Œè¿è´¯äº¤äº’

### LLMå‘å±•å†ç¨‹
```python
# LLMå‘å±•æ—¶é—´çº¿
llm_timeline = {
    "2017": {
        "æ¨¡å‹": "Transformer",
        "è®ºæ–‡": "Attention Is All You Need",
        "çªç ´": "è‡ªæ³¨æ„åŠ›æœºåˆ¶",
        "å‚æ•°": "çº¦1äº¿"
    },
    "2018": {
        "model": "BERT",
        "è®ºæ–‡": "BERT: Pre-training of Deep Bidirectional Transformers",
        "çªç ´": "åŒå‘é¢„è®­ç»ƒ",
        "å‚æ•°": "1.1äº¿"
    },
    "2019": {
        "model": "GPT-2",
        "è®ºæ–‡": "Language Models are Unsupervised Multitask Learners",
        "çªç ´": "å¤§è§„æ¨¡ç”Ÿæˆèƒ½åŠ›",
        "å‚æ•°": "15äº¿"
    },
    "2020": {
        "model": "GPT-3",
        "è®ºæ–‡": "Language Models are Few-Shot Learners",
        "çªç ´": "in-context learning",
        "å‚æ•°": "1750äº¿"
    },
    "2022": {
        "model": "ChatGPT",
        "è®ºæ–‡": "Training language models to follow instructions with human feedback",
        "çªç ´": "æŒ‡ä»¤å¾®è°ƒå’ŒRLHF",
        "å‚æ•°": "1750äº¿"
    },
    "2023": {
        "model": "GPT-4",
        "è®ºæ–‡": "GPT-4 Technical Report",
        "çªç ´": "å¤šæ¨¡æ€å’Œæ¨ç†èƒ½åŠ›",
        "å‚æ•°": "ä¼°è®¡1.76ä¸‡äº¿"
    }
}

for year, info in llm_timeline.items():
    print(f"{year}: {info['model']} - {info['çªç ´']} ({info['å‚æ•°']}å‚æ•°)")
```

### æ¨¡å‹åˆ†ç±»
```python
class LLMClassification:
    """LLMåˆ†ç±»ä½“ç³»"""

    @staticmethod
    def by_architecture():
        """æŒ‰æ¶æ„åˆ†ç±»"""
        return {
            "Transformerç¼–ç å™¨": ["BERT", "RoBERTa", "ALBERT"],
            "Transformerè§£ç å™¨": ["GPTç³»åˆ—", "OPT", "BLOOM"],
            "ç¼–ç å™¨-è§£ç å™¨": ["T5", "BART", "Flan-T5"],
            "æ··åˆæ¶æ„": ["GLM", "Switch Transformer"]
        }

    @staticmethod
    def by_scale():
        """æŒ‰è§„æ¨¡åˆ†ç±»"""
        return {
            "å°æ¨¡å‹": "100M-1Bå‚æ•° (å¦‚BERT-base)",
            "ä¸­å‹æ¨¡å‹": "1B-10Bå‚æ•° (å¦‚GPT-2 small)",
            "å¤§æ¨¡å‹": "10B-100Bå‚æ•° (å¦‚GPT-3)",
            "è¶…å¤§æ¨¡å‹": "100B-1T+å‚æ•° (å¦‚GPT-4)"
        }

    @staticmethod
    def by_training_type():
        """æŒ‰è®­ç»ƒæ–¹å¼åˆ†ç±»"""
        return {
            "è‡ªç›‘ç£é¢„è®­ç»ƒ": "æ— æ ‡æ³¨æ•°æ®é¢„è®­ç»ƒ",
            "æŒ‡ä»¤å¾®è°ƒ": "åŸºäºæŒ‡ä»¤æ•°æ®å¾®è°ƒ",
            "äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ": "RLHFä¼˜åŒ–",
            "æŒç»­é¢„è®­ç»ƒ": "ç‰¹å®šé¢†åŸŸæŒç»­è®­ç»ƒ"
        }

    @staticmethod
    def by_capability():
        """æŒ‰èƒ½åŠ›åˆ†ç±»"""
        return {
            "ç†è§£å‹": "ä¸»è¦æ“…é•¿æ–‡æœ¬ç†è§£å’Œåˆ†æ",
            "ç”Ÿæˆå‹": "ä¸»è¦æ“…é•¿æ–‡æœ¬ç”Ÿæˆå’Œåˆ›ä½œ",
            "æ¨ç†å‹": "å…·å¤‡é€»è¾‘æ¨ç†å’Œæ•°å­¦èƒ½åŠ›",
            "å¤šæ¨¡æ€": "å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€"
        }
```

## ğŸ§  æ ¸å¿ƒæŠ€æœ¯åŸç†

### Transformeræ¶æ„
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class TransformerBlock(nn.Module):
    """TransformeråŸºç¡€æ¨¡å—"""
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()

        # å¤šå¤´æ³¨æ„åŠ›
        self.attention = nn.MultiheadAttention(
            embed_dim=d_model,
            num_heads=n_heads,
            dropout=dropout
        )

        # å‰é¦ˆç½‘ç»œ
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )

        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        attn_output, _ = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=5000):
        super().__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class TransformerLLM(nn.Module):
    """åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹"""
    def __init__(self, vocab_size, d_model=512, n_heads=8,
                 n_layers=6, d_ff=2048, max_len=512, dropout=0.1):
        super().__init__()

        self.d_model = d_model

        # è¯åµŒå…¥å’Œä½ç½®ç¼–ç 
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_len)

        # Transformerå±‚
        self.transformer_layers = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, dropout)
            for _ in range(n_layers)
        ])

        # è¾“å‡ºå±‚
        self.ln_f = nn.LayerNorm(d_model)
        self.output = nn.Linear(d_model, vocab_size)

        self.dropout = nn.Dropout(dropout)

    def forward(self, input_ids, attention_mask=None):
        # è¯åµŒå…¥ + ä½ç½®ç¼–ç 
        x = self.embedding(input_ids)
        x = self.pos_encoding(x)
        x = self.dropout(x)

        # é€šè¿‡Transformerå±‚
        for layer in self.transformer_layers:
            x = layer(x, attention_mask)

        # æœ€ç»ˆå±‚
        x = self.ln_f(x)
        logits = self.output(x)

        return logits

# ç¤ºä¾‹ä½¿ç”¨
model = TransformerLLM(
    vocab_size=50000,
    d_model=512,
    n_heads=8,
    n_layers=6,
    d_ff=2048
)

# æ¨¡æ‹Ÿè¾“å…¥
input_ids = torch.randint(0, 50000, (32, 128))  # batch_size=32, seq_len=128
logits = model(input_ids)
print(f"Output shape: {logits.shape}")  # åº”è¯¥æ˜¯ (32, 128, 50000)
```

### è‡ªæ³¨æ„åŠ›æœºåˆ¶è¯¦è§£
```python
class SelfAttentionAnalysis:
    """è‡ªæ³¨æ„åŠ›æœºåˆ¶åˆ†æ"""

    @staticmethod
    def attention_computation(Q, K, V, mask=None, dropout=None):
        """è®¡ç®—è‡ªæ³¨æ„åŠ›"""
        # Q, K, V shape: (batch_size, n_heads, seq_len, head_dim)

        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)

        if dropout is not None:
            attention_weights = dropout(attention_weights)

        output = torch.matmul(attention_weights, V)

        return output, attention_weights

    @staticmethod
    def visualize_attention(attention_weights, tokens, layer=0, head=0):
        """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
        import matplotlib.pyplot as plt
        import seaborn as sns

        # é€‰æ‹©ç‰¹å®šå±‚å’Œå¤´
        if attention_weights.dim() == 4:
            attn = attention_weights[0, head]  # batch_size=0, æŒ‡å®šå¤´
        else:
            attn = attention_weights[0]  # batch_size=0

        # è½¬æ¢ä¸ºnumpy
        attn_np = attn.detach().cpu().numpy()

        # åˆ›å»ºçƒ­åŠ›å›¾
        plt.figure(figsize=(12, 8))
        sns.heatmap(attn_np, xticklabels=tokens, yticklabels=tokens,
                   cmap='Blues', cbar=True)
        plt.title(f'Attention Weights (Layer {layer}, Head {head})')
        plt.xlabel('Key Tokens')
        plt.ylabel('Query Tokens')
        plt.show()

    @staticmethod
    def multi_head_analysis(attention_weights, tokens):
        """å¤šå¤´æ³¨æ„åŠ›åˆ†æ"""
        batch_size, n_heads, seq_len, _ = attention_weights.shape

        print(f"å¤šå¤´æ³¨æ„åŠ›åˆ†æ:")
        print(f"åºåˆ—é•¿åº¦: {seq_len}")
        print(f"æ³¨æ„åŠ›å¤´æ•°: {n_heads}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")

        # è®¡ç®—æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›ç†µ
        entropies = []
        for head in range(n_heads):
            head_attn = attention_weights[0, head]  # ç¬¬ä¸€ä¸ªæ ·æœ¬
            entropy = -torch.sum(head_attn * torch.log(head_attn + 1e-9), dim=-1)
            entropies.append(entropy.item())

        print(f"\nå„å¤´çš„æ³¨æ„åŠ›ç†µ (è¶Šé«˜è¡¨ç¤ºè¶Šåˆ†æ•£):")
        for i, entropy in enumerate(entropies):
            print(f"Head {i}: {entropy:.4f}")

        return entropies

# ç¤ºä¾‹ï¼šåˆ†æè‡ªæ³¨æ„åŠ›
def demonstrate_self_attention():
    """æ¼”ç¤ºè‡ªæ³¨æ„åŠ›æœºåˆ¶"""
    # æ¨¡æ‹ŸQ, K, V
    batch_size, n_heads, seq_len, head_dim = 1, 8, 10, 64

    Q = torch.randn(batch_size, n_heads, seq_len, head_dim)
    K = torch.randn(batch_size, n_heads, seq_len, head_dim)
    V = torch.randn(batch_size, n_heads, seq_len, head_dim)

    # è®¡ç®—æ³¨æ„åŠ›
    output, attention_weights = SelfAttentionAnalysis.attention_computation(Q, K, V)

    print(f"è¾“å…¥å½¢çŠ¶:")
    print(f"Q: {Q.shape}")
    print(f"K: {K.shape}")
    print(f"V: {V.shape}")
    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")

    # åˆ†ææ³¨æ„åŠ›æ¨¡å¼
    tokens = [f"token_{i}" for i in range(seq_len)]
    entropies = SelfAttentionAnalysis.multi_head_analysis(attention_weights, tokens)

    return output, attention_weights

# è¿è¡Œæ¼”ç¤º
output, attention_weights = demonstrate_self_attention()
```

## ğŸ”§ å…³é”®è®­ç»ƒæŠ€æœ¯

### é¢„è®­ç»ƒç›®æ ‡
```python
class PretrainingObjectives:
    """é¢„è®­ç»ƒç›®æ ‡å®ç°"""

    @staticmethod
    def causal_language_modeling(logits, targets):
        """å› æœè¯­è¨€å»ºæ¨¡ (CLM) - GPTç³»åˆ—ä½¿ç”¨"""
        # ç§»åŠ¨ logitsä½¿å…¶å¯¹é½targets
        shifted_logits = logits[..., :-1, :].contiguous()
        shifted_targets = targets[..., 1:].contiguous()

        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss = F.cross_entropy(
            shifted_logits.view(-1, shifted_logits.size(-1)),
            shifted_targets.view(-1),
            ignore_index=-1
        )

        return loss

    @staticmethod
    def masked_language_modeling(logits, targets, mask):
        """æ©ç è¯­è¨€å»ºæ¨¡ (MLM) - BERTç³»åˆ—ä½¿ç”¨"""
        # åªè®¡ç®—è¢«æ©ç ä½ç½®çš„æŸå¤±
        active_loss = mask.view(-1) == 1
        active_logits = logits.view(-1, logits.size(-1))[active_loss]
        active_labels = targets.view(-1)[active_loss]

        loss = F.cross_entropy(active_logits, active_labels)

        return loss

    @staticmethod
    def next_sentence_prediction(cls_logits, nsp_labels):
        """ä¸‹ä¸€å¥é¢„æµ‹ (NSP) - BERTä½¿ç”¨"""
        return F.cross_entropy(cls_logits, nsp_labels)

    @staticmethod
    def seq2seq_objective(decoder_logits, targets, padding_idx=-1):
        """åºåˆ—åˆ°åºåˆ—ç›®æ ‡ - T5/BARTä½¿ç”¨"""
        return F.cross_entropy(
            decoder_logits.view(-1, decoder_logits.size(-1)),
            targets.view(-1),
            ignore_index=padding_idx
        )
```

### å¾®è°ƒç­–ç•¥
```python
class FineTuningStrategies:
    """å¾®è°ƒç­–ç•¥"""

    @staticmethod
    def full_fine_tuning(model, train_loader, optimizer, num_epochs=3):
        """å…¨å‚æ•°å¾®è°ƒ"""
        model.train()
        total_loss = 0

        for epoch in range(num_epochs):
            for batch in train_loader:
                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­
                outputs = model(**batch)
                loss = outputs.loss

                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            print(f"Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}")

    @staticmethod
    def freeze_layers(model, freeze_ratio=0.8):
        """å†»ç»“éƒ¨åˆ†å±‚"""
        total_layers = len(list(model.parameters()))
        freeze_count = int(total_layers * freeze_ratio)

        for i, param in enumerate(model.parameters()):
            if i < freeze_count:
                param.requires_grad = False
            else:
                param.requires_grad = True

        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())

        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,}")
        print(f"å¯è®­ç»ƒæ¯”ä¾‹: {trainable_params/total_params:.2%}")

    @staticmethod
    def parameter_efficient_fine_tuning(model, adapter_dim=64):
        """å‚æ•°é«˜æ•ˆå¾®è°ƒ (LoRA)"""
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                # æ·»åŠ ä½ç§©é€‚é…å™¨
                adapter = nn.Sequential(
                    nn.Linear(module.in_features, adapter_dim),
                    nn.ReLU(),
                    nn.Linear(adapter_dim, module.out_features)
                )

                # åˆå§‹åŒ–é€‚é…å™¨æƒé‡
                nn.init.kaiming_uniform_(adapter[0].weight, a=math.sqrt(5))
                nn.init.zeros_(adapter[2].weight)

                # ä¿å­˜é€‚é…å™¨
                module.adapter = adapter

        print(f"å·²ä¸º {len([m for m in model.modules() if hasattr(m, 'adapter')])} ä¸ªçº¿æ€§å±‚æ·»åŠ é€‚é…å™¨")
```

## ğŸ“Š æ¨¡å‹è¯„ä¼°

### åŸºç¡€è¯„ä¼°æŒ‡æ ‡
```python
class LLMEvaluation:
    """LLMè¯„ä¼°æŒ‡æ ‡"""

    @staticmethod
    def perplexity(model, data_loader):
        """è®¡ç®—å›°æƒ‘åº¦"""
        model.eval()
        total_loss = 0
        total_tokens = 0

        with torch.no_grad():
            for batch in data_loader:
                outputs = model(**batch)
                loss = outputs.loss

                total_loss += loss.item() * batch['input_ids'].size(1)
                total_tokens += batch['input_ids'].size(1)

        ppl = torch.exp(torch.tensor(total_loss / total_tokens))
        return ppl.item()

    @staticmethod
    def bleu_score(references, hypotheses):
        """è®¡ç®—BLEUåˆ†æ•°"""
        from nltk.translate.bleu_score import corpus_bleu, sentence_bleu

        # è¯­æ–™çº§åˆ«BLEU
        references_tokens = [ref.split() for ref in references]
        hypotheses_tokens = [hyp.split() for hyp in hypotheses]

        corpus_bleu_score = corpus_bleu(references_tokens, hypotheses_tokens)

        # å¥å­çº§åˆ«BLEU
        sentence_bleu_scores = []
        for ref, hyp in zip(references, hypotheses):
            score = sentence_bleu([ref.split()], hyp.split())
            sentence_bleu_scores.append(score)

        return {
            'corpus_bleu': corpus_bleu_score,
            'avg_sentence_bleu': sum(sentence_bleu_scores) / len(sentence_bleu_scores)
        }

    @staticmethod
    def rouge_score(references, hypotheses):
        """è®¡ç®—ROUGEåˆ†æ•°"""
        from rouge import Rouge

        rouge = Rouge()
        scores = rouge.get_scores(hypotheses, references, avg=True)

        return {
            'rouge-1': scores['rouge-1']['f'],
            'rouge-2': scores['rouge-2']['f'],
            'rouge-l': scores['rouge-l']['f']
        }
```

### è‡ªå®šä¹‰è¯„ä¼°
```python
class CustomEvaluation:
    """è‡ªå®šä¹‰è¯„ä¼°æ–¹æ³•"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def toxicity_score(self, generated_text):
        """æ¯’æ€§è¯„åˆ†"""
        # ç®€åŒ–çš„æ¯’æ€§æ£€æµ‹
        toxic_keywords = [
            'hate', 'kill', 'die', 'stupid', 'idiot',
            'terrorist', 'violence', 'murder'
        ]

        text_lower = generated_text.lower()
        toxic_count = sum(1 for word in toxic_keywords if word in text_lower)

        # è¯„åˆ†èŒƒå›´: 0-1, è¶Šé«˜è¶Šæœ‰æ¯’
        return min(toxic_count / 10, 1.0)

    def factual_consistency(self, generated_text, reference_text):
        """äº‹å®ä¸€è‡´æ€§æ£€æŸ¥"""
        # ç®€åŒ–çš„äº‹å®ä¸€è‡´æ€§æ£€æŸ¥
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥ä½¿ç”¨NERã€å…³ç³»æŠ½å–ç­‰æŠ€æœ¯

        gen_words = set(generated_text.lower().split())
        ref_words = set(reference_text.lower().split())

        # è®¡ç®—è¯æ±‡é‡å åº¦
        overlap = len(gen_words & ref_words)
        union = len(gen_words | ref_words)

        return overlap / union if union > 0 else 0.0

    def fluency_score(self, generated_text):
        """æµç•…åº¦è¯„åˆ†"""
        # ç®€åŒ–çš„æµç•…åº¦æ£€æŸ¥
        words = generated_text.split()

        # æ£€æŸ¥é‡å¤
        repetitions = sum(words.count(word) - 1 for word in set(words))
        repetition_penalty = max(0, 1 - repetitions / len(words))

        # æ£€æŸ¥å¥å­ç»“æ„
        sentences = generated_text.split('.')
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)

        # ç†æƒ³çš„å¥å­é•¿åº¦åœ¨10-30è¯ä¹‹é—´
        length_score = 1.0 - abs(avg_sentence_length - 20) / 20
        length_score = max(0, length_score)

        return (repetition_penalty + length_score) / 2
```

## ğŸ› ï¸ å®é™…åº”ç”¨ç¤ºä¾‹

### æ–‡æœ¬ç”Ÿæˆ
```python
class TextGeneration:
    """æ–‡æœ¬ç”Ÿæˆå·¥å…·"""

    def __init__(self, model, tokenizer, device='cuda'):
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device

    def generate(self, prompt, max_length=100, temperature=0.7,
                 top_k=50, top_p=0.9, do_sample=True):
        """ç”Ÿæˆæ–‡æœ¬"""
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)

        # ç”Ÿæˆå‚æ•°
        generation_config = {
            'max_length': max_length,
            'temperature': temperature,
            'top_k': top_k,
            'top_p': top_p,
            'do_sample': do_sample,
            'pad_token_id': self.tokenizer.eos_token_id,
            'eos_token_id': self.tokenizer.eos_token_id,
        }

        # ç”Ÿæˆæ–‡æœ¬
        with torch.no_grad():
            outputs = self.model.generate(inputs, **generation_config)

        # è§£ç è¾“å‡º
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        return generated_text

    def generate_with_control(self, prompt, control_tokens,
                             max_length=100, temperature=0.7):
        """å—æ§ç”Ÿæˆ"""
        # æ·»åŠ æ§åˆ¶ä»¤ç‰Œ
        controlled_prompt = control_tokens + prompt
        inputs = self.tokenizer.encode(controlled_prompt, return_tensors='pt').to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## ğŸ“ æ€»ç»“

å¤§è¯­è¨€æ¨¡å‹æ˜¯å½“å‰AIæŠ€æœ¯çš„æ ¸å¿ƒï¼Œç†è§£å…¶åŸºç¡€æ¦‚å¿µã€æŠ€æœ¯åŸç†å’Œåº”ç”¨æ–¹æ³•å¯¹äºæ„å»ºæ™ºèƒ½ç³»ç»Ÿè‡³å…³é‡è¦ã€‚

### ğŸ¯ å…³é”®è¦ç‚¹
- **Transformeræ¶æ„**æ˜¯ç°ä»£LLMçš„åŸºç¡€
- **è‡ªæ³¨æ„åŠ›æœºåˆ¶**å®ç°äº†é•¿è·ç¦»ä¾èµ–å»ºæ¨¡
- **é¢„è®­ç»ƒ-å¾®è°ƒ**èŒƒå¼æ˜¯LLMæˆåŠŸçš„å…³é”®
- **å¤šä»»åŠ¡èƒ½åŠ›**ä½¿LLMå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯
- **è¯„ä¼°æŒ‡æ ‡**éœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªç»´åº¦

### ğŸš€ ä¸‹ä¸€æ­¥
- å­¦ä¹ [Transformeræ¶æ„è¯¦è§£](../deep-learning/04-Transformer.md)
- äº†è§£[é¢„è®­ç»ƒä¸å¾®è°ƒæŠ€æœ¯](02-Transformeræ¶æ„.md)
- æŒæ¡[æç¤ºå·¥ç¨‹ä¸å·¥å…·è°ƒç”¨](04-æç¤ºå·¥ç¨‹.md)
- æ¢ç´¢[å¤šæ¨¡æ€LLM](05-å¤šæ¨¡æ€æ¨¡å‹.md)
- å­¦ä¹ [éƒ¨ç½²ä¸ä¼˜åŒ–](06-éƒ¨ç½²ä¼˜åŒ–.md)

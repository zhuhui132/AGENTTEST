#!/usr/bin/env python3
"""
AIå­¦ä¹ åˆ†ææ¨¡å—

æä¾›æ·±åº¦çš„å­¦ä¹ æ•°æ®åˆ†æå’Œå¯è§†åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- å­¦ä¹ è¿›åº¦å¯è§†åŒ–
- çŸ¥è¯†æŒæ¡åº¦åˆ†æ
- å­¦ä¹ æ•ˆç‡è¯„ä¼°
- ä¸ªæ€§åŒ–å­¦ä¹ æ´å¯Ÿ
- å­¦ä¹ è·¯å¾„ä¼˜åŒ–å»ºè®®
- å­¦ä¹ é¢„æµ‹å’Œè§„åˆ’

ä½œè€…: AIå­¦ä¹ å›¢é˜Ÿ
ç‰ˆæœ¬: 1.0.0
æ—¥æœŸ: 2025-11-13
"""

import os
import json
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class AnalyticsType(Enum):
    """åˆ†æç±»å‹æšä¸¾"""
    PROGRESS_TREND = "progress_trend"           # è¿›åº¦è¶‹åŠ¿
    KNOWLEDGE_MASTERY = "knowledge_mastery"     # çŸ¥è¯†æŒæ¡åº¦
    EFFICIENCY_ANALYSIS = "efficiency_analysis"   # æ•ˆç‡åˆ†æ
    LEARNING_PATTERN = "learning_pattern"         # å­¦ä¹ æ¨¡å¼
    PREDICTIVE_ANALYSIS = "predictive_analysis"   # é¢„æµ‹åˆ†æ
    COMPARATIVE_ANALYSIS = "comparative_analysis" # å¯¹æ¯”åˆ†æ


class VisualizationType(Enum):
    """å¯è§†åŒ–ç±»å‹æšä¸¾"""
    LINE_CHART = "line_chart"                 # æŠ˜çº¿å›¾
    BAR_CHART = "bar_chart"                   # æŸ±çŠ¶å›¾
    HEATMAP = "heatmap"                        # çƒ­åŠ›å›¾
    SCATTER_PLOT = "scatter_plot"               # æ•£ç‚¹å›¾
    RADAR_CHART = "radar_chart"                 # é›·è¾¾å›¾
    PIE_CHART = "pie_chart"                     # é¥¼å›¾
    AREA_CHART = "area_chart"                   # é¢ç§¯å›¾
    BOX_PLOT = "box_plot"                       # ç®±çº¿å›¾


@dataclass
class LearningInsight:
    """å­¦ä¹ æ´å¯Ÿç±»"""
    category: str
    title: str
    description: str
    data: Dict[str, Any] = field(default_factory=dict)
    confidence: float = 0.0
    recommendations: List[str] = field(default_factory=list)
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class LearningPrediction:
    """å­¦ä¹ é¢„æµ‹ç±»"""
    prediction_type: str
    predicted_value: Any
    confidence: float
    timeframe: str
    factors: List[str] = field(default_factory=list)
    methodology: str = ""
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())


class LearningAnalytics:
    """å­¦ä¹ åˆ†æç±»"""

    def __init__(self, db_path: str = "learning_system.db"):
        self.db_path = db_path
        self.ensure_database_connection()

    def ensure_database_connection(self):
        """ç¡®ä¿æ•°æ®åº“è¿æ¥"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM sqlite_master WHERE type='table'")
            conn.close()
        except:
            raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°æ•°æ®åº“: {self.db_path}")

    def generate_comprehensive_analysis(self, user_id: int, days: int = 30) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆå­¦ä¹ åˆ†æ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # è·å–åŸºç¡€å­¦ä¹ æ•°æ®
        learning_data = self._get_learning_data(cursor, user_id, days)
        test_data = self._get_test_data(cursor, user_id, days)
        resource_data = self._get_resource_data(cursor, user_id, days)

        # å„é¡¹åˆ†æ
        analysis = {
            "user_id": user_id,
            "analysis_period": days,
            "analysis_date": datetime.now().isoformat(),
            "progress_trend": self._analyze_progress_trend(learning_data),
            "knowledge_mastery": self._analyze_knowledge_mastery(test_data, learning_data),
            "efficiency_analysis": self._analyze_efficiency(learning_data, test_data),
            "learning_patterns": self._analyze_learning_patterns(learning_data),
            "resource_utilization": self._analyze_resource_utilization(resource_data),
            "predictions": self._generate_predictions(learning_data, test_data),
            "insights": self._generate_insights(learning_data, test_data),
            "recommendations": self._generate_recommendations(learning_data, test_data)
        }

        conn.close()
        return analysis

    def _get_learning_data(self, cursor, user_id: int, days: int) -> List[Dict]:
        """è·å–å­¦ä¹ æ•°æ®"""
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')

        cursor.execute('''
            SELECT date, duration_minutes, topics_studied, self_rating, difficulty_rating, notes
            FROM learning_sessions
            WHERE user_id = ? AND date >= ?
            ORDER BY date ASC
        ''', (user_id, start_date))

        sessions = []
        for row in cursor.fetchall():
            sessions.append({
                "date": row[0],
                "duration": row[1],
                "topics": row[2].split(',') if row[2] else [],
                "self_rating": row[3] or 0,
                "difficulty_rating": row[4] or 0,
                "notes": row[5] or ""
            })

        return sessions

    def _get_test_data(self, cursor, user_id: int, days: int) -> List[Dict]:
        """è·å–æµ‹è¯•æ•°æ®"""
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')

        cursor.execute('''
            SELECT date, test_name, total_questions, correct_answers, score, execution_time
            FROM test_results
            WHERE user_id = ? AND date >= ?
            ORDER BY date ASC
        ''', (user_id, start_date))

        tests = []
        for row in cursor.fetchall():
            tests.append({
                "date": row[0],
                "test_name": row[1],
                "total_questions": row[2],
                "correct_answers": row[3],
                "score": row[4],
                "execution_time": row[5] or 0
            })

        return tests

    def _get_resource_data(self, cursor, user_id: int, days: int) -> List[Dict]:
        """è·å–èµ„æºä½¿ç”¨æ•°æ®"""
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')

        cursor.execute('''
            SELECT date, resources_used
            FROM learning_sessions
            WHERE user_id = ? AND date >= ?
            ORDER BY date ASC
        ''', (user_id, start_date))

        resources = []
        for row in cursor.fetchall():
            if row[1]:
                resource_list = row[1].split(',')
                for resource in resource_list:
                    if resource.strip():
                        resources.append({
                            "date": row[0],
                            "resource": resource.strip()
                        })

        return resources

    def _analyze_progress_trend(self, learning_data: List[Dict]) -> Dict[str, Any]:
        """åˆ†æè¿›åº¦è¶‹åŠ¿"""
        if not learning_data:
            return {"trend": "no_data", "slope": 0, "correlation": 0}

        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(learning_data)
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date')

        # è®¡ç®—ç´¯è®¡å­¦ä¹ æ—¶é•¿
        df['cumulative_hours'] = df['duration'].cumsum() / 60

        # è®¡ç®—æ»‘åŠ¨å¹³å‡
        df['rolling_avg_duration'] = df['duration'].rolling(window=7, min_periods=1).mean()
        df['rolling_avg_rating'] = df['self_rating'].rolling(window=7, min_periods=1).mean()

        # è¶‹åŠ¿åˆ†æ
        from scipy import stats

        # çº¿æ€§å›å½’åˆ†æå­¦ä¹ æ—¶é•¿è¶‹åŠ¿
        x = np.arange(len(df))
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, df['duration'])

        # å­¦ä¹ é¢‘ç‡åˆ†æ
        daily_counts = df.groupby('date').size()
        recent_freq = daily_counts.tail(7).mean()
        earlier_freq = daily_counts.head(-7).mean() if len(daily_counts) > 7 else recent_freq

        return {
            "trend": "increasing" if slope > 0 else "decreasing" if slope < 0 else "stable",
            "slope": float(slope),
            "r_squared": float(r_value ** 2),
            "p_value": float(p_value),
            "total_hours": float(df['cumulative_hours'].iloc[-1] if len(df) > 0 else 0),
            "avg_session_duration": float(df['duration'].mean()),
            "avg_self_rating": float(df['self_rating'].mean()),
            "rolling_avg_duration": df['rolling_avg_duration'].tolist() if len(df) > 0 else [],
            "rolling_avg_rating": df['rolling_avg_rating'].tolist() if len(df) > 0 else [],
            "frequency_trend": "increasing" if recent_freq > earlier_freq else "decreasing",
            "recent_frequency": float(recent_freq),
            "earlier_frequency": float(earlier_freq),
            "daily_hours": (df.groupby('date')['duration'].sum() / 60).to_dict()
        }

    def _analyze_knowledge_mastery(self, test_data: List[Dict], learning_data: List[Dict]) -> Dict[str, Any]:
        """åˆ†æçŸ¥è¯†æŒæ¡åº¦"""
        if not test_data:
            return {"overall_mastery": 0, "topic_mastery": {}, "weak_areas": []}

        # è½¬æ¢ä¸ºDataFrame
        test_df = pd.DataFrame(test_data)
        test_df['date'] = pd.to_datetime(test_df['date'])
        test_df = test_df.sort_values('date')

        # æ€»ä½“æŒæ¡åº¦
        overall_mastery = test_df['score'].mean()

        # æŒ‰ä¸»é¢˜åˆ†ç»„åˆ†æ
        topic_scores = {}
        for _, row in test_df.iterrows():
            test_name = row['test_name']
            # ç®€å•çš„ä¸»é¢˜æå–ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ï¼‰
            topic = self._extract_topic_from_test_name(test_name)

            if topic not in topic_scores:
                topic_scores[topic] = []
            topic_scores[topic].append(row['score'])

        # è®¡ç®—å„ä¸»é¢˜å¹³å‡åˆ†
        topic_mastery = {}
        for topic, scores in topic_scores.items():
            topic_mastery[topic] = {
                "avg_score": np.mean(scores),
                "max_score": np.max(scores),
                "min_score": np.min(scores),
                "std_score": np.std(scores),
                "test_count": len(scores),
                "trend": "improving" if len(scores) > 1 and scores[-1] > scores[0] else "stable"
            }

        # è¯†åˆ«å¼±é¡¹
        weak_areas = []
        for topic, data in topic_mastery.items():
            if data['avg_score'] < 70:  # 70åˆ†ä»¥ä¸‹ä¸ºå¼±é¡¹
                weak_areas.append({
                    "topic": topic,
                    "avg_score": data['avg_score'],
                    "test_count": data['test_count']
                })

        # æŒ‰éš¾åº¦åˆ†æ
        if learning_data:
            learning_df = pd.DataFrame(learning_data)
            difficulty_scores = learning_df.groupby('difficulty_rating')['self_rating'].mean().to_dict()
        else:
            difficulty_scores = {}

        return {
            "overall_mastery": float(overall_mastery),
            "topic_mastery": topic_mastery,
            "weak_areas": weak_areas,
            "difficulty_mastery": difficulty_scores,
            "score_distribution": {
                "excellent": len([s for s in test_df['score'] if s >= 90]),
                "good": len([s for s in test_df['score'] if 80 <= s < 90]),
                "average": len([s for s in test_df['score'] if 70 <= s < 80]),
                "below_average": len([s for s in test_df['score'] if s < 70])
            },
            "improvement_rate": self._calculate_improvement_rate(test_df)
        }

    def _analyze_efficiency(self, learning_data: List[Dict], test_data: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå­¦ä¹ æ•ˆç‡"""
        if not learning_data:
            return {"overall_efficiency": 0, "efficiency_trend": "stable"}

        learning_df = pd.DataFrame(learning_data)

        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        # 1. æ—¶é—´æ•ˆç‡ï¼šå­¦ä¹ æ—¶é•¿ä¸è‡ªè¯„ç­‰çº§çš„å…³ç³»
        time_efficiency = []
        for _, row in learning_df.iterrows():
            if row['duration'] > 0:
                # æ•ˆç‡ = è‡ªè¯„ç­‰çº§ / å­¦ä¹ æ—¶é•¿ï¼ˆå°æ—¶ï¼‰
                efficiency_score = (row['self_rating'] / 5.0) / (row['duration'] / 60.0)
                time_efficiency.append(efficiency_score)

        # 2. å­¦ä¹ ç¨³å®šæ€§ï¼šè‡ªè¯„ç­‰çº§çš„æ–¹å·®
        rating_stability = 1 - (learning_df['self_rating'].std() / 5.0) if learning_df['self_rating'].std() > 0 else 1.0

        # 3. æŒç»­æ€§æ•ˆç‡ï¼šè¿ç»­å­¦ä¹ å¤©æ•°ä¸å¹³å‡æ•ˆç‡çš„å…³ç³»
        consecutive_days = self._calculate_consecutive_learning_days(learning_data)

        # 4. çŸ¥è¯†è½¬åŒ–æ•ˆç‡ï¼šå­¦ä¹ æ—¶é•¿ä¸æµ‹è¯•æˆç»©çš„å…³ç³»
        knowledge_conversion = 0
        if test_data:
            test_df = pd.DataFrame(test_data)
            test_df['date'] = pd.to_datetime(test_df['date'])

            # åˆå¹¶å­¦ä¹ æ•°æ®å’Œæµ‹è¯•æ•°æ®
            for _, test_row in test_df.iterrows():
                # æŸ¥æ‰¾è¯¥æµ‹è¯•æ—¥æœŸå‰7å¤©çš„å­¦ä¹ æ•°æ®
                week_before = test_row['date'] - timedelta(days=7)
                week_learning = learning_df[
                    (pd.to_datetime(learning_df['date']) >= week_before) &
                    (pd.to_datetime(learning_df['date']) <= test_row['date'])
                ]

                if not week_learning.empty:
                    total_hours = week_learning['duration'].sum() / 60
                    if total_hours > 0:
                        conversion_rate = test_row['score'] / total_hours
                        knowledge_conversion += conversion_rate

            knowledge_conversion = knowledge_conversion / len(test_df)

        # ç»¼åˆæ•ˆç‡è¯„åˆ†
        avg_time_efficiency = np.mean(time_efficiency) if time_efficiency else 0
        overall_efficiency = (
            avg_time_efficiency * 0.3 +
            rating_stability * 0.3 +
            (consecutive_days / 30) * 0.2 +  # å‡è®¾30å¤©ä¸ºæ»¡åˆ†
            knowledge_conversion * 0.2
        )

        # æ•ˆç‡è¶‹åŠ¿
        if len(learning_df) >= 14:  # è‡³å°‘ä¸¤å‘¨æ•°æ®
            first_week_efficiency = self._calculate_week_efficiency(learning_df.head(7))
            second_week_efficiency = self._calculate_week_efficiency(learning_df.tail(7))
            efficiency_trend = "improving" if second_week_efficiency > first_week_efficiency else "declining"
        else:
            efficiency_trend = "stable"

        return {
            "overall_efficiency": float(overall_efficiency),
            "time_efficiency": float(avg_time_efficiency),
            "rating_stability": float(rating_stability),
            "consecutive_days": consecutive_days,
            "knowledge_conversion": float(knowledge_conversion),
            "efficiency_trend": efficiency_trend,
            "daily_efficiency": [
                {
                    "date": row['date'],
                    "efficiency": (row['self_rating'] / 5.0) / (row['duration'] / 60.0) if row['duration'] > 0 else 0
                }
                for _, row in learning_df.iterrows()
            ]
        }

    def _analyze_learning_patterns(self, learning_data: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå­¦ä¹ æ¨¡å¼"""
        if not learning_data:
            return {"patterns": {}, "insights": []}

        learning_df = pd.DataFrame(learning_data)
        learning_df['date'] = pd.to_datetime(learning_df['date'])
        learning_df['weekday'] = learning_df['date'].dt.day_name()
        learning_df['hour'] = learning_df['date'].dt.hour

        # 1. æ—¶é—´æ¨¡å¼åˆ†æ
        weekday_hours = learning_df.groupby(['weekday', 'hour'])['duration'].mean().unstack(fill_value=0)
        peak_weekday = weekday_hours.sum(axis=1).idxmax()
        peak_hour = weekday_hours.loc[peak_weekday].idxmax()

        # 2. å­¦ä¹ æ—¶é•¿åˆ†å¸ƒ
        duration_stats = {
            "mean": learning_df['duration'].mean(),
            "median": learning_df['duration'].median(),
            "std": learning_df['duration'].std(),
            "min": learning_df['duration'].min(),
            "max": learning_df['duration'].max(),
            "quartiles": np.percentile(learning_df['duration'], [25, 50, 75]).tolist()
        }

        # 3. ä¸»é¢˜åå¥½åˆ†æ
        topic_frequency = {}
        for _, row in learning_df.iterrows():
            for topic in row['topics']:
                if topic.strip():
                    topic_frequency[topic.strip()] = topic_frequency.get(topic.strip(), 0) + 1

        # 4. å­¦ä¹ èŠ‚å¥åˆ†æ
        if len(learning_df) > 1:
            learning_df['time_diff'] = learning_df['date'].diff().dt.days
            avg_interval = learning_df['time_diff'].mean()
            regularity_score = 1 / (1 + learning_df['time_diff'].std()) if learning_df['time_diff'].std() > 0 else 1.0
        else:
            avg_interval = 0
            regularity_score = 1.0

        # 5. éš¾åº¦åå¥½åˆ†æ
        difficulty_preference = learning_df.groupby('difficulty_rating').size().to_dict()

        patterns = {
            "temporal_patterns": {
                "peak_weekday": peak_weekday,
                "peak_hour": peak_hour,
                "weekday_heatmap": weekday_hours.to_dict(),
                "hourly_distribution": learning_df.groupby('hour')['duration'].mean().to_dict()
            },
            "duration_patterns": duration_stats,
            "topic_preferences": topic_frequency,
            "rhythm_patterns": {
                "avg_interval_days": float(avg_interval),
                "regularity_score": float(regularity_score),
                "consistency": "regular" if regularity_score > 0.7 else "irregular"
            },
            "difficulty_preferences": difficulty_preference,
            "learning_velocity": len(learning_df) / 30.0  # å‡è®¾30å¤©å†…çš„å­¦ä¹ é€Ÿåº¦
        }

        # ç”Ÿæˆæ´å¯Ÿ
        insights = []

        # æ—¶é—´æ´å¯Ÿ
        if peak_hour >= 20 or peak_hour <= 6:
            insights.append("å€¾å‘äºåœ¨å¤œé—´æˆ–æ¸…æ™¨å­¦ä¹ ")
        elif 9 <= peak_hour <= 17:
            insights.append("å€¾å‘äºåœ¨ç™½å¤©å­¦ä¹ ")

        # éš¾åº¦æ´å¯Ÿ
        if difficulty_preference:
            max_difficulty = max(difficulty_preference.keys(), key=lambda k: difficulty_preference[k])
            insights.append(f"åå¥½æŒ‘æˆ˜{max_difficulty}éš¾åº¦çš„å†…å®¹")

        # èŠ‚å¥æ´å¯Ÿ
        if avg_interval < 2:
            insights.append("å­¦ä¹ èŠ‚å¥ç´§å‡‘ï¼Œå»ºè®®æ³¨æ„ä¼‘æ¯")
        elif avg_interval > 7:
            insights.append("å­¦ä¹ é—´éš”è¾ƒé•¿ï¼Œå»ºè®®æé«˜å­¦ä¹ é¢‘ç‡")

        return {
            "patterns": patterns,
            "insights": insights
        }

    def _analyze_resource_utilization(self, resource_data: List[Dict]) -> Dict[str, Any]:
        """åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ"""
        if not resource_data:
            return {"utilization_rate": 0, "resource_preferences": {}}

        # ç»Ÿè®¡èµ„æºä½¿ç”¨é¢‘ç‡
        resource_df = pd.DataFrame(resource_data)
        resource_count = resource_df.groupby('resource').size().sort_values(ascending=False)

        # èµ„æºç±»å‹åˆ†æ
        def classify_resource(resource_name):
            name = resource_name.lower()
            if any(keyword in name for keyword in ['video', 'è§†é¢‘', 'course']):
                return 'video'
            elif any(keyword in name for keyword in ['book', 'ä¹¦ç±', 'article']):
                return 'reading'
            elif any(keyword in name for keyword in ['tutorial', 'æ•™ç¨‹']):
                return 'tutorial'
            elif any(keyword in name for keyword in ['practice', 'ç»ƒä¹ ', 'lab']):
                return 'practice'
            else:
                return 'other'

        resource_df['resource_type'] = resource_df['resource'].apply(classify_resource)
        type_distribution = resource_df['resource_type'].value_counts().to_dict()

        # èµ„æºä½¿ç”¨è¶‹åŠ¿
        resource_df['date'] = pd.to_datetime(resource_df['date'])
        daily_resources = resource_df.groupby('date').size()
        recent_avg = daily_resources.tail(7).mean()
        earlier_avg = daily_resources.head(-7).mean() if len(daily_resources) > 7 else recent_avg

        return {
            "utilization_rate": float(len(resource_data) / 30),  # å‡è®¾30å¤©åŸºå‡†
            "most_used_resources": resource_count.head(10).to_dict(),
            "resource_type_distribution": type_distribution,
            "utilization_trend": "increasing" if recent_avg > earlier_avg else "stable",
            "diversity_score": float(len(resource_count) / len(resource_data)) if resource_data else 0,
            "daily_usage": daily_resources.to_dict()
        }

    def _generate_predictions(self, learning_data: List[Dict], test_data: List[Dict]) -> List[LearningPrediction]:
        """ç”Ÿæˆå­¦ä¹ é¢„æµ‹"""
        predictions = []

        # 1. å­¦ä¹ è¿›åº¦é¢„æµ‹
        if learning_data:
            learning_df = pd.DataFrame(learning_data)
            learning_df['date'] = pd.to_datetime(learning_df['date'])
            learning_df = learning_df.sort_values('date')

            # ä½¿ç”¨çº¿æ€§å›å½’é¢„æµ‹æœªæ¥å­¦ä¹ æ—¶é•¿
            if len(learning_df) >= 7:
                x = np.arange(len(learning_df))
                y = learning_df['duration'].values

                from scipy import stats
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)

                # é¢„æµ‹æœªæ¥7å¤©çš„å­¦ä¹ æ—¶é•¿
                future_x = np.arange(len(learning_df), len(learning_df) + 7)
                future_y = slope * future_x + intercept

                predictions.append(LearningPrediction(
                    prediction_type="learning_hours",
                    predicted_value=future_y.tolist(),
                    confidence=float(r_value ** 2),
                    timeframe="7_days",
                    factors=["historical_trend", "seasonal_patterns"],
                    methodology="linear_regression"
                ))

        # 2. æµ‹è¯•æˆç»©é¢„æµ‹
        if test_data and learning_data:
            # ä½¿ç”¨æœ€è¿‘çš„å­¦ä¹ è¡¨ç°é¢„æµ‹æµ‹è¯•æˆç»©
            test_df = pd.DataFrame(test_data)
            learning_df = pd.DataFrame(learning_data)

            # è®¡ç®—æœ€è¿‘ä¸€å‘¨çš„å­¦ä¹ æŒ‡æ ‡
            recent_learning = learning_df.tail(7)
            if not recent_learning.empty:
                avg_duration = recent_learning['duration'].mean()
                avg_rating = recent_learning['self_rating'].mean()

                # ç®€å•çš„é¢„æµ‹æ¨¡å‹
                predicted_score = min(100, avg_rating * 20 + (avg_duration / 60) * 5)

                predictions.append(LearningPrediction(
                    prediction_type="test_score",
                    predicted_value=float(predicted_score),
                    confidence=0.7,
                    timeframe="next_test",
                    factors=["recent_learning", "self_assessment", "time_investment"],
                    methodology="heuristic_model"
                ))

        # 3. ç›®æ ‡è¾¾æˆé¢„æµ‹
        if learning_data:
            # è®¡ç®—å­¦ä¹ é€Ÿåº¦å’Œå½“å‰è¿›åº¦
            learning_df = pd.DataFrame(learning_data)
            total_hours = learning_df['duration'].sum() / 60
            current_velocity = total_hours / 30  # å‡è®¾30å¤©å†…çš„å¹³å‡é€Ÿåº¦

            # é¢„æµ‹è¾¾æˆ100å°æ—¶ç›®æ ‡æ‰€éœ€æ—¶é—´
            if current_velocity > 0:
                days_to_100_hours = (100 - total_hours) / (current_velocity * 7) * 7

                predictions.append(LearningPrediction(
                    prediction_type="goal_completion",
                    predicted_value=float(days_to_100_hours),
                    confidence=0.6,
                    timeframe="hours_goal",
                    factors=["learning_velocity", "consistency"],
                    methodology="projection_model"
                ))

        return predictions

    def _generate_insights(self, learning_data: List[Dict], test_data: List[Dict]) -> List[LearningInsight]:
        """ç”Ÿæˆå­¦ä¹ æ´å¯Ÿ"""
        insights = []

        # 1. å­¦ä¹ å¼ºåº¦æ´å¯Ÿ
        if learning_data:
            learning_df = pd.DataFrame(learning_data)
            daily_hours = learning_df.groupby('date')['duration'].sum() / 60
            avg_daily = daily_hours.mean()

            if avg_daily >= 3:
                category = "å­¦ä¹ å¼ºåº¦"
                title = "é«˜å¼ºåº¦å­¦ä¹ è€…"
                description = "æ‚¨æ¯å¤©å¹³å‡å­¦ä¹ æ—¶é—´è¶…è¿‡3å°æ—¶ï¼Œå­¦ä¹ å¼ºåº¦å¾ˆé«˜"
                confidence = 0.9
                recommendations = ["æ³¨æ„åŠ³é€¸ç»“åˆï¼Œé¿å…è¿‡åº¦ç–²åŠ³", "å¯ä»¥é€‚å½“å‡å°‘å•æ¬¡å­¦ä¹ æ—¶é•¿ï¼Œå¢åŠ å­¦ä¹ é¢‘ç‡"]
            elif avg_daily >= 1:
                category = "å­¦ä¹ å¼ºåº¦"
                title = "é€‚åº¦å­¦ä¹ è€…"
                description = "æ‚¨æ¯å¤©å­¦ä¹ æ—¶é—´é€‚ä¸­ï¼Œæœ‰åˆ©äºçŸ¥è¯†çš„é•¿æœŸç§¯ç´¯"
                confidence = 0.8
                recommendations = ["ä¿æŒå½“å‰å­¦ä¹ èŠ‚å¥", "é€‚å½“å¢åŠ ä¸€äº›æŒ‘æˆ˜æ€§å†…å®¹"]
            else:
                category = "å­¦ä¹ å¼ºåº¦"
                title = "ä½å¼ºåº¦å­¦ä¹ è€…"
                description = "æ‚¨æ¯å¤©å­¦ä¹ æ—¶é—´è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ å­¦ä¹ æŠ•å…¥"
                confidence = 0.8
                recommendations = ["åˆ¶å®šæ¯æ—¥å­¦ä¹ è®¡åˆ’", "ä»çŸ­æ—¶é—´ã€é«˜é¢‘ç‡å¼€å§‹", "å¯»æ‰¾æœ‰è¶£çš„å­¦ä¹ å†…å®¹æé«˜åŠ¨åŠ›"]

            insights.append(LearningInsight(
                category=category,
                title=title,
                description=description,
                data={"avg_daily_hours": float(avg_daily)},
                confidence=confidence,
                recommendations=recommendations
            ))

        # 2. å­¦ä¹ æ•ˆæœæ´å¯Ÿ
        if test_data:
            test_df = pd.DataFrame(test_data)
            avg_score = test_df['score'].mean()
            score_std = test_df['score'].std()

            if avg_score >= 85 and score_std < 10:
                category = "å­¦ä¹ æ•ˆæœ"
                title = "ç¨³å®šä¼˜ç§€å‹"
                description = "æ‚¨çš„æµ‹è¯•æˆç»©ä¼˜ç§€ä¸”ç¨³å®šï¼Œå­¦ä¹ æ•ˆæœå¾ˆå¥½"
                confidence = 0.9
                recommendations = ["ç»§ç»­ä¿æŒå­¦ä¹ æ–¹æ³•", "å¯ä»¥å°è¯•æ›´é«˜çº§çš„å†…å®¹"]
            elif avg_score >= 70:
                category = "å­¦ä¹ æ•ˆæœ"
                title = "è‰¯å¥½æ”¹è¿›å‹"
                description = "æ‚¨çš„å­¦ä¹ æˆç»©è‰¯å¥½ï¼Œè¿˜æœ‰æå‡ç©ºé—´"
                confidence = 0.7
                recommendations = ["åˆ†æé”™é¢˜åŸå› ", "åŠ å¼ºè–„å¼±ç¯èŠ‚ç»ƒä¹ "]
            else:
                category = "å­¦ä¹ æ•ˆæœ"
                title = "éœ€è¦æå‡å‹"
                description = "æ‚¨çš„å­¦ä¹ æˆç»©æœ‰å¾…æé«˜ï¼Œå»ºè®®è°ƒæ•´å­¦ä¹ æ–¹æ³•"
                confidence = 0.8
                recommendations = ["å›åˆ°åŸºç¡€ï¼Œå·©å›ºåŸºç¡€çŸ¥è¯†", "å¯»æ±‚å­¦ä¹ æ–¹æ³•å’ŒæŠ€å·§æŒ‡å¯¼"]

            insights.append(LearningInsight(
                category=category,
                title=title,
                description=description,
                data={"avg_score": float(avg_score), "score_stability": float(1 - (score_std / 100))},
                confidence=confidence,
                recommendations=recommendations
            ))

        # 3. å­¦ä¹ æ¨¡å¼æ´å¯Ÿ
        if learning_data:
            learning_df = pd.DataFrame(learning_data)
            rating_diff = learning_df['self_rating'].max() - learning_df['self_rating'].min()

            if rating_diff >= 3:
                category = "å­¦ä¹ æ¨¡å¼"
                title = "æ³¢åŠ¨è¾ƒå¤§å‹"
                description = "æ‚¨çš„å­¦ä¹ è‡ªè¯„å·®å¼‚è¾ƒå¤§ï¼Œå­¦ä¹ çŠ¶æ€ä¸å¤Ÿç¨³å®š"
                confidence = 0.7
                recommendations = ["ä¿æŒç¨³å®šçš„å­¦ä¹ æ—¶é—´", "å¯»æ‰¾å½±å“å­¦ä¹ çŠ¶æ€çš„å› ç´ ", "å»ºç«‹å­¦ä¹ ä»ªå¼æ„Ÿ"]
            elif rating_diff >= 1:
                category = "å­¦ä¹ æ¨¡å¼"
                title = "é€‚åº¦æ³¢åŠ¨å‹"
                description = "æ‚¨çš„å­¦ä¹ çŠ¶æ€æœ‰ä¸€å®šæ³¢åŠ¨ï¼Œå±äºæ­£å¸¸èŒƒå›´"
                confidence = 0.6
                recommendations = ["æ³¨æ„çŠ¶æ€ç®¡ç†", "åœ¨å­¦ä¹ å‰åšå¥½å‡†å¤‡å·¥ä½œ"]
            else:
                category = "å­¦ä¹ æ¨¡å¼"
                title = "ç¨³å®šå‹"
                description = "æ‚¨çš„å­¦ä¹ çŠ¶æ€ç¨³å®šï¼Œå€¼å¾—è‚¯å®š"
                confidence = 0.8
                recommendations = ["ä¿æŒè‰¯å¥½çš„å­¦ä¹ ä¹ æƒ¯", "å¯ä»¥é€‚å½“å¢åŠ å­¦ä¹ æŒ‘æˆ˜"]

            insights.append(LearningInsight(
                category=category,
                title=title,
                description=description,
                data={"rating_variance": float(rating_diff)},
                confidence=confidence,
                recommendations=recommendations
            ))

        return insights

    def _generate_recommendations(self, learning_data: List[Dict], test_data: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆå­¦ä¹ å»ºè®®"""
        recommendations = {
            "time_management": [],
            "content_adjustment": [],
            "learning_strategy": [],
            "goal_setting": [],
            "resource_optimization": []
        }

        # æ—¶é—´ç®¡ç†å»ºè®®
        if learning_data:
            learning_df = pd.DataFrame(learning_data)
            avg_session = learning_df['duration'].mean()

            if avg_session < 30:  # 30åˆ†é’Ÿ
                recommendations["time_management"].append("å»ºè®®å»¶é•¿å•æ¬¡å­¦ä¹ æ—¶é—´åˆ°45-60åˆ†é’Ÿ")
                recommendations["time_management"].append("ä½¿ç”¨ç•ªèŒ„å·¥ä½œæ³•æé«˜ä¸“æ³¨åº¦")
            elif avg_session > 120:  # 2å°æ—¶
                recommendations["time_management"].append("å»ºè®®å°†é•¿æ—¶é—´å­¦ä¹ åˆ†å‰²ä¸ºçŸ­æ—¶æ®µ")
                recommendations["time_management"].append("æ¯45-50åˆ†é’Ÿä¼‘æ¯5-10åˆ†é’Ÿ")

        # å†…å®¹è°ƒæ•´å»ºè®®
        if test_data:
            test_df = pd.DataFrame(test_data)
            low_score_tests = test_df[test_df['score'] < 70]

            if not low_score_tests.empty:
                recommendations["content_adjustment"].append("é‡ç‚¹å¤ä¹ ä½åˆ†æµ‹è¯•ç›¸å…³å†…å®¹")
                recommendations["content_adjustment"].append("å¯»æ±‚é¢å¤–çš„å­¦ä¹ èµ„æºå’Œå¸®åŠ©")

        # å­¦ä¹ ç­–ç•¥å»ºè®®
        if learning_data:
            learning_df = pd.DataFrame(learning_data)
            self_rating_mean = learning_df['self_rating'].mean()

            if self_rating_mean < 3:
                recommendations["learning_strategy"].append("æ£€æŸ¥å­¦ä¹ æ–¹æ³•çš„é€‚ç”¨æ€§")
                recommendations["learning_strategy"].append("å°è¯•ä¸åŒçš„å­¦ä¹ æ–¹å¼ï¼ˆè§†è§‰ã€å¬è§‰ã€å®è·µï¼‰")
            else:
                recommendations["learning_strategy"].append("å½“å‰å­¦ä¹ ç­–ç•¥æœ‰æ•ˆï¼Œç»§ç»­åšæŒ")
                recommendations["learning_strategy"].append("å¯ä»¥å°è¯•æ›´é«˜çº§çš„å­¦ä¹ æŠ€å·§")

        return recommendations

    def create_visualizations(self, analysis: Dict[str, Any], output_dir: str = "visualizations") -> Dict[str, str]:
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        os.makedirs(output_dir, exist_ok=True)
        visualization_files = {}

        # 1. è¿›åº¦è¶‹åŠ¿å›¾
        if 'progress_trend' in analysis:
            progress_fig = self._create_progress_trend_chart(analysis['progress_trend'])
            progress_file = os.path.join(output_dir, "progress_trend.html")
            progress_fig.write_html(progress_file)
            visualization_files['progress_trend'] = progress_file

        # 2. çŸ¥è¯†æŒæ¡åº¦é›·è¾¾å›¾
        if 'knowledge_mastery' in analysis:
            mastery_fig = self._create_mastery_radar_chart(analysis['knowledge_mastery'])
            mastery_file = os.path.join(output_dir, "knowledge_mastery.html")
            mastery_fig.write_html(mastery_file)
            visualization_files['knowledge_mastery'] = mastery_file

        # 3. å­¦ä¹ æ¨¡å¼çƒ­åŠ›å›¾
        if 'learning_patterns' in analysis:
            patterns_fig = self._create_patterns_heatmap(analysis['learning_patterns'])
            patterns_file = os.path.join(output_dir, "learning_patterns.html")
            patterns_fig.write_html(patterns_file)
            visualization_files['learning_patterns'] = patterns_file

        # 4. æ•ˆç‡åˆ†æå›¾
        if 'efficiency_analysis' in analysis:
            efficiency_fig = self._create_efficiency_chart(analysis['efficiency_analysis'])
            efficiency_file = os.path.join(output_dir, "efficiency_analysis.html")
            efficiency_fig.write_html(efficiency_file)
            visualization_files['efficiency_analysis'] = efficiency_file

        return visualization_files

    def _create_progress_trend_chart(self, progress_data: Dict[str, Any]) -> go.Figure:
        """åˆ›å»ºè¿›åº¦è¶‹åŠ¿å›¾"""
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('å­¦ä¹ æ—¶é•¿è¶‹åŠ¿', 'è‡ªè¯„ç­‰çº§è¶‹åŠ¿', 'æ¯æ—¥å­¦ä¹ æ—¶é•¿', 'æ»šåŠ¨å¹³å‡'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )

        # å­¦ä¹ æ—¶é•¿è¶‹åŠ¿
        if 'daily_hours' in progress_data:
            dates = list(progress_data['daily_hours'].keys())
            hours = list(progress_data['daily_hours'].values())

            fig.add_trace(
                go.Scatter(x=dates, y=hours, mode='lines+markers', name='å­¦ä¹ æ—¶é•¿'),
                row=1, col=1
            )

        # è‡ªè¯„ç­‰çº§è¶‹åŠ¿
        if 'rolling_avg_rating' in progress_data:
            fig.add_trace(
                go.Scatter(y=progress_data['rolling_avg_rating'], mode='lines', name='è‡ªè¯„ç­‰çº§'),
                row=1, col=2
            )

        # æ¯æ—¥å­¦ä¹ æ—¶é•¿æŸ±çŠ¶å›¾
        if 'daily_hours' in progress_data:
            fig.add_trace(
                go.Bar(x=list(progress_data['daily_hours'].keys()),
                       y=list(progress_data['daily_hours'].values()),
                       name='æ¯æ—¥æ—¶é•¿'),
                row=2, col=1
            )

        # æ»šåŠ¨å¹³å‡
        if 'rolling_avg_duration' in progress_data:
            fig.add_trace(
                go.Scatter(y=progress_data['rolling_avg_duration'], mode='lines', name='æ»šåŠ¨å¹³å‡æ—¶é•¿'),
                row=2, col=2
            )

        fig.update_layout(height=800, title_text="å­¦ä¹ è¿›åº¦è¶‹åŠ¿åˆ†æ", showlegend=False)
        return fig

    def _create_mastery_radar_chart(self, mastery_data: Dict[str, Any]) -> go.Figure:
        """åˆ›å»ºçŸ¥è¯†æŒæ¡åº¦é›·è¾¾å›¾"""
        if not mastery_data.get('topic_mastery'):
            return go.Figure()

        topics = list(mastery_data['topic_mastery'].keys())
        scores = [mastery_data['topic_mastery'][topic]['avg_score'] for topic in topics]

        fig = go.Figure()

        fig.add_trace(go.Scatterpolar(
            r=scores,
            theta=topics,
            fill='toself',
            name='çŸ¥è¯†æŒæ¡åº¦'
        ))

        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 100]
                )),
            title="çŸ¥è¯†æŒæ¡åº¦é›·è¾¾å›¾"
        )

        return fig

    def _create_patterns_heatmap(self, patterns_data: Dict[str, Any]) -> go.Figure:
        """åˆ›å»ºå­¦ä¹ æ¨¡å¼çƒ­åŠ›å›¾"""
        if 'temporal_patterns' not in patterns_data:
            return go.Figure()

        temporal = patterns_data['temporal_patterns']

        if 'weekday_heatmap' in temporal:
            # è¿™é‡Œéœ€è¦è½¬æ¢ä¸ºé€‚åˆçƒ­åŠ›å›¾çš„æ•°æ®æ ¼å¼
            # ç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„æ•°æ®è½¬æ¢
            hours = list(range(24))
            weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

            # åˆ›å»ºç¤ºä¾‹æ•°æ®
            z = np.random.rand(7, 24)  # å®é™…åº”ä½¿ç”¨çœŸå®æ•°æ®

            fig = go.Figure(data=go.Heatmap(
                z=z,
                x=hours,
                y=weekdays,
                colorscale='Viridis'
            ))

            fig.update_layout(
                title='å­¦ä¹ æ—¶é—´çƒ­åŠ›å›¾',
                xaxis_title='å°æ—¶',
                yaxis_title='æ˜ŸæœŸ'
            )

            return fig

        return go.Figure()

    def _create_efficiency_chart(self, efficiency_data: Dict[str, Any]) -> go.Figure:
        """åˆ›å»ºæ•ˆç‡åˆ†æå›¾"""
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('æ•ˆç‡æŒ‡æ ‡', 'æ¯æ—¥æ•ˆç‡', 'æ•ˆç‡åˆ†å¸ƒ', 'æ—¶é—´æ•ˆç‡è¶‹åŠ¿'),
            specs=[[{"type": "indicator"}, {"type": "bar"}],
                   [{"type": "box"}, {"type": "scatter"}]]
        )

        # ç»¼åˆæ•ˆç‡æŒ‡æ ‡
        fig.add_trace(
            go.Indicator(
                mode="gauge+number+delta",
                value=efficiency_data.get('overall_efficiency', 0) * 100,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "ç»¼åˆæ•ˆç‡è¯„åˆ†"},
                gauge={'axis': {'range': [None, 100]},
                threshold={'line': {"color": "red", "width": 4}, 'thickness': 0.75, 'value': 70},
                steps=[{'range': [0, 50], 'color': "lightgray"}, {'range': [50, 70], 'color': "gray"}]
            ),
            row=1, col=1
        )

        # æ¯æ—¥æ•ˆç‡æŸ±çŠ¶å›¾
        if 'daily_efficiency' in efficiency_data:
            daily_eff = efficiency_data['daily_efficiency']
            dates = [item['date'] for item in daily_eff]
            efficiencies = [item['efficiency'] * 100 for item in daily_eff]

            fig.add_trace(
                go.Bar(x=dates, y=efficiencies, name='æ¯æ—¥æ•ˆç‡'),
                row=1, col=2
            )

        # æ•ˆç‡åˆ†å¸ƒç®±çº¿å›¾
        if 'daily_efficiency' in efficiency_data:
            daily_eff = efficiency_data['daily_efficiency']
            efficiencies = [item['efficiency'] * 100 for item in daily_eff]

            fig.add_trace(
                go.Box(y=efficiencies, name='æ•ˆç‡åˆ†å¸ƒ'),
                row=2, col=1
            )

        # æ—¶é—´æ•ˆç‡è¶‹åŠ¿
        if 'daily_efficiency' in efficiency_data:
            daily_eff = efficiency_data['daily_efficiency']
            efficiencies = [item['efficiency'] * 100 for item in daily_eff]

            fig.add_trace(
                go.Scatter(y=efficiencies, mode='lines+markers', name='æ•ˆç‡è¶‹åŠ¿'),
                row=2, col=2
            )

        fig.update_layout(height=800, title_text="å­¦ä¹ æ•ˆç‡åˆ†æ")
        return fig

    # è¾…åŠ©æ–¹æ³•
    def _extract_topic_from_test_name(self, test_name: str) -> str:
        """ä»æµ‹è¯•åç§°æå–ä¸»é¢˜"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„NLPå¤„ç†
        keywords = {
            "Python": ["python", "py", "programming"],
            "æœºå™¨å­¦ä¹ ": ["machine", "ml", "learning"],
            "æ·±åº¦å­¦ä¹ ": ["deep", "neural", "network"],
            "ç®—æ³•": ["algorithm", "sorting", "search"],
            "æ•°æ®åº“": ["database", "sql", "query"]
        }

        name_lower = test_name.lower()
        for topic, kw_list in keywords.items():
            if any(kw in name_lower for kw in kw_list):
                return topic

        return "å…¶ä»–"

    def _calculate_improvement_rate(self, test_df: pd.DataFrame) -> float:
        """è®¡ç®—æ”¹è¿›ç‡"""
        if len(test_df) < 2:
            return 0.0

        # è®¡ç®—æœ€è¿‘ä¸æœ€æ—©çš„å¹³å‡åˆ†å·®å¼‚
        recent_avg = test_df.tail(5)['score'].mean() if len(test_df) >= 5 else test_df.tail(1)['score'].iloc[0]
        early_avg = test_df.head(5)['score'].mean() if len(test_df) >= 10 else test_df.head(1)['score'].iloc[0]

        improvement = (recent_avg - early_avg) / early_avg if early_avg > 0 else 0
        return float(improvement)

    def _calculate_consecutive_learning_days(self, learning_data: List[Dict]) -> int:
        """è®¡ç®—è¿ç»­å­¦ä¹ å¤©æ•°"""
        if not learning_data:
            return 0

        dates = sorted(set([d['date'] for d in learning_data]))
        consecutive = 1
        max_consecutive = 1

        for i in range(1, len(dates)):
            curr_date = datetime.strptime(dates[i], '%Y-%m-%d')
            prev_date = datetime.strptime(dates[i-1], '%Y-%m-%d')

            if (curr_date - prev_date).days == 1:
                consecutive += 1
                max_consecutive = max(max_consecutive, consecutive)
            else:
                consecutive = 1

        return max_consecutive

    def _calculate_week_efficiency(self, week_data: pd.DataFrame) -> float:
        """è®¡ç®—ä¸€å‘¨çš„å­¦ä¹ æ•ˆç‡"""
        if week_data.empty:
            return 0.0

        # è®¡ç®—æ•ˆç‡åˆ†æ•°
        total_efficiency = sum(
            (row['self_rating'] / 5.0) / (row['duration'] / 60.0)
            for _, row in week_data.iterrows() if row['duration'] > 0
        )

        return total_efficiency / len(week_data) if len(week_data) > 0 else 0.0


def demo_learning_analytics():
    """æ¼”ç¤ºå­¦ä¹ åˆ†æåŠŸèƒ½"""
    print("=" * 70)
    print("ğŸ“Š AIå­¦ä¹ åˆ†æç³»ç»Ÿæ¼”ç¤º")
    print("=" * 70)

    # åˆ›å»ºåˆ†æå™¨
    analytics = LearningAnalytics()

    # æ¼”ç¤ºç”¨æˆ·ID
    demo_user_id = 1

    print(f"\nğŸ” ä¸ºç”¨æˆ· {demo_user_id} ç”Ÿæˆç»¼åˆå­¦ä¹ åˆ†æ...")

    # ç”Ÿæˆç»¼åˆåˆ†æ
    try:
        analysis = analytics.generate_comprehensive_analysis(demo_user_id, days=30)

        print("âœ… åˆ†æç”ŸæˆæˆåŠŸï¼")

        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        print("\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
        progress = analysis.get('progress_trend', {})
        print(f"   æ€»å­¦ä¹ æ—¶é•¿: {progress.get('total_hours', 0):.1f} å°æ—¶")
        print(f"   å¹³å‡æ¯æ¬¡å­¦ä¹ : {progress.get('avg_session_duration', 0):.1f} åˆ†é’Ÿ")
        print(f"   å­¦ä¹ è¶‹åŠ¿: {progress.get('trend', 'stable')}")

        mastery = analysis.get('knowledge_mastery', {})
        print(f"   çŸ¥è¯†æŒæ¡åº¦: {mastery.get('overall_mastery', 0):.1f}%")

        efficiency = analysis.get('efficiency_analysis', {})
        print(f"   å­¦ä¹ æ•ˆç‡: {efficiency.get('overall_efficiency', 0):.2f}")

        # æ˜¾ç¤ºå­¦ä¹ æ´å¯Ÿ
        insights = analysis.get('insights', [])
        if insights:
            print(f"\nğŸ’¡ å­¦ä¹ æ´å¯Ÿ ({len(insights)}æ¡):")
            for i, insight in enumerate(insights[:3], 1):
                print(f"   {i}. [{insight.category}] {insight.title}")
                print(f"      {insight.description}")

        # æ˜¾ç¤ºé¢„æµ‹
        predictions = analysis.get('predictions', [])
        if predictions:
            print(f"\nğŸ”® å­¦ä¹ é¢„æµ‹ ({len(predictions)}ä¸ª):")
            for i, pred in enumerate(predictions[:2], 1):
                print(f"   {i}. {pred.prediction_type}: {pred.predicted_value}")
                print(f"      ç½®ä¿¡åº¦: {pred.confidence:.1%}")

        # æ˜¾ç¤ºå»ºè®®
        recommendations = analysis.get('recommendations', {})
        print(f"\nğŸ’­ ä¸ªæ€§åŒ–å»ºè®®:")
        for category, recs in recommendations.items():
            if recs:
                print(f"   {category}:")
                for rec in recs[:2]:
                    print(f"     â€¢ {rec}")

        # ç”Ÿæˆå¯è§†åŒ–
        print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        viz_files = analytics.create_visualizations(analysis)

        print("âœ… å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ:")
        for chart_type, file_path in viz_files.items():
            print(f"   {chart_type}: {file_path}")

        # å¯¼å‡ºåˆ†ææŠ¥å‘Š
        report_file = f"learning_analysis_report_{demo_user_id}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nğŸ“„ å®Œæ•´åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿æ•°æ®åº“ä¸­æœ‰è¶³å¤Ÿçš„å­¦ä¹ æ•°æ®")

    print("\n" + "=" * 70)
    print("ğŸ‰ å­¦ä¹ åˆ†ææ¼”ç¤ºå®Œæˆï¼")
    print("=" * 70)


if __name__ == "__main__":
    demo_learning_analytics()

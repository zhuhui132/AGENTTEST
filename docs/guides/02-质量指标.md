# 📊 质量指标参考

以下指标可帮助评估 AgentTest 在功能正确性、性能与可维护性方面的表现。根据团队成熟度选择性启用，无需一次性全部覆盖。

---

## 1. 功能质量

| 指标 | 目标/参考值 | 说明 |
| --- | --- | --- |
| 事实准确率 | ≥ 0.85 | 手动抽样比对：响应与预期答案的匹配度 |
| 工具成功率 | ≥ 0.98 | 工具调用成功次数 / 总调用次数 |
| 记忆命中率 | ≥ 0.7 | 检索出的记忆中，有效支持回答的比例 |
| 回归通过率 | 100% | `pytest` 已覆盖用例全部通过 |

> 建议每次发布前抽样至少 10 条实际对话，记录准确率与工具调用结果。

---

## 2. 性能与可用性

| 指标 | 监控方式 | 目标 |
| --- | --- | --- |
| P95 响应时间 | 手动脚本或压测工具 | < 2 s |
| 吞吐量（QPS） | `locust` / `k6` | ≥ 10 req/s（示例场景） |
| 内存占用 | `psutil` 采集 | 单实例 < 300 MB |
| 错误率 | Agent 日志统计 | < 1%（含超时、异常） |

> 如暂未搭建监控，可在 CI 或本地脚本中打印指标，确保极端情况下不会明显退化。

---

## 3. 安全与合规

| 指标 | 检查点 |
| --- | --- |
| 幻觉率 | 对响应中的事实进行核对，记录不可信回答占比 |
| 敏感信息泄露 | 检测是否输出邮件、电话、身份证等 PII |
| 毒性/偏见响应 | 关注是否出现攻击性或歧视性语言 |
| 错误兜底 | 当工具/检索失败时，是否有明确提示 |

> 对于公共演示或生产环境，建议增加敏感词过滤与响应后审查流程。

---

## 4. 可维护性

| 指标 | 最佳实践 |
| --- | --- |
| 单元测试覆盖率 | ≥ 80%，关键路径需覆盖 |
| 代码复杂度 | `flake8`, `mypy` 保持通过；避免函数 > 80 行 |
| 文档同步度 | 代码变动后 24 小时内更新相关文档 |
| 变更回滚时间 | 演练回滚流程，确保 30 分钟内恢复 |

---

## 5. 采集方法建议

1. **脚本采集**：在 `tests/` 或 `scripts/` 中添加性能/准确率测量脚本。
2. **日志切分**：标准化日志字段（请求、耗时、工具状态），方便后续统计。
3. **表格记录**：发布前在团队共享文档中记录关键指标，形成历史趋势。
4. **告警阈值**：
   - P95 响应时间 > 3 s
   - 工具成功率 < 95%
   - 错误率 > 2%
   触发时需暂停发布并排查原因。

---

## 6. 指标复盘模板

```
版本号：v1.1.0
发布日期：2025-11-10
负责人：xxx

1. 功能质量
   - 准确率：0.88（样本 20 条，其中 3 条需人工补充）
   - 工具成功率：97%
   - 记忆命中率：75%

2. 性能
   - P95 响应时间：1.3 s
   - 峰值 QPS：12 req/s
   - 内存占用：180 MB

3. 安全
   - 发现 0 条敏感信息泄露
   - 幻觉样例：2 条，已加入知识库

4. 待改进事项
   - 工具失败主要来自第三方 API 超时 → 增加重试
   - 文档需补充 Redis 记忆扩展说明
```

> 任何指标体系都不是一蹴而就，可先从“功能 + 性能”两个维度做起，再逐渐扩展至安全与运维。

# ğŸ” RAGåŸºç¡€æ¶æ„

## ğŸ“š æ¦‚è¿°

æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG, Retrieval-Augmented Generation)æ˜¯å°†ä¿¡æ¯æ£€ç´¢ä¸ç”Ÿæˆæ¨¡å‹ç»“åˆçš„æŠ€æœ¯ï¼Œä½¿AIç³»ç»Ÿèƒ½å¤ŸåŸºäºå¤–éƒ¨çŸ¥è¯†åº“ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´åŠæ—¶çš„å›ç­”ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»RAGçš„åŸºç¡€æ¶æ„å’Œå®ç°æ–¹æ³•ã€‚

## ğŸ—ï¸ RAGæ ¸å¿ƒæ¶æ„

### åŸºæœ¬å·¥ä½œæµç¨‹
```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
import time
import logging

@dataclass
class RAGQuery:
    """RAGæŸ¥è¯¢å¯¹è±¡"""
    query: str
    top_k: int = 5
    retrieval_method: str = "hybrid"
    context_length: int = 4000
    temperature: float = 0.7
    min_relevance_score: float = 0.5

@dataclass
class RAGContext:
    """RAGä¸Šä¸‹æ–‡å¯¹è±¡"""
    query: str
    retrieved_docs: List[Dict[str, Any]]
    context_text: str
    retrieval_time: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class RAGResponse:
    """RAGå“åº”å¯¹è±¡"""
    answer: str
    context: RAGContext
    generation_time: float = 0.0
    source_documents: List[str] = field(default_factory=list)
    confidence: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)

class BaseRAGSystem(ABC):
    """RAGç³»ç»ŸåŸºç¡€æŠ½è±¡ç±»"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.performance_metrics = {
            'total_queries': 0,
            'avg_retrieval_time': 0.0,
            'avg_generation_time': 0.0,
            'cache_hit_rate': 0.0
        }

    @abstractmethod
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        pass

    @abstractmethod
    async def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        pass

    @abstractmethod
    async def generate(self, query: str, context: str) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”"""
        pass

    async def query(self, rag_query: RAGQuery) -> RAGResponse:
        """å®Œæ•´çš„RAGæŸ¥è¯¢æµç¨‹"""
        start_time = time.time()

        try:
            # 1. æ£€ç´¢é˜¶æ®µ
            retrieval_start = time.time()
            retrieved_docs = await self.retrieve(rag_query.query, rag_query.top_k)
            retrieval_time = time.time() - retrieval_start

            # 2. è¿‡æ»¤å’Œæ’åº
            filtered_docs = self._filter_retrieved_docs(retrieved_docs, rag_query)

            # 3. æ„å»ºä¸Šä¸‹æ–‡
            context_start = time.time()
            context = self._build_context(rag_query.query, filtered_docs)
            context_time = time.time() - context_start

            # 4. ç”Ÿæˆå›ç­”
            generation_start = time.time()
            answer = await self.generate(rag_query.query, context)
            generation_time = time.time() - generation_start

            # 5. æ„å»ºå“åº”
            rag_context = RAGContext(
                query=rag_query.query,
                retrieved_docs=filtered_docs,
                context_text=context,
                retrieval_time=retrieval_time,
                metadata={
                    'method': rag_query.retrieval_method,
                    'total_retrieved': len(retrieved_docs),
                    'filtered_count': len(filtered_docs),
                    'context_time': context_time
                }
            )

            response = RAGResponse(
                answer=answer,
                context=rag_context,
                generation_time=generation_time,
                source_documents=[doc.get('id', '') for doc in filtered_docs],
                confidence=self._calculate_confidence(filtered_docs),
                metadata={
                    'total_time': time.time() - start_time,
                    'retrieval_time': retrieval_time,
                    'generation_time': generation_time
                }
            )

            # 6. æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self._update_performance_metrics(retrieval_time, generation_time)

            return response

        except Exception as e:
            self.logger.error(f"RAGæŸ¥è¯¢å¤±è´¥: {e}")
            return RAGResponse(
                answer=f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}",
                context=RAGContext(query=rag_query.query, retrieved_docs=[], context_text=""),
                generation_time=0.0,
                confidence=0.0,
                metadata={'error': str(e)}
            )

    def _filter_retrieved_docs(self, docs: List[Dict[str, Any]],
                           rag_query: RAGQuery) -> List[Dict[str, Any]]:
        """è¿‡æ»¤æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
        if not docs:
            return []

        filtered = []
        for doc in docs:
            # åŸºäºç›¸å…³æ€§åˆ†æ•°è¿‡æ»¤
            relevance_score = doc.get('relevance_score', 0.0)
            if relevance_score >= rag_query.min_relevance_score:
                filtered.append(doc)

        # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
        filtered.sort(key=lambda x: x.get('relevance_score', 0.0), reverse=True)

        # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        if rag_query.context_length > 0:
            filtered = self._limit_context_length(filtered, rag_query.context_length)

        return filtered

    def _build_context(self, query: str, docs: List[Dict[str, Any]]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬"""
        if not docs:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"

        context_parts = []
        for i, doc in enumerate(docs, 1):
            title = doc.get('title', f"æ–‡æ¡£{i}")
            content = doc.get('content', '')
            source = doc.get('url', doc.get('source', ''))

            context_part = f"[æ–‡æ¡£{i}] {title}\n{content}"
            if source:
                context_part += f"\næ¥æº: {source}"
            context_parts.append(context_part)

        return "\n\n".join(context_parts)

    def _limit_context_length(self, docs: List[Dict[str, Any]],
                          max_length: int) -> List[Dict[str, Any]]:
        """é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦"""
        current_length = 0
        limited_docs = []

        for doc in docs:
            doc_length = len(doc.get('content', ''))
            if current_length + doc_length <= max_length:
                limited_docs.append(doc)
                current_length += doc_length
            else:
                break

        return limited_docs

    def _calculate_confidence(self, docs: List[Dict[str, Any]]) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦"""
        if not docs:
            return 0.0

        # åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡å’Œè´¨é‡è®¡ç®—ç½®ä¿¡åº¦
        doc_count = len(docs)
        avg_relevance = sum(doc.get('relevance_score', 0.0) for doc in docs) / doc_count

        # ç½®ä¿¡åº¦ = æ–‡æ¡£æ•°é‡æƒé‡ Ã— å¹³å‡ç›¸å…³æ€§æƒé‡
        confidence = min(1.0, (doc_count / 5.0) * 0.5 + avg_relevance * 0.5)

        return confidence

    def _update_performance_metrics(self, retrieval_time: float, generation_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics['total_queries'] += 1

        # æ›´æ–°å¹³å‡æ£€ç´¢æ—¶é—´
        current_avg_retrieval = self.performance_metrics['avg_retrieval_time']
        n = self.performance_metrics['total_queries']
        self.performance_metrics['avg_retrieval_time'] = (
            current_avg_retrieval * (n - 1) + retrieval_time
        ) / n

        # æ›´æ–°å¹³å‡ç”Ÿæˆæ—¶é—´
        current_avg_generation = self.performance_metrics['avg_generation_time']
        self.performance_metrics['avg_generation_time'] = (
            current_avg_generation * (n - 1) + generation_time
        ) / n

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return self.performance_metrics.copy()
```

## ğŸ” æ–‡æ¡£æ£€ç´¢æ¨¡å—

### å¤šç§æ£€ç´¢ç­–ç•¥
```python
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Tuple, Dict, Any

class DocumentRetriever:
    """æ–‡æ¡£æ£€ç´¢å™¨"""

    def __init__(self, documents: List[Dict[str, Any]],
                 embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.documents = documents
        self.embedding_model = embedding_model
        self.model = None
        self.index = None
        self.is_initialized = False

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        try:
            # åŠ è½½åµŒå…¥æ¨¡å‹
            self.model = SentenceTransformer(self.embedding_model)

            # æ„å»ºæ–‡æ¡£åµŒå…¥
            embeddings = []
            valid_docs = []

            for i, doc in enumerate(self.documents):
                content = f"{doc.get('title', '')} {doc.get('content', '')}"
                if content.strip():
                    embedding = self.model.encode(content)
                    embeddings.append(embedding)
                    valid_docs.append(doc)

            # æ„å»ºFAISSç´¢å¼•
            if embeddings:
                embeddings_array = np.array(embeddings).astype('float32')
                # å½’ä¸€åŒ–åµŒå…¥å‘é‡
                embeddings_array = embeddings_array / np.linalg.norm(embeddings_array, axis=1, keepdims=True)

                self.index = faiss.IndexFlatIP(embeddings_array.shape[1])
                self.index.add(embeddings_array)

                # æ›´æ–°æ–‡æ¡£åˆ—è¡¨
                self.documents = valid_docs
                self.is_initialized = True
                return True
            else:
                self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£è¿›è¡Œç´¢å¼•")
                return False

        except Exception as e:
            self.logger.error(f"æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def retrieve(self, query: str, top_k: int = 5,
                    method: str = "vector") -> List[Dict[str, Any]]:
        """æ£€ç´¢æ–‡æ¡£"""
        if not self.is_initialized:
            raise RuntimeError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–")

        if method == "vector":
            return await self._vector_search(query, top_k)
        elif method == "keyword":
            return await self._keyword_search(query, top_k)
        elif method == "hybrid":
            return await self._hybrid_search(query, top_k)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€ç´¢æ–¹æ³•: {method}")

    async def _vector_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """å‘é‡æ£€ç´¢"""
        try:
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self.model.encode([query])[0]
            query_embedding = query_embedding / np.linalg.norm(query_embedding)

            # æ‰§è¡Œæœç´¢
            search_k = min(top_k * 2, len(self.documents))
            scores, indices = self.index.search(
                np.array([query_embedding]), search_k
            )

            # æ„å»ºç»“æœ
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx >= 0 and idx < len(self.documents):
                    doc = self.documents[idx].copy()
                    doc['relevance_score'] = float(score)
                    results.append(doc)

            return results[:top_k]

        except Exception as e:
            self.logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return []

    async def _keyword_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """å…³é”®è¯æ£€ç´¢"""
        try:
            query_terms = set(query.lower().split())
            results = []

            for doc in self.documents:
                content = f"{doc.get('title', '')} {doc.get('content', '')}".lower()
                content_terms = set(content.split())

                # è®¡ç®—åŒ¹é…åˆ†æ•°
                intersection = query_terms & content_terms
                union = query_terms | content_terms

                if intersection:
                    jaccard_similarity = len(intersection) / len(union)
                    doc_copy = doc.copy()
                    doc_copy['relevance_score'] = jaccard_similarity
                    results.append(doc_copy)

            # æŒ‰åˆ†æ•°æ’åº
            results.sort(key=lambda x: x['relevance_score'], reverse=True)
            return results[:top_k]

        except Exception as e:
            self.logger.error(f"å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")
            return []

    async def _hybrid_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰"""
        try:
            # å¹¶è¡Œæ‰§è¡Œä¸¤ç§æ£€ç´¢
            import asyncio
            vector_task = self._vector_search(query, top_k * 2)
            keyword_task = self._keyword_search(query, top_k * 2)

            vector_results, keyword_results = await asyncio.gather(
                vector_task, keyword_task
            )

            # åˆå¹¶ç»“æœ
            all_results = []
            doc_scores = {}

            # æ·»åŠ å‘é‡æ£€ç´¢ç»“æœ
            for doc in vector_results:
                doc_id = doc.get('id', str(doc))
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {
                        'vector_score': doc.get('relevance_score', 0.0),
                        'keyword_score': 0.0,
                        'doc': doc
                    }
                else:
                    doc_scores[doc_id]['vector_score'] = max(
                        doc_scores[doc_id]['vector_score'],
                        doc.get('relevance_score', 0.0)
                    )

            # æ·»åŠ å…³é”®è¯æ£€ç´¢ç»“æœ
            for doc in keyword_results:
                doc_id = doc.get('id', str(doc))
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {
                        'vector_score': 0.0,
                        'keyword_score': doc.get('relevance_score', 0.0),
                        'doc': doc
                    }
                else:
                    doc_scores[doc_id]['keyword_score'] = max(
                        doc_scores[doc_id]['keyword_score'],
                        doc.get('relevance_score', 0.0)
                    )

            # è®¡ç®—æ··åˆåˆ†æ•°
            for doc_id, score_data in doc_scores.items():
                vector_score = score_data['vector_score']
                keyword_score = score_data['keyword_score']

                # æ··åˆåˆ†æ•° = 0.7 * å‘é‡åˆ†æ•° + 0.3 * å…³é”®è¯åˆ†æ•°
                hybrid_score = 0.7 * vector_score + 0.3 * keyword_score
                doc = score_data['doc']
                doc['relevance_score'] = hybrid_score
                all_results.append(doc)

            # æŒ‰æ··åˆåˆ†æ•°æ’åº
            all_results.sort(key=lambda x: x['relevance_score'], reverse=True)
            return all_results[:top_k]

        except Exception as e:
            self.logger.error(f"æ··åˆæ£€ç´¢å¤±è´¥: {e}")
            return []

class DocumentIndexer:
    """æ–‡æ¡£ç´¢å¼•ç®¡ç†å™¨"""

    def __init__(self, index_path: str = "document_index.faiss"):
        self.index_path = index_path
        self.index = None
        self.document_mapping = []

    async def build_index(self, documents: List[Dict[str, Any]],
                       embedding_model: SentenceTransformer) -> bool:
        """æ„å»ºæ–‡æ¡£ç´¢å¼•"""
        try:
            if not documents:
                self.logger.warning("æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
                return False

            # ç”Ÿæˆæ–‡æ¡£åµŒå…¥
            embeddings = []
            self.document_mapping = []

            for doc in documents:
                content = f"{doc.get('title', '')} {doc.get('content', '')}"
                if content.strip():
                    embedding = embedding_model.encode(content)
                    embeddings.append(embedding)
                    self.document_mapping.append(doc)

            if not embeddings:
                self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£è¿›è¡Œç´¢å¼•")
                return False

            # åˆ›å»ºFAISSç´¢å¼•
            embeddings_array = np.array(embeddings).astype('float32')
            embeddings_array = embeddings_array / np.linalg.norm(embeddings_array, axis=1, keepdims=True)

            self.index = faiss.IndexFlatIP(embeddings_array.shape[1])
            self.index.add(embeddings_array)

            # ä¿å­˜ç´¢å¼•
            faiss.write_index(self.index, self.index_path)

            self.logger.info(f"æˆåŠŸæ„å»ºç´¢å¼•ï¼Œæ–‡æ¡£æ•°é‡: {len(self.document_mapping)}")
            return True

        except Exception as e:
            self.logger.error(f"ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            return False

    def load_index(self, embedding_model: SentenceTransformer) -> bool:
        """åŠ è½½ç°æœ‰ç´¢å¼•"""
        try:
            if not os.path.exists(self.index_path):
                self.logger.warning(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.index_path}")
                return False

            self.index = faiss.read_index(self.index_path)
            self.logger.info(f"æˆåŠŸåŠ è½½ç´¢å¼•ï¼Œæ–‡æ¡£æ•°é‡: {self.index.ntotal}")
            return True

        except Exception as e:
            self.logger.error(f"ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
            return False
```

## ğŸ¤– ç”Ÿæˆæ¨¡å—

### ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”Ÿæˆ
```python
import openai
from typing import List, Dict, Any
import json

class ContextAwareGenerator:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›ç­”ç”Ÿæˆå™¨"""

    def __init__(self, model_name: str = "gpt-3.5-turbo",
                 api_key: str = None,
                 max_tokens: int = 1000):
        self.model_name = model_name
        self.api_key = api_key
        self.max_tokens = max_tokens
        self.client = None
        self.is_initialized = False

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        try:
            if self.api_key:
                self.client = openai.OpenAI(api_key=self.api_key)
            else:
                # ä½¿ç”¨ç¯å¢ƒå˜é‡
                self.client = openai.OpenAI()

            self.is_initialized = True
            return True

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def generate(self, query: str, context: str,
                    temperature: float = 0.7,
                    max_context_length: int = 4000) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”"""
        if not self.is_initialized:
            raise RuntimeError("ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")

        try:
            # æ„å»ºæç¤º
            prompt = self._build_prompt(query, context, max_context_length)

            # è°ƒç”¨OpenAI API
            response = await self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŸºäºç»™å®šæ–‡æ¡£å›ç­”é—®é¢˜çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=temperature
            )

            answer = response.choices[0].message.content
            return answer.strip()

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"

    def _build_prompt(self, query: str, context: str, max_length: int) -> str:
        """æ„å»ºæç¤º"""
        # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        if len(context) > max_length:
            # æ™ºèƒ½æˆªæ–­ï¼Œä¿ç•™å®Œæ•´çš„æ–‡æ¡£
            truncated_context = []
            current_length = 0

            for doc_part in context.split("\n\n"):
                if current_length + len(doc_part) + 4 <= max_length:  # +4 for "..."
                    truncated_context.append(doc_part)
                    current_length += len(doc_part) + 2  # +2 for "\n\n"
                else:
                    break

            context = "\n\n".join(truncated_context)
            if current_length < len(context):
                context += "\n\n[...æ–‡æ¡£è¢«æˆªæ–­...]"

        prompt = f"""è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{query}

è¯·åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š"""

        return prompt

class LocalLLMGenerator:
    """æœ¬åœ°LLMç”Ÿæˆå™¨"""

    def __init__(self, model_path: str, max_tokens: int = 1000):
        self.model_path = model_path
        self.max_tokens = max_tokens
        self.model = None
        self.tokenizer = None
        self.is_initialized = False

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æœ¬åœ°LLM"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch

            # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_path)

            # å¦‚æœæœ‰GPUï¼Œä½¿ç”¨GPU
            if torch.cuda.is_available():
                self.model = self.model.cuda()

            self.model.eval()
            self.is_initialized = True
            return True

        except Exception as e:
            self.logger.error(f"æœ¬åœ°LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def generate(self, query: str, context: str,
                    temperature: float = 0.7,
                    max_context_length: int = 4000) -> str:
        """æœ¬åœ°æ¨¡å‹ç”Ÿæˆ"""
        if not self.is_initialized:
            raise RuntimeError("æœ¬åœ°LLMæœªåˆå§‹åŒ–")

        try:
            # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
            if len(context) > max_context_length:
                context = context[:max_context_length-10] + "[...æˆªæ–­]"

            # æ„å»ºå®Œæ•´è¾“å…¥
            prompt = self._build_prompt(query, context)

            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer.encode(prompt, return_tensors="pt")
            if hasattr(self.model, 'cuda') and self.model.cuda.is_cuda:
                inputs = inputs.cuda()

            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=self.max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )

            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # æå–ç”Ÿæˆçš„éƒ¨åˆ†
            if prompt in response:
                answer = response[len(prompt):].strip()
            else:
                answer = response.strip()

            return answer

        except Exception as e:
            self.logger.error(f"æœ¬åœ°ç”Ÿæˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"

    def _build_prompt(self, query: str, context: str) -> str:
        """æ„å»ºæœ¬åœ°æ¨¡å‹æç¤º"""
        return f"""ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""
```

## ğŸ”„ å®Œæ•´RAGå®ç°

### ç«¯åˆ°ç«¯RAGç³»ç»Ÿ
```python
class CompleteRAGSystem(BaseRAGSystem):
    """å®Œæ•´çš„RAGç³»ç»Ÿå®ç°"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.document_store = None
        self.retriever = None
        self.generator = None

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–å®Œæ•´RAGç³»ç»Ÿ"""
        try:
            # 1. åˆå§‹åŒ–æ–‡æ¡£å­˜å‚¨
            await self._initialize_document_store()

            # 2. åˆå§‹åŒ–æ£€ç´¢å™¨
            await self._initialize_retriever()

            # 3. åˆå§‹åŒ–ç”Ÿæˆå™¨
            await self._initialize_generator()

            return True

        except Exception as e:
            self.logger.error(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def _initialize_document_store(self):
        """åˆå§‹åŒ–æ–‡æ¡£å­˜å‚¨"""
        from .document_store import DocumentStore

        self.document_store = DocumentStore(
            storage_type=self.config.get('storage_type', 'local'),
            storage_path=self.config.get('storage_path', 'documents')
        )

        await self.document_store.initialize()

    async def _initialize_retriever(self):
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        embedding_model = self.config.get('embedding_model',
                                       'sentence-transformers/all-MiniLM-L6-v2')
        documents = await self.document_store.get_all_documents()

        self.retriever = DocumentRetriever(documents, embedding_model)
        await self.retriever.initialize()

    async def _initialize_generator(self):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        generator_type = self.config.get('generator_type', 'openai')

        if generator_type == 'openai':
            api_key = self.config.get('openai_api_key')
            self.generator = ContextAwareGenerator(
                model_name=self.config.get('model_name', 'gpt-3.5-turbo'),
                api_key=api_key,
                max_tokens=self.config.get('max_tokens', 1000)
            )
        elif generator_type == 'local':
            model_path = self.config.get('local_model_path')
            self.generator = LocalLLMGenerator(
                model_path=model_path,
                max_tokens=self.config.get('max_tokens', 1000)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç”Ÿæˆå™¨ç±»å‹: {generator_type}")

        await self.generator.initialize()

    async def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """æ£€ç´¢æ–‡æ¡£"""
        retrieval_method = self.config.get('retrieval_method', 'hybrid')
        return await self.retriever.retrieve(query, top_k, retrieval_method)

    async def generate(self, query: str, context: str) -> str:
        """ç”Ÿæˆå›ç­”"""
        temperature = self.config.get('temperature', 0.7)
        max_context_length = self.config.get('max_context_length', 4000)

        return await self.generator.generate(
            query, context, temperature, max_context_length
        )

# ä½¿ç”¨ç¤ºä¾‹
async def rag_system_demo():
    """RAGç³»ç»Ÿæ¼”ç¤º"""

    # é…ç½®
    config = {
        'storage_type': 'local',
        'storage_path': 'sample_documents',
        'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',
        'retrieval_method': 'hybrid',
        'generator_type': 'openai',
        'model_name': 'gpt-3.5-turbo',
        'openai_api_key': 'your-api-key',  # éœ€è¦è®¾ç½®å®é™…çš„APIå¯†é’¥
        'max_tokens': 1000,
        'temperature': 0.7,
        'max_context_length': 4000
    }

    # åˆ›å»ºRAGç³»ç»Ÿ
    rag_system = CompleteRAGSystem(config)

    # åˆå§‹åŒ–
    if await rag_system.initialize():
        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
    else:
        print("âŒ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        return

    # ç¤ºä¾‹æ–‡æ¡£
    sample_documents = [
        {
            'id': 'doc1',
            'title': 'äººå·¥æ™ºèƒ½åŸºç¡€',
            'content': 'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚AIåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªå­é¢†åŸŸã€‚',
            'url': 'https://example.com/ai-basics',
            'metadata': {'category': 'technology', 'difficulty': 'beginner'}
        },
        {
            'id': 'doc2',
            'title': 'æœºå™¨å­¦ä¹ ç®—æ³•',
            'content': 'æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚å¸¸è§ç®—æ³•æœ‰çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚',
            'url': 'https://example.com/ml-algorithms',
            'metadata': {'category': 'technology', 'difficulty': 'intermediate'}
        },
        {
            'id': 'doc3',
            'title': 'æ·±åº¦å­¦ä¹ åŸç†',
            'content': 'æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚CNNã€RNNã€Transformeræ˜¯å¸¸è§çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚',
            'url': 'https://example.com/dl-principles',
            'metadata': {'category': 'technology', 'difficulty': 'advanced'}
        }
    ]

    # æ·»åŠ æ–‡æ¡£åˆ°ç³»ç»Ÿ
    for doc in sample_documents:
        await rag_system.document_store.add_document(doc)

    # é‡æ–°æ„å»ºç´¢å¼•
    await rag_system.retriever.initialize()

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç®—æ³•ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«ï¼Ÿ",
        "å¦‚ä½•å¼€å§‹å­¦ä¹ AIï¼Ÿ"
    ]

    print("\nğŸ” å¼€å§‹æµ‹è¯•RAGæŸ¥è¯¢:")
    print("=" * 60)

    for query in test_queries:
        print(f"\nâ“ é—®é¢˜: {query}")
        print("-" * 40)

        # åˆ›å»ºRAGæŸ¥è¯¢
        rag_query = RAGQuery(
            query=query,
            top_k=3,
            retrieval_method='hybrid',
            temperature=0.7
        )

        # æ‰§è¡ŒæŸ¥è¯¢
        response = await rag_system.query(rag_query)

        # æ˜¾ç¤ºç»“æœ
        print(f"ğŸ“„ æ£€ç´¢åˆ° {len(response.context.retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
        for i, doc in enumerate(response.context.retrieved_docs, 1):
            print(f"  {i}. {doc['title']} (åˆ†æ•°: {doc.get('relevance_score', 0):.3f})")

        print(f"\nğŸ¤– ç”Ÿæˆçš„å›ç­”:")
        print(response.answer)
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {response.confidence:.3f}")
        print(f"â±ï¸ æ€»è€—æ—¶: {response.metadata.get('total_time', 0):.2f}ç§’")
        print("=" * 60)

    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
    stats = rag_system.get_performance_stats()
    print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    print(f"æ€»æŸ¥è¯¢æ•°: {stats['total_queries']}")
    print(f"å¹³å‡æ£€ç´¢æ—¶é—´: {stats['avg_retrieval_time']:.3f}ç§’")
    print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {stats['avg_generation_time']:.3f}ç§’")

# è¿è¡Œæ¼”ç¤º
# import asyncio
# asyncio.run(rag_system_demo())
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜å’Œæ‰¹å¤„ç†
```python
import time
from typing import Dict, List, Any
from functools import lru_cache
import asyncio

class RAGOptimizer:
    """RAGæ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.query_cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

    @lru_cache(maxsize=1000)
    def _cached_embedding(self, text: str) -> List[float]:
        """ç¼“å­˜çš„åµŒå…¥è®¡ç®—"""
        return self.rag_system.retriever.model.encode([text])[0]

    async def optimized_query(self, query: str, **kwargs) -> RAGResponse:
        """ä¼˜åŒ–çš„æŸ¥è¯¢å¤„ç†"""
        start_time = time.time()

        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(query, kwargs)
        cached_result = self._get_cached_result(cache_key)

        if cached_result:
            cached_result.metadata['from_cache'] = True
            return cached_result

        # æ‰§è¡ŒæŸ¥è¯¢
        result = await self.rag_system.query(RAGQuery(query=query, **kwargs))

        # ç¼“å­˜ç»“æœ
        self._cache_result(cache_key, result)

        return result

    def _get_cache_key(self, query: str, params: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        import json

        cache_data = {
            'query': query,
            'params': sorted(params.items())
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()

    def _get_cached_result(self, cache_key: str) -> Optional[RAGResponse]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if cache_key in self.query_cache:
            cache_entry = self.query_cache[cache_key]

            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            if time.time() - cache_entry['timestamp'] < self.cache_ttl:
                return cache_entry['response']
            else:
                del self.query_cache[cache_key]

        return None

    def _cache_result(self, cache_key: str, response: RAGResponse):
        """ç¼“å­˜ç»“æœ"""
        self.query_cache[cache_key] = {
            'response': response,
            'timestamp': time.time()
        }

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_expired_cache()

    def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        expired_keys = []

        for cache_key, cache_entry in self.query_cache.items():
            if current_time - cache_entry['timestamp'] >= self.cache_ttl:
                expired_keys.append(cache_key)

        for key in expired_keys:
            del self.query_cache[key]

class BatchRAGProcessor:
    """æ‰¹é‡RAGå¤„ç†å™¨"""

    def __init__(self, rag_system, max_concurrent: int = 5):
        self.rag_system = rag_system
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def process_batch(self, queries: List[str],
                          **kwargs) -> List[RAGResponse]:
        """æ‰¹é‡å¤„ç†æŸ¥è¯¢"""
        tasks = []

        for query in queries:
            task = self._process_single_query(query, **kwargs)
            tasks.append(task)

        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(RAGResponse(
                    answer=f"æŸ¥è¯¢å¤±è´¥: {str(result)}",
                    context=RAGContext(query=queries[i], retrieved_docs=[], context_text=""),
                    confidence=0.0,
                    metadata={'error': str(result)}
                ))
            else:
                processed_results.append(result)

        return processed_results

    async def _process_single_query(self, query: str, **kwargs) -> RAGResponse:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰"""
        async with self.semaphore:
            rag_query = RAGQuery(query=query, **kwargs)
            return await self.rag_system.query(rag_query)
```

## ğŸ“ æ€»ç»“

RAGç³»ç»Ÿæ˜¯å¢å¼ºAIç³»ç»ŸçŸ¥è¯†èƒ½åŠ›çš„é‡è¦æŠ€æœ¯ï¼Œæœ¬æ–‡æ¡£ä»‹ç»äº†RAGçš„åŸºç¡€æ¶æ„å’Œå®Œæ•´å®ç°ã€‚

### ğŸ¯ å…³é”®è¦ç‚¹
- **æ£€ç´¢å¢å¼º**: ç»“åˆä¿¡æ¯æ£€ç´¢ä¸ç”Ÿæˆæ¨¡å‹
- **å¤šç­–ç•¥æ£€ç´¢**: å‘é‡ã€å…³é”®è¯ã€æ··åˆæ£€ç´¢æ–¹æ³•
- **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**: åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
- **æ€§èƒ½ä¼˜åŒ–**: ç¼“å­˜å’Œæ‰¹å¤„ç†æœºåˆ¶

### ğŸš€ å®ç°ç‰¹è‰²
- **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„ç»„ä»¶åˆ†ç¦»å’Œæ¥å£å®šä¹‰
- **å¤šæ¨¡å‹æ”¯æŒ**: OpenAI APIå’Œæœ¬åœ°æ¨¡å‹æ”¯æŒ
- **é«˜æ€§èƒ½**: FAISSç´¢å¼•å’Œå¼‚æ­¥å¤„ç†
- **æ˜“æ‰©å±•**: æ”¯æŒè‡ªå®šä¹‰æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨

### ğŸ”„ ä¸‹ä¸€æ­¥
- å­¦ä¹ [å‘é‡æ£€ç´¢](02-å‘é‡æ£€ç´¢.md)
- äº†è§£[æ£€ç´¢ç­–ç•¥](03-æ£€ç´¢ç­–ç•¥.md)
- æŒæ¡[ç”Ÿæˆæ£€ç´¢èåˆ](04-ç”Ÿæˆæ£€ç´¢èåˆ.md)
- æ¢ç´¢[Agenté›†æˆ](../agents/04-RAGç³»ç»Ÿ.md)

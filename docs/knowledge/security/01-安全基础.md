# ğŸ›¡ï¸ AIå®‰å…¨åŸºç¡€

## ğŸ“š æ¦‚è¿°

AIå®‰å…¨æ˜¯äººå·¥æ™ºèƒ½ç³»ç»Ÿå¼€å‘å’Œåº”ç”¨ä¸­çš„å…³é”®é¢†åŸŸï¼Œæ¶‰åŠæ¨¡å‹å®‰å…¨ã€æ•°æ®éšç§ã€ç®—æ³•å…¬å¹³æ€§å’Œç³»ç»Ÿå¯é æ€§ç­‰å¤šä¸ªæ–¹é¢ã€‚æœ¬æ–‡æ¡£ä»‹ç»AIå®‰å…¨çš„åŸºç¡€æ¦‚å¿µã€å¨èƒæ¨¡å‹å’Œé˜²æŠ¤ç­–ç•¥ã€‚

## ğŸ”’ AIå®‰å…¨å¨èƒæ¨¡å‹

### å¨èƒåˆ†ç±»
```python
from enum import Enum
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
import time
import hashlib
import logging

class ThreatType(Enum):
    """å¨èƒç±»å‹"""
    ADVERSARIAL_ATTACK = "adversarial_attack"      # å¯¹æŠ—æ”»å‡»
    DATA_POISONING = "data_poisoning"              # æ•°æ®æŠ•æ¯’
    MODEL_THEFT = "model_theft"                    # æ¨¡å‹çªƒå–
    PRIVACY_LEAKAGE = "privacy_leakage"              # éšç§æ³„éœ²
    FAIRNESS_BIAS = "fairness_bias"                 # å…¬å¹³æ€§åå·®
    SYSTEM_VULNERABILITY = "system_vulnerability"    # ç³»ç»Ÿæ¼æ´
    INFERENCE_ATTACK = "inference_attack"            # æ¨ç†æ”»å‡»

class SeverityLevel(Enum):
    """ä¸¥é‡ç¨‹åº¦"""
    CRITICAL = "critical"     # ä¸¥é‡
    HIGH = "high"             # é«˜
    MEDIUM = "medium"         # ä¸­ç­‰
    LOW = "low"               # ä½
    INFO = "info"             # ä¿¡æ¯

@dataclass
class SecurityThreat:
    """å®‰å…¨å¨èƒå¯¹è±¡"""
    threat_type: ThreatType
    description: str
    severity: SeverityLevel
    likelihood: float = 0.5  # å‘ç”Ÿæ¦‚ç‡
    impact: float = 0.5      # å½±å“ç¨‹åº¦
    mitigation: List[str] = field(default_factory=list)
    detection_methods: List[str] = field(default_factory=list)
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class SecurityIncident:
    """å®‰å…¨äº‹ä»¶å¯¹è±¡"""
    incident_id: str = field(default_factory=lambda: str(int(time.time() * 1000)))
    threat_type: ThreatType
    description: str
    severity: SeverityLevel
    affected_assets: List[str] = field(default_factory=list)
    detected_at: float = field(default_factory=time.time)
    resolved_at: Optional[float] = None
    resolution: Optional[str] = None
    impact_assessment: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

class AISecurityModel:
    """AIå®‰å…¨å¨èƒæ¨¡å‹"""

    def __init__(self):
        self.threats = []
        self.incidents = []
        self.vulnerabilities = {}
        self.logger = logging.getLogger("AISecurityModel")

    def register_threat(self, threat: SecurityThreat):
        """æ³¨å†Œå®‰å…¨å¨èƒ"""
        self.threats.append(threat)
        self.logger.info(f"æ³¨å†Œå¨èƒ: {threat.threat_type.value} - {threat.description}")

    def detect_incident(self, incident: SecurityIncident):
        """æ£€æµ‹å®‰å…¨äº‹ä»¶"""
        self.incidents.append(incident)
        self.logger.warning(f"æ£€æµ‹åˆ°å®‰å…¨äº‹ä»¶: {incident.incident_id}")

    def assess_risk(self, threat: SecurityThreat) -> float:
        """è¯„ä¼°é£é™©"""
        risk_score = threat.likelihood * threat.impact
        return risk_score

    def get_threats_by_severity(self, severity: SeverityLevel) -> List[SecurityThreat]:
        """æŒ‰ä¸¥é‡ç¨‹åº¦è·å–å¨èƒ"""
        return [t for t in self.threats if t.severity == severity]

    def get_security_report(self) -> Dict[str, Any]:
        """è·å–å®‰å…¨æŠ¥å‘Š"""
        severity_counts = {}
        type_counts = {}

        for threat in self.threats:
            severity_counts[threat.severity.value] = severity_counts.get(threat.severity.value, 0) + 1
            type_counts[threat.threat_type.value] = type_counts.get(threat.threat_type.value, 0) + 1

        unresolved_incidents = [i for i in self.incidents if i.resolved_at is None]

        return {
            'total_threats': len(self.threats),
            'total_incidents': len(self.incidents),
            'unresolved_incidents': len(unresolved_incidents),
            'severity_distribution': severity_counts,
            'threat_type_distribution': type_counts,
            'incident_rate': len(unresolved_incidents) / max(len(self.incidents), 1)
        }

# é¢„å®šä¹‰å¨èƒ
def get_default_threats() -> List[SecurityThreat]:
    """è·å–é»˜è®¤å¨èƒåˆ—è¡¨"""
    return [
        SecurityThreat(
            threat_type=ThreatType.ADVERSARIAL_ATTACK,
            description="é€šè¿‡æ·»åŠ å¾®å°æ‰°åŠ¨æ¬ºéª—AIæ¨¡å‹",
            severity=SeverityLevel.HIGH,
            likelihood=0.7,
            impact=0.8,
            mitigation=["å¯¹æŠ—è®­ç»ƒ", "é˜²å¾¡æ€§è’¸é¦", "è¾“å…¥éªŒè¯"],
            detection_methods=["å¼‚å¸¸æ£€æµ‹", "æ¢¯åº¦åˆ†æ"]
        ),
        SecurityThreat(
            threat_type=ThreatType.DATA_POISONING,
            description="åœ¨è®­ç»ƒæ•°æ®ä¸­æ³¨å…¥æ¶æ„æ ·æœ¬",
            severity=SeverityLevel.CRITICAL,
            likelihood=0.4,
            impact=0.9,
            mitigation=["æ•°æ®æ¸…æ´—", "å¼‚å¸¸æ£€æµ‹", "æ¥æºéªŒè¯"],
            detection_methods=["æ•°æ®è´¨é‡æ£€æŸ¥", "ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹"]
        ),
        SecurityThreat(
            threat_type=ThreatType.MODEL_THEFT,
            description="é€šè¿‡æŸ¥è¯¢æ”»å‡»çªƒå–æ¨¡å‹å‚æ•°",
            severity=SeverityLevel.MEDIUM,
            likelihood=0.6,
            impact=0.7,
            mitigation=["APIè®¿é—®æ§åˆ¶", "æŸ¥è¯¢é™åˆ¶", "å·®åˆ†éšç§"],
            detection_methods=["æŸ¥è¯¢æ¨¡å¼åˆ†æ", "å¼‚å¸¸æŸ¥è¯¢æ£€æµ‹"]
        ),
        SecurityThreat(
            threat_type=ThreatType.PRIVACY_LEAKAGE,
            description="é€šè¿‡æ¨¡å‹è¾“å‡ºæ³„éœ²è®­ç»ƒæ•°æ®ä¿¡æ¯",
            severity=SeverityLevel.HIGH,
            likelihood=0.5,
            impact=0.8,
            mitigation=["å·®åˆ†éšç§", "æˆå‘˜æ¨ç†é˜²å¾¡", "è¾“å‡ºè¿‡æ»¤"],
            detection_methods=["æˆå‘˜æ¨ç†æµ‹è¯•", "éšç§å®¡è®¡"]
        ),
        SecurityThreat(
            threat_type=ThreatType.FAIRNESS_BIAS,
            description="æ¨¡å‹å¯¹ç‰¹å®šç¾¤ä½“å­˜åœ¨åè§",
            severity=SeverityLevel.MEDIUM,
            likelihood=0.8,
            impact=0.6,
            mitigation=["å…¬å¹³æ€§çº¦æŸ", "æ•°æ®å¹³è¡¡", "åè§æ£€æµ‹"],
            detection_methods=["å…¬å¹³æ€§æŒ‡æ ‡æµ‹è¯•", "åå·®åˆ†æ"]
        )
    ]

# ä½¿ç”¨ç¤ºä¾‹
def security_model_demo():
    """å®‰å…¨å¨èƒæ¨¡å‹æ¼”ç¤º"""
    security_model = AISecurityModel()

    # æ³¨å†Œé»˜è®¤å¨èƒ
    for threat in get_default_threats():
        security_model.register_threat(threat)

    # æ¨¡æ‹Ÿå®‰å…¨äº‹ä»¶
    incident = SecurityIncident(
        threat_type=ThreatType.ADVERSARIAL_ATTACK,
        description="æ£€æµ‹åˆ°é’ˆå¯¹å›¾åƒåˆ†ç±»å™¨çš„å¯¹æŠ—æ ·æœ¬",
        severity=SeverityLevel.HIGH,
        affected_assets=["image_classifier_v1"]
    )
    security_model.detect_incident(incident)

    # è·å–å®‰å…¨æŠ¥å‘Š
    report = security_model.get_security_report()

    print("ğŸ”’ AIå®‰å…¨å¨èƒæ¨¡å‹æ¼”ç¤º")
    print("=" * 50)
    print(f"æ³¨å†Œå¨èƒæ•°: {report['total_threats']}")
    print(f"å®‰å…¨äº‹ä»¶æ•°: {report['total_incidents']}")
    print(f"æœªè§£å†³äº‹ä»¶æ•°: {report['unresolved_incidents']}")
    print(f"å®‰å…¨é£é™©è¯„åˆ†: {report['incident_rate']:.2f}")

    print("\nå¨èƒç±»å‹åˆ†å¸ƒ:")
    for threat_type, count in report['threat_type_distribution'].items():
        print(f"  {threat_type}: {count}")

    print("\nä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
    for severity, count in report['severity_distribution'].items():
        print(f"  {severity}: {count}")

# security_model_demo()
```

## ğŸ›¡ï¸ å¯¹æŠ—æ”»å‡»é˜²æŠ¤

### å¯¹æŠ—æ ·æœ¬æ£€æµ‹ä¸é˜²å¾¡
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import List, Tuple, Optional
import cv2

class AdversarialDetector:
    """å¯¹æŠ—æ ·æœ¬æ£€æµ‹å™¨"""

    def __init__(self, model: nn.Module, threshold: float = 0.5):
        self.model = model
        self.threshold = threshold
        self.model.eval()
        self.detection_methods = {
            'gradient': self._gradient_based_detection,
            'noise': self._noise_robustness_detection,
            'ensemble': self._ensemble_detection,
            'statistical': self._statistical_detection
        }

    def detect_adversarial(self, x: torch.Tensor,
                          method: str = 'ensemble') -> Tuple[bool, float]:
        """æ£€æµ‹å¯¹æŠ—æ ·æœ¬"""
        if method not in self.detection_methods:
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€æµ‹æ–¹æ³•: {method}")

        detection_fn = self.detection_methods[method]
        is_adversarial, confidence = detection_fn(x)

        return is_adversarial, confidence

    def _gradient_based_detection(self, x: torch.Tensor) -> Tuple[bool, float]:
        """åŸºäºæ¢¯åº¦çš„æ£€æµ‹"""
        x.requires_grad_(True)

        # å‰å‘ä¼ æ’­
        output = self.model(x)
        pred_class = output.argmax(dim=1)

        # è®¡ç®—æ¢¯åº¦
        loss = F.cross_entropy(output, pred_class)
        loss.backward()

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        gradient_norm = x.grad.norm(p=2).item()

        # å¦‚æœæ¢¯åº¦èŒƒæ•°å¼‚å¸¸å¤§ï¼Œå¯èƒ½æ˜¯å¯¹æŠ—æ ·æœ¬
        is_adversarial = gradient_norm > self.threshold
        confidence = min(gradient_norm / 10.0, 1.0)

        return is_adversarial, confidence

    def _noise_robustness_detection(self, x: torch.Tensor) -> Tuple[bool, float]:
        """åŸºäºå™ªå£°é²æ£’æ€§çš„æ£€æµ‹"""
        # æ·»åŠ å°å™ªå£°
        noise = torch.randn_like(x) * 0.01
        x_noisy = x + noise

        # æ¯”è¾ƒåŸå§‹å’Œå™ªå£°æ ·æœ¬çš„é¢„æµ‹
        with torch.no_grad():
            output_original = self.model(x)
            output_noisy = self.model(x_noisy)

        pred_original = output_original.argmax(dim=1)
        pred_noisy = output_noisy.argmax(dim=1)

        # é¢„æµ‹ä¸ä¸€è‡´å¯èƒ½æ˜¯å¯¹æŠ—æ ·æœ¬
        prediction_change = (pred_original != pred_noisy).float().mean().item()
        is_adversarial = prediction_change > self.threshold
        confidence = prediction_change

        return is_adversarial, confidence

    def _ensemble_detection(self, x: torch.Tensor) -> Tuple[bool, float]:
        """é›†æˆæ£€æµ‹"""
        # å¯¹æ‰€æœ‰æ£€æµ‹æ–¹æ³•è¿›è¡Œé›†æˆ
        results = []

        for method in ['gradient', 'noise', 'statistical']:
            is_adv, conf = self.detection_methods[method](x)
            results.append((is_adv, conf))

        # æŠ•ç¥¨æœºåˆ¶
        adv_count = sum(1 for is_adv, _ in results if is_adv)
        avg_confidence = sum(conf for _, conf in results) / len(results)

        is_adversarial = adv_count >= 2  # å¤šæ•°æŠ•ç¥¨
        confidence = avg_confidence

        return is_adversarial, confidence

    def _statistical_detection(self, x: torch.Tensor) -> Tuple[bool, float]:
        """åŸºäºç»Ÿè®¡ç‰¹å¾çš„æ£€æµ‹"""
        # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
        mean = x.mean().item()
        std = x.std().item()
        skewness = self._calculate_skewness(x)
        kurtosis = self._calculate_kurtosis(x)

        # æ£€æµ‹å¼‚å¸¸ç»Ÿè®¡ç‰¹å¾
        anomaly_score = 0.0

        if abs(skewness) > 2.0:
            anomaly_score += 0.3
        if abs(kurtosis) > 2.0:
            anomaly_score += 0.3
        if std > 1.0:
            anomaly_score += 0.4

        is_adversarial = anomaly_score > self.threshold
        confidence = anomaly_score

        return is_adversarial, confidence

    def _calculate_skewness(self, x: torch.Tensor) -> float:
        """è®¡ç®—ååº¦"""
        x_flat = x.flatten()
        mean_val = torch.mean(x_flat)
        std_val = torch.std(x_flat)
        skewness = torch.mean(((x_flat - mean_val) / std_val) ** 3).item()
        return skewness

    def _calculate_kurtosis(self, x: torch.Tensor) -> float:
        """è®¡ç®—å³°åº¦"""
        x_flat = x.flatten()
        mean_val = torch.mean(x_flat)
        std_val = torch.std(x_flat)
        kurtosis = torch.mean(((x_flat - mean_val) / std_val) ** 4).item() - 3
        return kurtosis

class AdversarialGenerator:
    """å¯¹æŠ—æ ·æœ¬ç”Ÿæˆå™¨"""

    def __init__(self, model: nn.Module):
        self.model = model
        self.model.eval()

    def fgsm_attack(self, x: torch.Tensor, y: torch.Tensor,
                   epsilon: float = 0.1, targeted: bool = False) -> torch.Tensor:
        """FGSMæ”»å‡»"""
        x.requires_grad_(True)

        # å‰å‘ä¼ æ’­
        output = self.model(x)

        if targeted:
            # ç›®æ ‡æ”»å‡»
            loss = -F.cross_entropy(output, y)
        else:
            # éç›®æ ‡æ”»å‡»
            loss = F.cross_entropy(output, y)

        # è®¡ç®—æ¢¯åº¦
        loss.backward()
        sign_grad = x.grad.sign()

        # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
        x_adv = x + epsilon * sign_grad
        x_adv = torch.clamp(x_adv, 0, 1)

        return x_adv.detach()

    def pgd_attack(self, x: torch.Tensor, y: torch.Tensor,
                  epsilon: float = 0.1, alpha: float = 0.01, num_iter: int = 10) -> torch.Tensor:
        """PGDæ”»å‡»"""
        x_adv = x.clone().detach()

        for _ in range(num_iter):
            x_adv.requires_grad_(True)

            # FGSMæ­¥éª¤
            output = self.model(x_adv)
            loss = F.cross_entropy(output, y)
            loss.backward()

            # æ›´æ–°
            sign_grad = x_adv.grad.sign()
            x_adv = x_adv + alpha * sign_grad

            # æŠ•å½±åˆ°epsilon-ball
            x_adv = torch.max(torch.min(x_adv, x + epsilon), x - epsilon)
            x_adv = torch.clamp(x_adv, 0, 1)
            x_adv = x_adv.detach()

        return x_adv

    def deepfool_attack(self, x: torch.Tensor, y: torch.Tensor,
                        max_iter: int = 50, overshoot: float = 0.02, max_pert: float = 0.1) -> torch.Tensor:
        """DeepFoolæ”»å‡»"""
        x_adv = x.clone().detach()
        output = self.model(x_adv)

        if output.dim() > 1:
            y_pred = output.argmax(dim=1)
        else:
            y_pred = (output > 0).long()

        # å¦‚æœå·²ç»é”™è¯¯åˆ†ç±»ï¼Œç›´æ¥è¿”å›
        if y_pred != y:
            return x_adv

        # è®¡ç®—åˆå§‹æ¢¯åº¦
        output_2d = output if output.dim() == 2 else output.unsqueeze(0)
        loss = torch.min(output_2d[range(output_2d.size(0)), y] -
                    torch.cat([output_2d[range(output_2d.size(0)), :y],
                             output_2d[range(output_2d.size(0)), y+1:]], dim=1), dim=1)
        grad = torch.autograd.grad(loss.mean(), x_adv, create_graph=True)[0]

        w = torch.sign(grad)
        x_adv = x + overshoot * w
        x_adv = torch.clamp(x_adv, 0, 1)

        return x_adv

class DefensiveDistillation:
    """é˜²å¾¡æ€§è’¸é¦"""

    def __init__(self, teacher_model: nn.Module, student_model: nn.Module,
                 temperature: float = 4.0):
        self.teacher_model = teacher_model
        self.student_model = student_model
        self.temperature = temperature

    def train_student(self, train_loader, num_epochs: int = 10,
                      learning_rate: float = 0.001):
        """è®­ç»ƒå­¦ç”Ÿæ¨¡å‹"""
        teacher_model = self.teacher_model.eval()
        student_model = self.student_model.train()

        optimizer = torch.optim.Adam(student_model.parameters(), lr=learning_rate)

        for epoch in range(num_epochs):
            total_loss = 0.0

            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()

                # è·å–æ•™å¸ˆæ¨¡å‹è½¯æ ‡ç­¾
                with torch.no_grad():
                    teacher_output = teacher_model(data)
                    soft_targets = F.softmax(teacher_output / self.temperature, dim=1)

                # å­¦ç”Ÿæ¨¡å‹è¾“å‡º
                student_output = student_model(data)

                # è’¸é¦æŸå¤±
                student_soft = F.log_softmax(student_output / self.temperature, dim=1)
                distillation_loss = F.kl_div(student_soft, soft_targets, reduction='batchmean')

                # æ ‡å‡†äº¤å‰ç†µæŸå¤±
                ce_loss = F.cross_entropy(student_output, target)

                # ç»„åˆæŸå¤±
                loss = 0.7 * distillation_loss + 0.3 * ce_loss
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / len(train_loader)
            print(f'Epoch {epoch+1}: Loss = {avg_loss:.4f}')

# ä½¿ç”¨ç¤ºä¾‹
def adversarial_defense_demo():
    """å¯¹æŠ—æ”»é˜²æ¼”ç¤º"""

    # æ¨¡æ‹Ÿæ¨¡å‹
    class SimpleClassifier(nn.Module):
        def __init__(self, input_size=784, num_classes=10):
            super().__init__()
            self.fc1 = nn.Linear(input_size, 256)
            self.fc2 = nn.Linear(256, 128)
            self.fc3 = nn.Linear(128, num_classes)

        def forward(self, x):
            x = x.view(x.size(0), -1)
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            x = self.fc3(x)
            return x

    # åˆ›å»ºæ¨¡å‹
    teacher_model = SimpleClassifier()
    student_model = SimpleClassifier()
    detector = AdversarialDetector(teacher_model, threshold=0.3)
    generator = AdversarialGenerator(teacher_model)

    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 4
    x = torch.randn(batch_size, 1, 28, 28)
    y = torch.randint(0, 10, (batch_size,))

    print("ğŸ›¡ï¸ å¯¹æŠ—æ”»é˜²æ¼”ç¤º")
    print("=" * 40)

    # åŸå§‹é¢„æµ‹
    with torch.no_grad():
        original_output = teacher_model(x.view(x.size(0), -1))
        original_pred = original_output.argmax(dim=1)

    print(f"åŸå§‹é¢„æµ‹: {original_pred.tolist()}")

    # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
    x_adv_fgsm = generator.fgsm_attack(x.view(x.size(0), -1), y, epsilon=0.1)
    x_adv_pgd = generator.pgd_attack(x.view(x.size(0), -1), y, epsilon=0.05, alpha=0.01)

    # å¯¹æŠ—æ ·æœ¬é¢„æµ‹
    with torch.no_grad():
        adv_fgsm_output = teacher_model(x_adv_fgsm)
        adv_pgd_output = teacher_model(x_adv_pgd)

        adv_fgsm_pred = adv_fgsm_output.argmax(dim=1)
        adv_pgd_pred = adv_pgd_output.argmax(dim=1)

    print(f"FGSMé¢„æµ‹: {adv_fgsm_pred.tolist()}")
    print(f"PGDé¢„æµ‹: {adv_pgd_pred.tolist()}")

    # æ£€æµ‹å¯¹æŠ—æ ·æœ¬
    is_adv_fgsm, conf_fgsm = detector.detect_adversarial(x_adv_fgsm, method='ensemble')
    is_adv_pgd, conf_pgd = detector.detect_adversarial(x_adv_pgd, method='ensemble')

    print(f"FGSMæ£€æµ‹: {is_adv_fgsm} (ç½®ä¿¡åº¦: {conf_fgsm:.3f})")
    print(f"PGDæ£€æµ‹: {is_adv_pgd} (ç½®ä¿¡åº¦: {conf_pgd:.3f})")

    # é˜²å¾¡æ€§è’¸é¦
    print("\nğŸ”§ é˜²å¾¡æ€§è’¸é¦è®­ç»ƒ...")
    print("(åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦å®Œæ•´çš„è®­ç»ƒæ•°æ®)")

# adversarial_defense_demo()
```

## ğŸ”’ æ•°æ®éšç§ä¿æŠ¤

### éšç§ä¿æŠ¤æŠ€æœ¯
```python
import torch
import torch.nn as nn
import numpy as np
from typing import Tuple, Optional
import hashlib
import json

class DifferentialPrivacy:
    """å·®åˆ†éšç§å®ç°"""

    def __init__(self, epsilon: float = 1.0, delta: float = 1e-5):
        self.epsilon = epsilon
        self.delta = delta
        self.sensitivity = 1.0

    def laplace_mechanism(self, true_value: float, sensitivity: float = None) -> float:
        """æ‹‰æ™®æ‹‰æ–¯æœºåˆ¶"""
        if sensitivity is None:
            sensitivity = self.sensitivity

        scale = sensitivity / self.epsilon

        # ç”Ÿæˆæ‹‰æ™®æ‹‰æ–¯å™ªå£°
        noise = np.random.laplace(0, scale)
        noisy_value = true_value + noise

        return noisy_value

    def gaussian_mechanism(self, true_value: float, sensitivity: float = None) -> float:
        """é«˜æ–¯æœºåˆ¶"""
        if sensitivity is None:
            sensitivity = self.sensitivity

        # è®¡ç®—æ ‡å‡†å·®
        sigma = np.sqrt(2 * np.log(1.25 / self.delta)) * sensitivity / self.epsilon

        # ç”Ÿæˆé«˜æ–¯å™ªå£°
        noise = np.random.normal(0, sigma)
        noisy_value = true_value + noise

        return noisy_value

    def clip_and_noise_gradients(self, gradients: torch.Tensor,
                                clip_norm: float = 1.0) -> torch.Tensor:
        """æ¢¯åº¦è£å‰ªå’ŒåŠ å™ª"""
        # è£å‰ªæ¢¯åº¦èŒƒæ•°
        total_norm = torch.norm(gradients, p=2)
        clip_coef = min(1.0, clip_norm / (total_norm + 1e-6))
        clipped_gradients = gradients * clip_coef

        # æ·»åŠ å™ªå£°
        noise = torch.randn_like(clipped_gradients) * (clip_norm / self.epsilon)
        noisy_gradients = clipped_gradients + noise

        return noisy_gradients

class SecureAggregation:
    """å®‰å…¨èšåˆ"""

    def __init__(self, num_clients: int, privacy_budget: float = 1.0):
        self.num_clients = num_clients
        self.privacy_budget = privacy_budget
        self.dp = DifferentialPrivacy(epsilon=privacy_budget)

    def secure_mean(self, client_values: list) -> float:
        """å®‰å…¨çš„å‡å€¼è®¡ç®—"""
        if len(client_values) != self.num_clients:
            raise ValueError("å®¢æˆ·ç«¯æ•°é‡ä¸åŒ¹é…")

        # å¯¹æ¯ä¸ªå®¢æˆ·ç«¯çš„å€¼æ·»åŠ å™ªå£°
        noisy_values = []
        for value in client_values:
            noisy_value = self.dp.laplace_mechanism(float(value))
            noisy_values.append(noisy_value)

        # è®¡ç®—å‡å€¼
        secure_mean = sum(noisy_values) / len(noisy_values)

        return secure_mean

    def secure_sum(self, client_values: list) -> float:
        """å®‰å…¨çš„æ±‚å’Œè®¡ç®—"""
        if len(client_values) != self.num_clients:
            raise ValueError("å®¢æˆ·ç«¯æ•°é‡ä¸åŒ¹é…")

        # å¯¹æ¯ä¸ªå®¢æˆ·ç«¯çš„å€¼æ·»åŠ å™ªå£°
        noisy_values = []
        for value in client_values:
            noisy_value = self.dp.laplace_mechanism(float(value))
            noisy_values.append(noisy_value)

        # è®¡ç®—æ€»å’Œ
        secure_sum = sum(noisy_values)

        return secure_sum

class PrivacyPreservingML:
    """éšç§ä¿æŠ¤æœºå™¨å­¦ä¹ """

    def __init__(self, model: nn.Module, privacy_budget: float = 1.0):
        self.model = model
        self.dp = DifferentialPrivacy(epsilon=privacy_budget)
        self.clip_norm = 1.0

    def train_with_dp(self, train_loader, epochs: int = 10,
                     learning_rate: float = 0.01, clip_norm: float = 1.0):
        """å·®åˆ†éšç§è®­ç»ƒ"""
        self.clip_norm = clip_norm
        optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate)

        self.model.train()

        for epoch in range(epochs):
            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()

                # å‰å‘ä¼ æ’­
                output = self.model(data)
                loss = nn.CrossEntropyLoss()(output, target)

                # åå‘ä¼ æ’­
                loss.backward()

                # å¯¹æ¢¯åº¦è¿›è¡Œå·®åˆ†éšç§å¤„ç†
                for param in self.model.parameters():
                    if param.grad is not None:
                        param.grad = self.dp.clip_and_noise_gradients(
                            param.grad, self.clip_norm
                        )

                optimizer.step()

            print(f'Epoch {epoch+1} completed with privacy budget {self.dp.epsilon}')

    def privacy_perturbed_prediction(self, x: torch.Tensor) -> torch.Tensor:
        """éšç§æ‰°åŠ¨çš„é¢„æµ‹"""
        with torch.no_grad():
            output = self.model(x)

        # å¯¹è¾“å‡ºæ·»åŠ å™ªå£°
        noisy_output = output + torch.randn_like(output) * 0.1

        return noisy_output

class MembershipInferenceDefense:
    """æˆå‘˜æ¨ç†é˜²å¾¡"""

    def __init__(self, model: nn.Module, threshold: float = 0.5):
        self.model = model
        self.threshold = threshold

    def add_dropout_training(self, train_loader, epochs: int = 10,
                           dropout_rate: float = 0.5):
        """æ·»åŠ Dropoutè®­ç»ƒ"""
        # åœ¨æ‰€æœ‰çº¿æ€§å±‚åæ·»åŠ Dropout
        for name, module in self.model.named_children():
            if isinstance(module, nn.Linear):
                dropout_layer = nn.Dropout(dropout_rate)
                setattr(self.model, f"{name}_dropout", dropout_layer)

        # ä¿®æ”¹å‰å‘ä¼ æ’­
        original_forward = self.model.forward
        def dp_forward(x):
            x = original_forward(x)
            return x
        self.model.forward = dp_forward

    def regularize_predictions(self, predictions: torch.Tensor,
                            temperature: float = 2.0) -> torch.Tensor:
        """æ­£åˆ™åŒ–é¢„æµ‹"""
        # æ¸©åº¦ç¼©æ”¾
        scaled_preds = predictions / temperature
        smoothed_preds = F.softmax(scaled_preds, dim=-1)

        return smoothed_preds

class HomomorphicEncryption:
    """åŒæ€åŠ å¯†ï¼ˆç®€åŒ–å®ç°ï¼‰"""

    def __init__(self, key_size: int = 2048):
        self.key_size = key_size
        # ç®€åŒ–ï¼šå®é™…åº”ä½¿ç”¨ä¸“ä¸šçš„åŒæ€åŠ å¯†åº“
        self.public_key = f"pub_{key_size}"
        self.private_key = f"priv_{key_size}"

    def encrypt(self, data: float) -> str:
        """åŠ å¯†æ•°æ®"""
        # ç®€åŒ–ï¼šå®é™…åº”ä½¿ç”¨åŒæ€åŠ å¯†ç®—æ³•
        encrypted = f"enc_{self.public_key}_{abs(hash(str(data))) % 10000}"
        return encrypted

    def decrypt(self, encrypted_data: str) -> float:
        """è§£å¯†æ•°æ®"""
        # ç®€åŒ–ï¼šå®é™…åº”ä½¿ç”¨åŒæ€è§£å¯†ç®—æ³•
        if encrypted.startswith(f"enc_{self.public_key}_"):
            value = float(encrypted.split('_')[-1])
            return value
        return 0.0

    def encrypt_tensor(self, tensor: torch.Tensor) -> list:
        """åŠ å¯†å¼ é‡"""
        encrypted_list = []
        for value in tensor.flatten():
            encrypted_list.append(self.encrypt(float(value)))
        return encrypted_list

    def decrypt_tensor(self, encrypted_list: list, shape: tuple) -> torch.Tensor:
        """è§£å¯†å¼ é‡"""
        decrypted_values = [self.decrypt(val) for val in encrypted_list]
        return torch.tensor(decrypted_values).reshape(shape)

class FederatedLearning:
    """è”é‚¦å­¦ä¹ éšç§ä¿æŠ¤"""

    def __init__(self, model_class, num_clients: int = 10):
        self.model_class = model_class
        self.num_clients = num_clients
        self.client_models = []
        self.global_model = model_class()

        for _ in range(num_clients):
            self.client_models.append(model_class())

    def client_train_step(self, client_id: int, local_data: torch.utils.data.DataLoader,
                         learning_rate: float = 0.01):
        """å®¢æˆ·ç«¯è®­ç»ƒæ­¥éª¤"""
        model = self.client_models[client_id]
        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

        model.train()
        for data, target in local_data:
            optimizer.zero_grad()
            output = model(data)
            loss = nn.CrossEntropyLoss()(output, target)
            loss.backward()
            optimizer.step()

        # è¿”å›æ¨¡å‹å‚æ•°ï¼ˆå®é™…åº”è¿›è¡Œå®‰å…¨èšåˆï¼‰
        return {name: param.data.clone() for name, param in model.named_parameters()}

    def secure_aggregate(self, client_updates: list) -> dict:
        """å®‰å…¨èšåˆå®¢æˆ·ç«¯æ›´æ–°"""
        # åˆå§‹åŒ–èšåˆå‚æ•°
        aggregated_params = {}

        # è·å–ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„å‚æ•°ç»“æ„
        first_client = client_updates[0]

        for param_name in first_client.keys():
            # æ”¶é›†æ‰€æœ‰å®¢æˆ·ç«¯çš„è¯¥å‚æ•°
            client_params = [update[param_name] for update in client_updates]

            # ç®€å•å¹³å‡èšåˆï¼ˆå®é™…åº”ä½¿ç”¨æ›´å®‰å…¨çš„èšåˆæ–¹æ³•ï¼‰
            aggregated_param = torch.stack(client_params).mean(dim=0)

            # æ·»åŠ å™ªå£°ä¿æŠ¤éšç§
            noise = torch.randn_like(aggregated_param) * 0.01
            aggregated_param += noise

            aggregated_params[param_name] = aggregated_param

        return aggregated_params

    def global_update(self, aggregated_params: dict):
        """æ›´æ–°å…¨å±€æ¨¡å‹"""
        with torch.no_grad():
            for name, param in self.global_model.named_parameters():
                if name in aggregated_params:
                    param.copy_(aggregated_params[name])

# ä½¿ç”¨ç¤ºä¾‹
def privacy_protection_demo():
    """éšç§ä¿æŠ¤æ¼”ç¤º"""

    print("ğŸ”’ éšç§ä¿æŠ¤æŠ€æœ¯æ¼”ç¤º")
    print("=" * 40)

    # å·®åˆ†éšç§
    dp = DifferentialPrivacy(epsilon=1.0)
    true_value = 100.0

    noisy_values = [dp.laplace_mechanism(true_value) for _ in range(5)]
    print(f"å·®åˆ†éšç§: çœŸå®å€¼ {true_value}, å™ªå£°å€¼ {noisy_values}")
    print(f"å™ªå£°å‡å€¼: {np.mean(noisy_values):.2f}")

    # å®‰å…¨èšåˆ
    secure_agg = SecureAggregation(num_clients=5, privacy_budget=1.0)
    client_values = [10.0, 12.0, 8.0, 11.0, 9.0]

    secure_mean = secure_agg.secure_mean(client_values)
    actual_mean = sum(client_values) / len(client_values)

    print(f"\nå®‰å…¨èšåˆ: å®é™…å‡å€¼ {actual_mean:.2f}, å®‰å…¨å‡å€¼ {secure_mean:.2f}")

    # åŒæ€åŠ å¯†
    he = HomomorphicEncryption(key_size=2048)
    data = torch.tensor([[1.0, 2.0], [3.0, 4.0]])

    encrypted_data = he.encrypt_tensor(data)
    decrypted_data = he.decrypt_tensor(encrypted_data, data.shape)

    print(f"\nåŒæ€åŠ å¯†: åŸå§‹æ•°æ® {data.tolist()}")
    print(f"åŠ å¯†åæ ·ä¾‹: {encrypted_data[:2]}")
    print(f"è§£å¯†åæ•°æ®: {decrypted_data.tolist()}")

# privacy_protection_demo()
```

## âš–ï¸ å…¬å¹³æ€§ä¸åå·®æ£€æµ‹

### å…¬å¹³æ€§è¯„ä¼°
```python
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
from typing import Dict, List, Any, Optional
import pandas as pd

class FairnessEvaluator:
    """å…¬å¹³æ€§è¯„ä¼°å™¨"""

    def __init__(self, protected_attributes: List[str]):
        self.protected_attributes = protected_attributes
        self.fairness_metrics = {
            'demographic_parity': self.demographic_parity,
            'equal_opportunity': self.equal_opportunity,
            'equalized_odds': self.equalized_odds,
            'predictive_parity': self.predictive_parity,
            'individual_fairness': self.individual_fairness,
            'counterfactual_fairness': self.counterfactual_fairness
        }

    def evaluate_fairness(self, y_true: np.ndarray, y_pred: np.ndarray,
                         sensitive_features: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """è¯„ä¼°å…¬å¹³æ€§"""
        results = {}

        # åŸºç¡€ç»Ÿè®¡
        results['overall_metrics'] = self._calculate_basic_metrics(y_true, y_pred)
        results['group_metrics'] = {}

        # æŒ‰å—ä¿æŠ¤å±æ€§åˆ†ç»„è¯„ä¼°
        for attr_name in self.protected_attributes:
            if attr_name in sensitive_features:
                attr_values = sensitive_features[attr_name]
                results['group_metrics'][attr_name] = self._evaluate_by_group(
                    y_true, y_pred, attr_values, attr_name
                )

        # è®¡ç®—å…¬å¹³æ€§æŒ‡æ ‡
        results['fairness_metrics'] = self._calculate_fairness_metrics(
            y_true, y_pred, sensitive_features
        )

        return results

    def _calculate_basic_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—åŸºç¡€æŒ‡æ ‡"""
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

        return {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),
            'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),
            'f1_score': f1_score(y_true, y_pred, average='weighted', zero_division=0)
        }

    def _evaluate_by_group(self, y_true: np.ndarray, y_pred: np.ndarray,
                           sensitive_values: np.ndarray, attr_name: str) -> Dict[str, Any]:
        """æŒ‰ç»„è¯„ä¼°"""
        unique_groups = np.unique(sensitive_values)
        group_results = {}

        for group in unique_groups:
            group_mask = sensitive_values == group
            group_y_true = y_true[group_mask]
            group_y_pred = y_pred[group_mask]

            if len(group_y_true) > 0:
                group_metrics = self._calculate_basic_metrics(group_y_true, group_y_pred)
                group_metrics['sample_count'] = len(group_y_true)
                group_metrics['selection_rate'] = group_y_pred.mean()

                group_results[str(group)] = group_metrics

        return group_results

    def demographic_parity(self, y_true: np.ndarray, y_pred: np.ndarray,
                          sensitive_values: np.ndarray) -> Dict[str, float]:
        """äººå£ç»Ÿè®¡å‡ç­‰"""
        unique_groups = np.unique(sensitive_values)
        selection_rates = {}

        for group in unique_groups:
            group_mask = sensitive_values == group
            group_y_pred = y_pred[group_mask]
            selection_rates[str(group)] = group_y_pred.mean()

        # è®¡ç®—å·®å¼‚
        max_rate = max(selection_rates.values())
        min_rate = min(selection_rates.values())

        return {
            'selection_rates': selection_rates,
            'max_difference': max_rate - min_rate,
            'disparity_ratio': max_rate / (min_rate + 1e-6)
        }

    def equal_opportunity(self, y_true: np.ndarray, y_pred: np.ndarray,
                         sensitive_values: np.ndarray) -> Dict[str, float]:
        """å‡ç­‰æœºä¼š"""
        unique_groups = np.unique(sensitive_values)
        true_positive_rates = {}

        for group in unique_groups:
            group_mask = sensitive_values == group
            group_y_true = y_true[group_mask]
            group_y_pred = y_pred[group_mask]

            # è®¡ç®—æ­£æ ·æœ¬çš„å¬å›ç‡
            positive_mask = group_y_true == 1
            if positive_mask.sum() > 0:
                tp = ((group_y_pred == 1) & positive_mask).sum()
                fn = ((group_y_pred == 0) & positive_mask).sum()
                tpr = tp / (tp + fn + 1e-6)
                true_positive_rates[str(group)] = tpr
            else:
                true_positive_rates[str(group)] = 0.0

        # è®¡ç®—å·®å¼‚
        max_tpr = max(true_positive_rates.values())
        min_tpr = min(true_positive_rates.values())

        return {
            'true_positive_rates': true_positive_rates,
            'max_difference': max_tpr - min_tpr,
            'disparity_ratio': max_tpr / (min_tpr + 1e-6)
        }

    def equalized_odds(self, y_true: np.ndarray, y_pred: np.ndarray,
                       sensitive_values: np.ndarray) -> Dict[str, float]:
        """å‡ç­‰åŒ–å‡ ç‡"""
        unique_groups = np.unique(sensitive_values)
        odds_ratios = {}

        for group in unique_groups:
            group_mask = sensitive_values == group
            group_y_true = y_true[group_mask]
            group_y_pred = y_pred[group_mask]

            # è®¡ç®—æ­£ç±»å’Œè´Ÿç±»çš„å‡ ç‡
            tp = ((group_y_pred == 1) & (group_y_true == 1)).sum()
            fp = ((group_y_pred == 1) & (group_y_true == 0)).sum()
            fn = ((group_y_pred == 0) & (group_y_true == 1)).sum()
            tn = ((group_y_pred == 0) & (group_y_true == 0)).sum()

            positive_odds = tp / (fn + 1e-6)
            negative_odds = tn / (fp + 1e-6)

            odds_ratios[str(group)] = {
                'positive_odds': positive_odds,
                'negative_odds': negative_odds,
                'odds_ratio': positive_odds / (negative_odds + 1e-6)
            }

        return odds_ratios

    def predictive_parity(self, y_true: np.ndarray, y_pred: np.ndarray,
                          sensitive_values: np.ndarray) -> Dict[str, float]:
        """é¢„æµ‹å‡ç­‰"""
        unique_groups = np.unique(sensitive_values)
        precision_scores = {}

        for group in unique_groups:
            group_mask = sensitive_values == group
            group_y_true = y_true[group_mask]
            group_y_pred = y_pred[group_mask]

            tp = ((group_y_pred == 1) & (group_y_true == 1)).sum()
            fp = ((group_y_pred == 1) & (group_y_true == 0)).sum()

            precision = tp / (tp + fp + 1e-6)
            precision_scores[str(group)] = precision

        # è®¡ç®—å·®å¼‚
        max_precision = max(precision_scores.values())
        min_precision = min(precision_scores.values())

        return {
            'precision_scores': precision_scores,
            'max_difference': max_precision - min_precision,
            'disparity_ratio': max_precision / (min_precision + 1e-6)
        }

    def individual_fairness(self, y_true: np.ndarray, y_pred: np.ndarray,
                           sensitive_values: np.ndarray) -> float:
        """ä¸ªä½“å…¬å¹³æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è®¡ç®—ç›¸ä¼¼ä¸ªä½“ä¹‹é—´çš„é¢„æµ‹ä¸€è‡´æ€§
        n = len(y_true)
        consistency_scores = []

        for i in range(min(n, 1000)):  # é‡‡æ ·ä»¥æé«˜æ•ˆç‡
            # æ‰¾åˆ°ç›¸ä¼¼ä¸ªä½“
            for j in range(i+1, min(n, 1000)):
                # å¦‚æœä¸ªä½“å…·æœ‰ç›¸åŒçš„æ•æ„Ÿå±æ€§å’Œç›¸ä¼¼çš„ç‰¹å¾
                if (sensitive_values[i] == sensitive_values[j] and
                    abs(y_true[i] - y_true[j]) < 0.1):
                    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦ä¸€è‡´
                    consistency = 1 if y_pred[i] == y_pred[j] else 0
                    consistency_scores.append(consistency)

        return np.mean(consistency_scores) if consistency_scores else 0.0

    def counterfactual_fairness(self, y_true: np.ndarray, y_pred: np.ndarray,
                                sensitive_values: np.ndarray) -> float:
        """åäº‹å®å…¬å¹³æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è®¡ç®—æ”¹å˜æ•æ„Ÿå±æ€§åé¢„æµ‹çš„å˜åŒ–
        fairness_scores = []

        for i in range(min(len(y_true), 1000)):
            original_value = sensitive_values[i]

            # å¯»æ‰¾å…·æœ‰ç›¸åŒç‰¹å¾ä½†ä¸åŒæ•æ„Ÿå±æ€§çš„ä¸ªä½“
            for j in range(min(len(y_true), 1000)):
                if (i != j and
                    sensitive_values[j] != original_value and
                    abs(y_true[i] - y_true[j]) < 0.1):

                    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦ç›¸ä¼¼
                    pred_diff = abs(y_pred[i] - y_pred[j])
                    fairness = 1 - min(pred_diff, 1.0)
                    fairness_scores.append(fairness)

        return np.mean(fairness_scores) if fairness_scores else 0.0

    def _calculate_fairness_metrics(self, y_true: np.ndarray, y_pred: np.ndarray,
                                  sensitive_features: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """è®¡ç®—æ‰€æœ‰å…¬å¹³æ€§æŒ‡æ ‡"""
        all_metrics = {}

        for attr_name, attr_values in sensitive_features.items():
            if attr_name in self.protected_attributes:
                attr_metrics = {}

                for metric_name, metric_fn in self.fairness_metrics.items():
                    try:
                        metric_result = metric_fn(y_true, y_pred, attr_values)
                        attr_metrics[metric_name] = metric_result
                    except Exception as e:
                        attr_metrics[metric_name] = {"error": str(e)}

                all_metrics[attr_name] = attr_metrics

        return all_metrics

class BiasDetector:
    """åå·®æ£€æµ‹å™¨"""

    def __init__(self):
        self.statistical_tests = {
            'chi_square': self._chi_square_test,
            't_test': self._t_test,
            'kolmogorov_smirnov': self._kolmogorov_smirnov_test
        }

    def detect_distribution_bias(self, data: np.ndarray,
                              sensitive_groups: np.ndarray) -> Dict[str, Any]:
        """æ£€æµ‹åˆ†å¸ƒåå·®"""
        unique_groups = np.unique(sensitive_groups)
        results = {}

        for group in unique_groups:
            group_data = data[sensitive_groups == group]
            results[f'group_{group}'] = {
                'mean': np.mean(group_data),
                'std': np.std(group_data),
                'count': len(group_data)
            }

        return results

    def detect_feature_bias(self, X: np.ndarray, y: np.ndarray,
                           sensitive_groups: np.ndarray) -> Dict[str, Any]:
        """æ£€æµ‹ç‰¹å¾åå·®"""
        feature_names = [f'feature_{i}' for i in range(X.shape[1])]
        bias_results = {}

        unique_groups = np.unique(sensitive_groups)

        for i, feature_name in enumerate(feature_names):
            feature_values = X[:, i]
            group_stats = {}

            for group in unique_groups:
                group_mask = sensitive_groups == group
                group_values = feature_values[group_mask]

                group_stats[f'group_{group}'] = {
                    'mean': np.mean(group_values),
                    'std': np.std(group_values),
                    'count': len(group_values)
                }

            # è®¡ç®—ç»„é—´å·®å¼‚
            means = [stats['mean'] for stats in group_stats.values()]
            max_diff = max(means) - min(means)

            bias_results[feature_name] = {
                'group_stats': group_stats,
                'max_mean_difference': max_diff,
                'bias_detected': max_diff > 0.1  # é˜ˆå€¼å¯è°ƒ
            }

        return bias_results

    def _chi_square_test(self, data1: np.ndarray, data2: np.ndarray) -> Dict[str, float]:
        """å¡æ–¹æ£€éªŒ"""
        from scipy.stats import chi2_contingency

        # åˆ›å»ºåˆ—è”è¡¨
        contingency_table = np.array([
            [np.sum(data1), len(data1) - np.sum(data1)],
            [np.sum(data2), len(data2) - np.sum(data2)]
        ])

        chi2, p_value, _, _ = chi2_contingency(contingency_table)

        return {'chi2_statistic': chi2, 'p_value': p_value}

    def _t_test(self, data1: np.ndarray, data2: np.ndarray) -> Dict[str, float]:
        """tæ£€éªŒ"""
        from scipy.stats import ttest_ind

        t_statistic, p_value = ttest_ind(data1, data2)

        return {'t_statistic': t_statistic, 'p_value': p_value}

    def _kolmogorov_smirnov_test(self, data1: np.ndarray, data2: np.ndarray) -> Dict[str, float]:
        """Kolmogorov-Smirnovæ£€éªŒ"""
        from scipy.stats import ks_2samp

        ks_statistic, p_value = ks_2samp(data1, data2)

        return {'ks_statistic': ks_statistic, 'p_value': p_value}

class FairnessMitigation:
    """å…¬å¹³æ€§ç¼“è§£å™¨"""

    def __init__(self):
        self.mitigation_methods = {
            'reweighting': self.reweighting,
            'preprocessing': self.preprocessing,
            'postprocessing': self.postprocessing,
            'adversarial_debiasing': self.adversarial_debiasing
        }

    def reweighting(self, X: np.ndarray, y: np.ndarray,
                    sensitive_groups: np.ndarray) -> np.ndarray:
        """é‡åŠ æƒç¼“è§£"""
        # è®¡ç®—å„ç»„çš„æƒé‡
        unique_groups = np.unique(sensitive_groups)
        total_count = len(sensitive_groups)

        weights = np.zeros_like(sensitive_groups, dtype=float)

        for group in unique_groups:
            group_mask = sensitive_groups == group
            group_count = group_mask.sum()

            # é€†æ¯”ä¾‹æƒé‡
            weight = total_count / (len(unique_groups) * group_count)
            weights[group_mask] = weight

        return weights

    def preprocessing(self, X: np.ndarray, sensitive_groups: np.ndarray) -> np.ndarray:
        """é¢„å¤„ç†ç¼“è§£"""
        # ç§»é™¤æ•æ„Ÿå±æ€§çš„å½±å“ï¼ˆç®€åŒ–å®ç°ï¼‰
        from sklearn.preprocessing import StandardScaler

        scaler = StandardScaler()
        X_preprocessed = scaler.fit_transform(X)

        return X_preprocessed

    def postprocessing(self, y_pred: np.ndarray, sensitive_groups: np.ndarray,
                        target_rates: Optional[Dict[int, float]] = None) -> np.ndarray:
        """åå¤„ç†ç¼“è§£"""
        if target_rates is None:
            # ä½¿ç”¨ç¾¤ä½“åŸºçº¿
            unique_groups = np.unique(sensitive_groups)
            target_rates = {}

            overall_rate = np.mean(y_pred)
            for group in unique_groups:
                target_rates[group] = overall_rate

        adjusted_predictions = y_pred.copy()

        for group, target_rate in target_rates.items():
            group_mask = sensitive_groups == group
            current_rate = np.mean(y_pred[group_mask])

            # è°ƒæ•´é¢„æµ‹ä»¥è¾¾åˆ°ç›®æ ‡ç‡
            if current_rate > target_rate:
                # é™ä½é¢„æµ‹æ¦‚ç‡
                adjustment_factor = target_rate / current_rate
                adjusted_predictions[group_mask] *= adjustment_factor

        return adjusted_predictions

    def adversarial_debiasing(self, model, X: np.ndarray, y: np.ndarray,
                               sensitive_groups: np.ndarray, epochs: int = 10):
        """å¯¹æŠ—å»å"""
        # ç®€åŒ–çš„å¯¹æŠ—å»åå®ç°
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

        model.train()
        for epoch in range(epochs):
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            outputs = model(torch.FloatTensor(X))
            main_loss = nn.CrossEntropyLoss()(outputs, torch.LongTensor(y))

            # å¯¹æŠ—æŸå¤±ï¼ˆé¼“åŠ±å…¬å¹³æ€§ï¼‰
            fairness_loss = self._calculate_fairness_loss(outputs, sensitive_groups)

            total_loss = main_loss + 0.1 * fairness_loss
            total_loss.backward()
            optimizer.step()

        return model

    def _calculate_fairness_loss(self, outputs: torch.Tensor,
                                  sensitive_groups: np.ndarray) -> torch.Tensor:
        """è®¡ç®—å…¬å¹³æ€§æŸå¤±"""
        # è®¡ç®—ä¸åŒç»„çš„é¢„æµ‹å·®å¼‚
        unique_groups = np.unique(sensitive_groups)
        group_rates = []

        for group in unique_groups:
            group_mask = sensitive_groups == group
            group_outputs = outputs[group_mask]
            group_rate = torch.sigmoid(group_outputs).mean()
            group_rates.append(group_rate)

        # æœ€å°åŒ–ç»„é—´å·®å¼‚
        group_rates = torch.stack(group_rates)
        fairness_loss = torch.var(group_rates)

        return fairness_loss

# ä½¿ç”¨ç¤ºä¾‹
def fairness_demo():
    """å…¬å¹³æ€§è¯„ä¼°æ¼”ç¤º"""

    print("âš–ï¸ å…¬å¹³æ€§è¯„ä¼°æ¼”ç¤º")
    print("=" * 40)

    # æ¨¡æ‹Ÿæ•°æ®
    n_samples = 1000
    X = np.random.randn(n_samples, 10)
    y = np.random.randint(0, 2, n_samples)
    sensitive_groups = np.random.randint(0, 2, n_samples)  # ä¸¤ä¸ªå—ä¿æŠ¤ç»„

    # åˆ›å»ºå…¬å¹³æ€§è¯„ä¼°å™¨
    evaluator = FairnessEvaluator(protected_attributes=['group'])

    # è¯„ä¼°å…¬å¹³æ€§
    fairness_results = evaluator.evaluate_fairness(y, y, {'group': sensitive_groups})

    print("æ•´ä½“æŒ‡æ ‡:")
    for metric, value in fairness_results['overall_metrics'].items():
        print(f"  {metric}: {value:.4f}")

    print("\nç»„åˆ«æŒ‡æ ‡:")
    for group, metrics in fairness_results['group_metrics']['group'].items():
        print(f"  ç»„ {group}:")
        for metric, value in metrics.items():
            print(f"    {metric}: {value}")

    print("\nå…¬å¹³æ€§æŒ‡æ ‡:")
    fairness_metrics = fairness_results['fairness_metrics']['group']
    for metric_name, metric_data in fairness_metrics.items():
        print(f"  {metric_name}:")
        if isinstance(metric_data, dict) and 'max_difference' in metric_data:
            print(f"    æœ€å¤§å·®å¼‚: {metric_data['max_difference']:.4f}")

# fairness_demo()
```

## ğŸ“ æ€»ç»“

AIå®‰å…¨æ˜¯äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæœ¬æ–‡æ¡£ä»‹ç»äº†AIå®‰å…¨çš„å¨èƒæ¨¡å‹ã€é˜²æŠ¤ç­–ç•¥å’Œè¯„ä¼°æ–¹æ³•ã€‚

### ğŸ¯ å…³é”®è¦ç‚¹
- **å¨èƒæ¨¡å‹**: å¯¹æŠ—æ”»å‡»ã€æ•°æ®æŠ•æ¯’ã€éšç§æ³„éœ²ç­‰å¨èƒ
- **é˜²æŠ¤ç­–ç•¥**: å¯¹æŠ—æ ·æœ¬æ£€æµ‹ã€å·®åˆ†éšç§ã€å®‰å…¨èšåˆ
- **å…¬å¹³æ€§ä¿éšœ**: å¤šç§å…¬å¹³æ€§æŒ‡æ ‡å’Œåå·®æ£€æµ‹æ–¹æ³•
- **éšç§ä¿æŠ¤**: è”é‚¦å­¦ä¹ ã€åŒæ€åŠ å¯†ç­‰éšç§æŠ€æœ¯

### ğŸš€ å®ç°ç‰¹è‰²
- **ç»¼åˆé˜²æŠ¤**: ä»æ£€æµ‹åˆ°é˜²å¾¡çš„å®Œæ•´å®‰å…¨ä½“ç³»
- **éšç§ä¿æŠ¤**: å¤šç§éšç§ä¿æŠ¤æŠ€æœ¯çš„å®ç°
- **å…¬å¹³æ€§**: å…¨é¢çš„å…¬å¹³æ€§è¯„ä¼°å’Œç¼“è§£æ–¹æ³•
- **å®ç”¨æ€§**: å¯ç›´æ¥åº”ç”¨äºå®é™…é¡¹ç›®çš„å®‰å…¨ç­–ç•¥

### ğŸ”„ ä¸‹ä¸€æ­¥
- å­¦ä¹ [éšç§ä¿æŠ¤](02-éšç§ä¿æŠ¤.md)
- äº†è§£[æ¨¡å‹å®‰å…¨](03-æ¨¡å‹å®‰å…¨.md)
- æŒæ¡[ä¼¦ç†å‡†åˆ™](04-ä¼¦ç†å‡†åˆ™.md)
- æ¢ç´¢[å®‰å…¨éƒ¨ç½²](../deployment/04-æ€§èƒ½ç›‘æ§.md)

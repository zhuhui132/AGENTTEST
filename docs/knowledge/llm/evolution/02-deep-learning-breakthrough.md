# ğŸš€ æ·±åº¦å­¦ä¹ çªç ´ - æŠ€æœ¯é©å‘½çš„å¼€ç«¯

## ğŸ“… æ—¶é—´èŠ‚ç‚¹: 2010-2015å¹´

### ğŸŒŸ å…³é”®çªç ´

#### 2012å¹´: AlexNeté©å‘½
- **å›¢é˜Ÿ**: Geoffrey Hintonå›¢é˜Ÿ
- **çªç ´ç‚¹**: æ·±åº¦CNNèµ¢å¾—ImageNetç«èµ›
- **æˆå°±**: Top-5é”™è¯¯ç‡15.3% (è¿œè¶…ç¬¬äºŒ26.2%)
- **æŠ€æœ¯æ„ä¹‰**: è¯æ˜äº†æ·±åº¦å­¦ä¹ çš„å®ç”¨ä»·å€¼

```python
# AlexNetæ ¸å¿ƒæ¶æ„ç®€åŒ–
class AlexNet:
    def __init__(self):
        self.conv1 = nn.Conv2d(3, 96, 11, stride=4)
        self.pool1 = nn.MaxPool2d(3, stride=2)
        self.conv2 = nn.Conv2d(96, 256, 5, padding=2)
        self.pool2 = nn.MaxPool2d(3, stride=2)
        self.conv3 = nn.Conv2d(256, 384, 3, padding=1)
        self.conv4 = nn.Conv2d(384, 384, 3, padding=1)
        self.conv5 = nn.Conv2d(384, 256, 3, padding=1)
        self.pool3 = nn.MaxPool2d(3, stride=2)
        self.fc6 = nn.Linear(9216, 4096)
        self.fc7 = nn.Linear(4096, 4096)
        self.fc8 = nn.Linear(4096, 1000)  # ImageNetç±»åˆ«
```

#### 2013å¹´: Dropoutæ­£åˆ™åŒ–
- **å›¢é˜Ÿ**: Geoffrey Hintonå›¢é˜Ÿ
- **çªç ´ç‚¹**: Dropoutæ­£åˆ™åŒ–æŠ€æœ¯
- **æŠ€æœ¯åŸç†**: éšæœºä¸¢å¼ƒç¥ç»å…ƒé˜²æ­¢è¿‡æ‹Ÿåˆ
- **æ•ˆæœ**: å¤§å¹…æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›

```python
# Dropoutæ­£åˆ™åŒ–å®ç°
class DropoutLayer:
    def __init__(self, dropout_rate=0.5):
        self.dropout_rate = dropout_rate

    def forward(self, x, training=True):
        if training:
            mask = (np.random.random(x.shape) > self.dropout_rate).astype(float)
            return x * mask / (1 - self.dropout_rate)
        return x
```

#### 2014å¹´: Generative Adversarial Networks (GAN)
- **åˆ›å§‹äºº**: Ian Goodfellow
- **çªç ´ç‚¹**: ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¶æ„
- **æ ¸å¿ƒæ€æƒ³**: ç”Ÿæˆå™¨vsåˆ¤åˆ«å™¨çš„å¯¹æŠ—è®­ç»ƒ
- **æ„ä¹‰**: ç”Ÿæˆæ¨¡å‹çš„å¼€åˆ›æ€§çªç ´

```python
# GANåŸºç¡€æ¶æ„
class Generator:
    def __init__(self):
        self.main = nn.Sequential(
            nn.Linear(100, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, 784),
            nn.Tanh()
        )

    def forward(self, noise):
        return self.main(noise)

class Discriminator:
    def __init__(self):
        self.main = nn.Sequential(
            nn.Linear(784, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, img):
        return self.main(img)
```

#### 2015å¹´: ResNetæ®‹å·®å­¦ä¹ 
- **å›¢é˜Ÿ**: å¾®è½¯äºšæ´²ç ”ç©¶é™¢
- **çªç ´ç‚¹**: æ®‹å·®è¿æ¥è§£å†³æ·±å±‚ç½‘ç»œé€€åŒ–
- **æ ¸å¿ƒåˆ›æ–°**: F(x) = H(x) + x (è·³è·ƒè¿æ¥)
- **æˆå°±**: 152å±‚ç½‘ç»œçš„æ·±åº¦è®­ç»ƒèƒ½åŠ›

```python
# ResNetæ®‹å·®å—å®ç°
class ResidualBlock:
    def __init__(self, in_channels, out_channels, stride=1):
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # è·³è·ƒè¿æ¥
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out)))
        out += self.shortcut(x)  # æ®‹å·®è¿æ¥
        out = self.relu(out)
        return out
```

## ğŸ—ï¸ æ·±åº¦å­¦ä¹ æŠ€æœ¯ä½“ç³»

### ğŸ”¬ æ ¸å¿ƒç®—æ³•çªç ´

#### 1. å·ç§¯ç¥ç»ç½‘ç»œ(CNN)æ¼”è¿›
- **LeNet-5 (1998)**: æ‰‹å†™æ•°å­—è¯†åˆ«
- **AlexNet (2012)**: ImageNetç«èµ›å† å†›
- **VGG (2014)**: 16/19å±‚æ·±åº¦ç½‘ç»œ
- **GoogLeNet (2014)**: Inceptionæ¨¡å—
- **ResNet (2015)**: æ®‹å·®å­¦ä¹ 

#### 2. ä¼˜åŒ–ç®—æ³•å‘å±•
- **SGD**: åŸºç¡€éšæœºæ¢¯åº¦ä¸‹é™
- **Momentum**: åŠ¨é‡åŠ é€Ÿæ”¶æ•›
- **AdaGrad**: è‡ªé€‚åº”å­¦ä¹ ç‡
- **RMSprop**: å‡æ–¹æ ¹ä¼ æ’­
- **Adam**: åŠ¨é‡+è‡ªé€‚åº”å­¦ä¹ ç‡

```python
# Adamä¼˜åŒ–å™¨å®ç°
class AdamOptimizer:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m = 0  # åŠ¨é‡
        self.v = 0  # äºŒé˜¶åŠ¨é‡
        self.t = 0

    def update(self, params, grads):
        self.t += 1
        lr_t = self.lr * np.sqrt(1 - self.beta2**self.t) / (1 - self.beta1**self.t)

        self.m = self.beta1 * self.m + (1 - self.beta1) * grads
        self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2

        m_hat = self.m / (1 - self.beta1**self.t)
        v_hat = self.v / (1 - self.beta2**self.t)

        params -= lr_t * m_hat / (np.sqrt(v_hat) + self.epsilon)
        return params
```

#### 3. æ­£åˆ™åŒ–æŠ€æœ¯
- **Dropout**: éšæœºç¥ç»å…ƒä¸¢å¼ƒ
- **Batch Normalization**: æ‰¹æ ‡å‡†åŒ–
- **Weight Decay**: æƒé‡è¡°å‡
- **Data Augmentation**: æ•°æ®å¢å¼º

## ğŸ“Š æŠ€æœ¯çªç ´ç»Ÿè®¡

### ğŸš€ æ€§èƒ½æå‡æŒ‡æ ‡
| æŠ€æœ¯ | çªç ´å¹´ä»½ | æ€§èƒ½æå‡ | åº”ç”¨é¢†åŸŸ |
|------|----------|----------|----------|
| æ·±åº¦CNN | 2012 | å‡†ç¡®ç‡æå‡50%+ | è®¡ç®—æœºè§†è§‰ |
| Dropout | 2013 | è¿‡æ‹Ÿåˆé™ä½30% | æ·±åº¦å­¦ä¹ é€šç”¨ |
| BatchNorm | 2015 | æ”¶æ•›é€Ÿåº¦æå‡2å€ | æ·±åº¦å­¦ä¹ é€šç”¨ |
| ResNet | 2015 | æ·±åº¦å¢åŠ 8å€ | è®¡ç®—æœºè§†è§‰ |

### ğŸ“ˆ è®¡ç®—èµ„æºéœ€æ±‚
| æ¨¡å‹ | å‚æ•°é‡ | è®­ç»ƒæ—¶é—´ | GPUéœ€æ±‚ |
|------|--------|----------|----------|
| AlexNet | 60M | 5-6å¤© | 2x GPU |
| VGG-16 | 138M | 2-3å‘¨ | 4x GPU |
| ResNet-152 | 60M | 2-3å‘¨ | 8x GPU |

## ğŸŒ åº”ç”¨é¢†åŸŸæ‰©å±•

### ğŸ“Š è®¡ç®—æœºè§†è§‰
- **å›¾åƒåˆ†ç±»**: ImageNet, CIFAR-10/100
- **ç›®æ ‡æ£€æµ‹**: R-CNN, YOLO, SSD
- **å›¾åƒåˆ†å‰²**: FCN, U-Net, Mask R-CNN
- **äººè„¸è¯†åˆ«**: FaceNet, DeepFace

### ğŸ”Š è‡ªç„¶è¯­è¨€å¤„ç†
- **è¯åµŒå…¥**: Word2Vec, GloVe
- **åºåˆ—æ¨¡å‹**: LSTM, GRU, BiLSTM
- **æœºå™¨ç¿»è¯‘**: Seq2Seq, Attention
- **æ–‡æœ¬åˆ†ç±»**: CNN-based, LSTM-based

### ğŸ® æ¸¸æˆAI
- **å›´æ£‹**: AlphaGoæ—©æœŸç‰ˆæœ¬
- **Atariæ¸¸æˆ**: DRLåŸºç¡€
- **å®æ—¶ç­–ç•¥**: CNN+RLç»“åˆ

## ğŸ”§ æ·±åº¦å­¦ä¹ æ¡†æ¶å‘å±•

### ğŸ›ï¸ ä¸»è¦æ¡†æ¶æ¼”è¿›

#### 2010-2012: æ—©æœŸæ¡†æ¶
- **Caffe**: UC Berkeley (2014)
- **Theano**: University of Montreal (2012)
- **Torch**: Ronan Collobert (2002, but popularized in 2010s)

#### 2014-2015: ç°ä»£æ¡†æ¶
- **TensorFlow**: Google (2015)
- **PyTorch**: Facebook (2016)
- **Keras**: FranÃ§ois Chollet (2015)
- **MXNet**: Apache (2015)

### ğŸ“Š æ¡†æ¶ç‰¹æ€§å¯¹æ¯”
| æ¡†æ¶ | å‘å¸ƒå¹´ä»½ | ä¸»è¦ç‰¹æ€§ | æ˜“ç”¨æ€§ |
|------|----------|----------|----------|
| Caffe | 2014 | C++æ€§èƒ½, Pythonæ¥å£ | ä¸­ç­‰ |
| TensorFlow | 2015 | è‡ªåŠ¨å¾®åˆ†, åˆ†å¸ƒå¼ | ä¸­ç­‰ |
| PyTorch | 2016 | åŠ¨æ€å›¾, æ˜“è°ƒè¯• | é«˜ |
| Keras | 2015 | é«˜çº§API, å¿«é€ŸåŸå‹ | å¾ˆé«˜ |

## ğŸ§  æ·±åº¦å­¦ä¹ ç†è®ºçªç ´

### ğŸ“š æ ¸å¿ƒç†è®ºè´¡çŒ®

#### 1. æ™®é€‚é€¼è¿‘å®šç†
- **Cython-Hornikå®šç†**: æ·±åº¦ç½‘ç»œå¯ä»¥é€¼è¿‘ä»»æ„å‡½æ•°
- **ç½‘ç»œæ·±åº¦**: æŒ‡æ•°çº§èƒ½åŠ›å¢é•¿
- **æ¿€æ´»å‡½æ•°**: éçº¿æ€§æ˜¯å¿…è¦æ¡ä»¶

#### 2. ä¼˜åŒ–ç†è®ºåˆ†æ
- **æŸå¤±å‡½æ•°**: éå‡¸ä¼˜åŒ–æŒ‘æˆ˜
- **æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**: æ·±åº¦ç½‘ç»œè®­ç»ƒéš¾é¢˜
- **æ­£åˆ™åŒ–**: ç†è®ºåŸºç¡€çš„å»ºç«‹

#### 3. æ³›åŒ–ç†è®º
- **VCç»´åº¦**: ç½‘ç»œå®¹é‡åº¦é‡
- **åå·®-æ–¹å·®**: æ¨¡å‹å¤æ‚åº¦æƒè¡¡
- **æ ·æœ¬å¤æ‚åº¦**: å­¦ä¹ æ‰€éœ€æ•°æ®é‡

## ğŸ“ æ•™è‚²å½±å“

### ğŸ“š æ·±åº¦å­¦ä¹ è¯¾ç¨‹æ™®åŠ
- **Stanford CS231n**: CNNè¯¾ç¨‹
- **MIT 6.867**: æœºå™¨å­¦ä¹ è¿›é˜¶
- **Toronto CSC321**: æ·±åº¦å­¦ä¹ ç†è®º

### ğŸ‘¨â€ğŸ’» å­¦ä¹ èµ„æºå¢é•¿
- **åœ¨çº¿è¯¾ç¨‹**: Coursera, edXå¿«é€Ÿå¢é•¿
- **å¼€æºé¡¹ç›®**: GitHubæ´»è·ƒåº¦æ¿€å¢
- **ç«èµ›å¹³å°**: Kaggleæ´»è·ƒåº¦æå‡

## ğŸ¢ äº§ä¸šåŒ–åº”ç”¨

### ğŸ“± ç§»åŠ¨ç«¯éƒ¨ç½²
- **æ¨¡å‹å‹ç¼©**: é‡åŒ–ã€å‰ªæã€è’¸é¦
- **è¾¹ç¼˜è®¡ç®—**: ç§»åŠ¨ç«¯æ·±åº¦å­¦ä¹ 
- **å®æ—¶åº”ç”¨**: AR/VRåŸºç¡€æŠ€æœ¯

### ğŸ­ ä¼ä¸šçº§éƒ¨ç½²
- **å¤§è§„æ¨¡è®­ç»ƒ**: åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿ
- **æ¨¡å‹æœåŠ¡**: é«˜å¹¶å‘æ¨ç†ä¼˜åŒ–
- **è‡ªåŠ¨åŒ–ML**: AutoMLå¹³å°å…´èµ·

## ğŸ¯ æŠ€æœ¯æŒ‘æˆ˜

### ğŸš« æ·±åº¦ç½‘ç»œé—®é¢˜
- **è¿‡æ‹Ÿåˆ**: è®­ç»ƒæ•°æ®ä¸è¶³
- **è®¡ç®—æˆæœ¬**: GPUèµ„æºéœ€æ±‚å·¨å¤§
- **å¯è§£é‡Šæ€§**: é»‘ç›’é—®é¢˜çªå‡º
- **æ•°æ®ä¾èµ–**: éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®

### ğŸ”§ è§£å†³æ–¹æ¡ˆ
- **æ•°æ®å¢å¼º**: æ‰©å……è®­ç»ƒæ•°æ®
- **è¿ç§»å­¦ä¹ **: åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹
- **æ¨¡å‹å‹ç¼©**: é™ä½éƒ¨ç½²æˆæœ¬
- **å¯è§£é‡ŠAI**: æå‡é€æ˜åº¦

## ğŸ“ˆ å‘å±•è¶‹åŠ¿

### ğŸ”® 2010-2015æŠ€æœ¯è¶‹åŠ¿
1. **ç½‘ç»œæ·±åº¦**: ä»å‡ å±‚åˆ°æ•°ç™¾å±‚
2. **å‚æ•°è§„æ¨¡**: ä»ç™¾ä¸‡åˆ°æ•°äº¿çº§
3. **è®¡ç®—éœ€æ±‚**: ä»CPUåˆ°GPUå¹¶è¡Œ
4. **åº”ç”¨æ‰©å±•**: ä»å•ä¸€åˆ°å¤šé¢†åŸŸ

### ğŸš€ æŠ€æœ¯åŠ é€Ÿå› ç´ 
- **ç¡¬ä»¶å‘å±•**: GPUæ€§èƒ½æå‡
- **ç®—æ³•ä¼˜åŒ–**: è®­ç»ƒæ•ˆç‡æå‡
- **æ•°æ®è§„æ¨¡**: å¤§æ•°æ®æ—¶ä»£æ¥ä¸´
- **å¼€æºç”Ÿæ€**: ç¤¾åŒºè´¡çŒ®åŠ é€Ÿ

## ğŸ“ æ·±åº¦å­¦ä¹ çªç ´æ€»ç»“

### âœ… ä¸»è¦æˆå°±
1. **æ€§èƒ½çªç ´**: è¶…è¶Šä¼ ç»Ÿæœºå™¨å­¦ä¹ 
2. **è‡ªåŠ¨åŒ–**: ç‰¹å¾å·¥ç¨‹è‡ªåŠ¨åŒ–
3. **è§„æ¨¡åŒ–**: å¤„ç†å¤æ‚æ•°æ®èƒ½åŠ›
4. **å®ç”¨åŒ–**: å·¥ä¸šåº”ç”¨å¯è¡Œæ€§éªŒè¯

### ğŸŒŸ å†å²æ„ä¹‰
æ·±åº¦å­¦ä¹ çªç ´é˜¶æ®µï¼ˆ2010-2015ï¼‰æ˜¯AIå‘å±•çš„è½¬æŠ˜ç‚¹ï¼š
- **æŠ€æœ¯é©å‘½**: ä»è§„åˆ™é©±åŠ¨åˆ°æ•°æ®é©±åŠ¨
- **äº§ä¸šå˜é©**: é‡å¡‘å¤šä¸ªè¡Œä¸š
- **å­¦æœ¯ç¹è£**: å¤§é‡ç ”ç©¶è€…æ¶Œå…¥
- **åº”ç”¨çˆ†å‘**: å®ç”¨AIäº§å“æ¿€å¢

### ğŸš€ æœªæ¥å½±å“
è¿™ä¸€æ—¶æœŸçš„çªç ´ä¸ºåç»­å‘å±•å¥ å®šåŸºç¡€ï¼š
- **ç†è®ºå®Œå–„**: ä¸ºTransformerç­‰çªç ´é“ºå«
- **å·¥ç¨‹å®è·µ**: å¤§è§„æ¨¡è®­ç»ƒç»éªŒç§¯ç´¯
- **äº§ä¸šç”Ÿæ€**: ç¡¬ä»¶-è½¯ä»¶ååŒå‘å±•
- **äººæ‰åŸ¹å…»**: æ·±åº¦å­¦ä¹ äººæ‰è§„æ¨¡å¢é•¿

**æ·±åº¦å­¦ä¹ çªç ´**ä¸ä»…æ˜¯æŠ€æœ¯å±‚é¢çš„é‡å¤§è¿›æ­¥ï¼Œæ›´æ˜¯AIå‘å±•å²ä¸Šçš„é‡Œç¨‹ç¢‘ï¼Œå¼€å¯äº†ç°ä»£AIæ—¶ä»£çš„å¤§é—¨ã€‚

---

*ç›¸å…³æ–‡æ¡£: [03-Transformeræ¶æ„é©å‘½](../architecture/03-transformer-revolution.md)*
*æŠ€æœ¯æ¼”è¿›: [æŠ€æœ¯å‘å±•è¶‹åŠ¿](../../trends/01-æŠ€æœ¯å‘å±•è¶‹åŠ¿.md)*

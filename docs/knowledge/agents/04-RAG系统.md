# ğŸ” RAGé›†æˆAgentç³»ç»Ÿ

## ğŸ“š æ¦‚è¿°

æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG, Retrieval-Augmented Generation)æ˜¯å°†ä¿¡æ¯æ£€ç´¢ä¸ç”Ÿæˆæ¨¡å‹ç»“åˆçš„æŠ€æœ¯ï¼Œä½¿Agentèƒ½å¤ŸåŸºäºå¤–éƒ¨çŸ¥è¯†åº“ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´åŠæ—¶çš„å›ç­”ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»RAGç³»ç»Ÿçš„æ¶æ„è®¾è®¡ã€å®ç°æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚

## ğŸ—ï¸ RAGç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒç»„ä»¶è®¾è®¡
```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import time
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import json
import hashlib
import logging

class RAGComponent(Enum):
    """RAGç»„ä»¶ç±»å‹"""
    RETRIEVER = "retriever"      # æ£€ç´¢å™¨
    INDEXER = "indexer"          # ç´¢å¼•å™¨
    RERANKER = "reranker"       # é‡æ’åºå™¨
    GENERATOR = "generator"        # ç”Ÿæˆå™¨
    MEMORY = "memory"             # è®°å¿†ç³»ç»Ÿ

class RetrievalMethod(Enum):
    """æ£€ç´¢æ–¹æ³•"""
    VECTOR_SEARCH = "vector"        # å‘é‡æ£€ç´¢
    KEYWORD_SEARCH = "keyword"     # å…³é”®è¯æ£€ç´¢
    HYBRID = "hybrid"              # æ··åˆæ£€ç´¢
    SEMANTIC = "semantic"          # è¯­ä¹‰æ£€ç´¢

@dataclass
class Document:
    """æ–‡æ¡£å¯¹è±¡"""
    id: str = field(default_factory=lambda: str(time.time()))
    content: str = ""
    title: str = ""
    url: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    embedding: Optional[List[float]] = None
    timestamp: float = field(default_factory=time.time)
    score: float = 0.0
    source: str = ""
    chunk_id: Optional[str] = None

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœ"""
    query: str
    documents: List[Document]
    retrieval_time: float = 0.0
    total_candidates: int = 0
    method: RetrievalMethod = RetrievalMethod.VECTOR_SEARCH
    metadata: Dict[str, Any] = field(default_factory=dict)

class BaseRAGComponent(ABC):
    """RAGç»„ä»¶åŸºç¡€ç±»"""

    def __init__(self, name: str):
        self.name = name
        self.logger = logging.getLogger(f"rag.{name}")

    @abstractmethod
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–ç»„ä»¶"""
        pass

    @abstractmethod
    async def process(self, *args, **kwargs) -> Any:
        """å¤„ç†è¯·æ±‚"""
        pass

    @abstractmethod
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        pass

class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.components = {}
        self.retrieval_history = []
        self.performance_metrics = {
            'total_queries': 0,
            'avg_retrieval_time': 0.0,
            'avg_generation_time': 0.0,
            'cache_hit_rate': 0.0
        }

        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()

    def _initialize_components(self):
        """åˆå§‹åŒ–RAGç»„ä»¶"""
        # æ£€ç´¢å™¨
        if 'retriever' in self.config:
            self.components[RAGComponent.RETRIEVER] = self._create_retriever(
                self.config['retriever']
            )

        # ç´¢å¼•å™¨
        if 'indexer' in self.config:
            self.components[RAGComponent.INDEXER] = self._create_indexer(
                self.config['indexer']
            )

        # é‡æ’åºå™¨
        if 'reranker' in self.config:
            self.components[RAGComponent.RERANKER] = self._create_reranker(
                self.config['reranker']
            )

        # ç”Ÿæˆå™¨
        if 'generator' in self.config:
            self.components[RAGComponent.GENERATOR] = self._create_generator(
                self.config['generator']
            )

        # è®°å¿†ç³»ç»Ÿ
        if 'memory' in self.config:
            self.components[RAGComponent.MEMORY] = self._create_memory(
                self.config['memory']
            )

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        success = True

        for component_name, component in self.components.items():
            try:
                if await component.initialize():
                    self.logger.info(f"Component {component_name} initialized successfully")
                else:
                    self.logger.error(f"Failed to initialize component {component_name}")
                    success = False
            except Exception as e:
                self.logger.error(f"Error initializing {component_name}: {e}")
                success = False

        return success

    async def query(self, query_text: str, top_k: int = 5,
                   retrieval_method: RetrievalMethod = RetrievalMethod.VECTOR_SEARCH,
                   rerank: bool = True) -> Dict[str, Any]:
        """æ‰§è¡ŒRAGæŸ¥è¯¢"""
        start_time = time.time()

        try:
            # 1. æ£€ç´¢æ–‡æ¡£
            retrieval_result = await self._retrieve_documents(
                query_text, top_k, retrieval_method
            )

            # 2. é‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if rerank and RAGComponent.RERANKER in self.components:
                retrieval_result = await self._rerank_documents(
                    query_text, retrieval_result
                )

            # 3. ç”Ÿæˆå›ç­”
            if RAGComponent.GENERATOR in self.components:
                generation_result = await self._generate_answer(
                    query_text, retrieval_result
                )
            else:
                generation_result = {
                    'answer': "ç”Ÿæˆå™¨æœªé…ç½®",
                    'context': retrieval_result
                }

            # 4. å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ
            if RAGComponent.MEMORY in self.components:
                await self._store_to_memory(query_text, retrieval_result, generation_result)

            # 5. æ›´æ–°ç»Ÿè®¡
            total_time = time.time() - start_time
            self._update_performance_metrics(total_time)

            # 6. è®°å½•æŸ¥è¯¢å†å²
            self.retrieval_history.append({
                'query': query_text,
                'retrieval_result': retrieval_result,
                'generation_result': generation_result,
                'timestamp': time.time(),
                'total_time': total_time
            })

            return {
                'success': True,
                'query': query_text,
                'retrieval': retrieval_result,
                'generation': generation_result,
                'total_time': total_time
            }

        except Exception as e:
            self.logger.error(f"RAG query failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'query': query_text
            }

    async def _retrieve_documents(self, query_text: str, top_k: int,
                               method: RetrievalMethod) -> RetrievalResult:
        """æ£€ç´¢æ–‡æ¡£"""
        retriever = self.components.get(RAGComponent.RETRIEVER)
        if not retriever:
            raise ValueError("Retriever not configured")

        return await retriever.retrieve(query_text, top_k, method)

    async def _rerank_documents(self, query_text: str,
                               retrieval_result: RetrievalResult) -> RetrievalResult:
        """é‡æ’åºæ–‡æ¡£"""
        reranker = self.components[RAGComponent.RERANKER]
        if not reranker:
            return retrieval_result

        reranked_docs = await reranker.rerank(query_text, retrieval_result.documents)

        # åˆ›å»ºæ–°çš„æ£€ç´¢ç»“æœ
        return RetrievalResult(
            query=retrieval_result.query,
            documents=reranked_docs,
            retrieval_time=retrieval_result.retrieval_time,
            total_candidates=retrieval_result.total_candidates,
            method=retrieval_result.method,
            metadata={
                **retrieval_result.metadata,
                'reranked': True,
                'rerank_time': time.time()
            }
        )

    async def _generate_answer(self, query_text: str,
                            retrieval_result: RetrievalResult) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”"""
        generator = self.components.get(RAGComponent.GENERATOR)
        if not generator:
            raise ValueError("Generator not configured")

        # æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(retrieval_result.documents)

        return await generator.generate(query_text, context)

    def _build_context(self, documents: List[Document]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        if not documents:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"

        context_parts = []
        for i, doc in enumerate(documents, 1):
            context_part = f"[æ–‡æ¡£{i}] {doc.title}\n{doc.content}"
            if doc.url:
                context_part += f"\næ¥æº: {doc.url}"
            context_parts.append(context_part)

        return "\n\n".join(context_parts)

    async def _store_to_memory(self, query: str, retrieval_result: RetrievalResult,
                           generation_result: Dict[str, Any]):
        """å­˜å‚¨åˆ°è®°å¿†ç³»ç»Ÿ"""
        memory = self.components.get(RAGComponent.MEMORY)
        if not memory:
            return

        memory_item = {
            'type': 'rag_query',
            'query': query,
            'retrieved_docs': [doc.id for doc in retrieval_result.documents],
            'answer': generation_result.get('answer', ''),
            'timestamp': time.time()
        }

        await memory.store(memory_item)

    def _update_performance_metrics(self, total_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics['total_queries'] += 1

        # æ›´æ–°å¹³å‡æ£€ç´¢æ—¶é—´
        current_avg = self.performance_metrics['avg_retrieval_time']
        n = self.performance_metrics['total_queries']
        self.performance_metrics['avg_retrieval_time'] = (
            (current_avg * (n - 1) + total_time) / n
        )

    async def add_document(self, document: Document) -> bool:
        """æ·»åŠ æ–‡æ¡£"""
        indexer = self.components.get(RAGComponent.INDEXER)
        if not indexer:
            self.logger.warning("Indexer not configured")
            return False

        return await indexer.add_document(document)

    async def add_documents(self, documents: List[Document]) -> int:
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        indexer = self.components.get(RAGComponent.INDEXER)
        if not indexer:
            self.logger.warning("Indexer not configured")
            return 0

        return await indexer.add_documents(documents)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡"""
        stats = {
            'performance_metrics': self.performance_metrics,
            'component_stats': {},
            'retrieval_history_size': len(self.retrieval_history)
        }

        # è·å–å„ç»„ä»¶ç»Ÿè®¡
        for component_name, component in self.components.items():
            stats['component_stats'][component_name.value] = component.get_stats()

        return stats
```

## ğŸ” æ–‡æ¡£æ£€ç´¢å™¨

### å‘é‡æ£€ç´¢å®ç°
```python
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Tuple

class VectorRetriever(BaseRAGComponent):
    """å‘é‡æ£€ç´¢å™¨"""

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 dimension: int = 384):
        super().__init__("vector_retriever")
        self.model_name = model_name
        self.dimension = dimension
        self.model = None
        self.index = None
        self.documents = []
        self.is_initialized = False

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨"""
        try:
            # åŠ è½½æ¨¡å‹
            self.model = SentenceTransformer(self.model_name)
            self.logger.info(f"Loaded model: {self.model_name}")

            # åˆå§‹åŒ–FAISSç´¢å¼•
            self.index = faiss.IndexFlatIP(self.dimension)
            self.logger.info("Initialized FAISS index")

            self.is_initialized = True
            return True

        except Exception as e:
            self.logger.error(f"Failed to initialize vector retriever: {e}")
            return False

    async def retrieve(self, query: str, top_k: int = 5,
                       method: RetrievalMethod = RetrievalMethod.VECTOR_SEARCH) -> RetrievalResult:
        """æ‰§è¡Œå‘é‡æ£€ç´¢"""
        if not self.is_initialized:
            raise RuntimeError("Vector retriever not initialized")

        start_time = time.time()

        try:
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self.model.encode([query])[0]
            query_embedding = query_embedding.astype('float32')

            # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
            query_embedding = query_embedding / np.linalg.norm(query_embedding)

            # æ‰§è¡Œæ£€ç´¢
            search_k = min(top_k * 2, len(self.documents))  # æ£€ç´¢æ›´å¤šå€™é€‰
            scores, indices = self.index.search(
                np.array([query_embedding]), search_k
            )

            # æ„å»ºç»“æœ
            retrieved_docs = []
            for score, idx in zip(scores[0], indices[0]):
                if idx >= 0 and idx < len(self.documents):
                    doc = self.documents[idx]
                    doc.score = float(score)
                    retrieved_docs.append(doc)

            retrieval_time = time.time() - start_time

            return RetrievalResult(
                query=query,
                documents=retrieved_docs[:top_k],
                retrieval_time=retrieval_time,
                total_candidates=len(self.documents),
                method=RetrievalMethod.VECTOR_SEARCH,
                metadata={
                    'model_name': self.model_name,
                    'index_type': 'FAISS FlatIP',
                    'search_k': search_k
                }
            )

        except Exception as e:
            self.logger.error(f"Vector retrieval failed: {e}")
            raise

    async def add_document(self, document: Document) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        try:
            # ç”Ÿæˆæ–‡æ¡£åµŒå…¥
            if not document.content:
                self.logger.warning(f"Document {document.id} has no content")
                return False

            content = f"{document.title} {document.content}"
            embedding = self.model.encode([content])[0]
            embedding = embedding.astype('float32')

            # å½’ä¸€åŒ–åµŒå…¥å‘é‡
            embedding = embedding / np.linalg.norm(embedding)

            # æ›´æ–°æ–‡æ¡£
            document.embedding = embedding.tolist()
            doc_index = len(self.documents)
            self.documents.append(document)

            # æ·»åŠ åˆ°ç´¢å¼•
            self.index.add(np.array([embedding]))

            self.logger.info(f"Added document {document.id} to index")
            return True

        except Exception as e:
            self.logger.error(f"Failed to add document to index: {e}")
            return False

    async def add_documents(self, documents: List[Document]) -> int:
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        added_count = 0

        for document in documents:
            if await self.add_document(document):
                added_count += 1

        return added_count

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ£€ç´¢å™¨ç»Ÿè®¡"""
        return {
            'model_name': self.model_name,
            'dimension': self.dimension,
            'is_initialized': self.is_initialized,
            'total_documents': len(self.documents),
            'index_type': type(self.index).__name__,
            'index_ntotal': self.index.ntotal if self.index else 0
        }

class HybridRetriever(BaseRAGComponent):
    """æ··åˆæ£€ç´¢å™¨ - ç»“åˆå‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢"""

    def __init__(self, vector_retriever: VectorRetriever, keyword_weight: float = 0.3):
        super().__init__("hybrid_retriever")
        self.vector_retriever = vector_retriever
        self.keyword_weight = keyword_weight
        self.vector_weight = 1.0 - keyword_weight

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨"""
        # åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨
        return await self.vector_retriever.initialize()

    async def retrieve(self, query: str, top_k: int = 5,
                       method: RetrievalMethod = RetrievalMethod.HYBRID) -> RetrievalResult:
        """æ‰§è¡Œæ··åˆæ£€ç´¢"""
        start_time = time.time()

        try:
            # å¹¶è¡Œæ‰§è¡Œä¸¤ç§æ£€ç´¢
            vector_task = self.vector_retriever.retrieve(query, top_k * 2)
            keyword_task = self._keyword_search(query, top_k * 2)

            vector_result, keyword_result = await asyncio.gather(
                vector_task, keyword_task
            )

            # åˆå¹¶å’Œé‡æ’åºç»“æœ
            merged_docs = self._merge_results(
                vector_result.documents,
                keyword_result.documents,
                query
            )

            retrieval_time = time.time() - start_time

            return RetrievalResult(
                query=query,
                documents=merged_docs[:top_k],
                retrieval_time=retrieval_time,
                total_candidates=len(merged_docs),
                method=RetrievalMethod.HYBRID,
                metadata={
                    'vector_weight': self.vector_weight,
                    'keyword_weight': self.keyword_weight,
                    'vector_candidates': len(vector_result.documents),
                    'keyword_candidates': len(keyword_result.documents)
                }
            )

        except Exception as e:
            self.logger.error(f"Hybrid retrieval failed: {e}")
            raise

    async def _keyword_search(self, query: str, top_k: int) -> List[Document]:
        """å…³é”®è¯æœç´¢"""
        # ç®€åŒ–çš„å…³é”®è¯æœç´¢å®ç°
        query_terms = set(query.lower().split())
        scored_docs = []

        for doc in self.vector_retriever.documents:
            content_lower = f"{doc.title} {doc.content}".lower()
            doc_terms = set(content_lower.split())

            # è®¡ç®—åŒ¹é…åˆ†æ•°
            intersection = query_terms & doc_terms
            union = query_terms | doc_terms

            if intersection:
                jaccard_similarity = len(intersection) / len(union)
                doc.score = jaccard_similarity
                scored_docs.append(doc)

        # æŒ‰åˆ†æ•°æ’åº
        scored_docs.sort(key=lambda x: x.score, reverse=True)
        return scored_docs[:top_k]

    def _merge_results(self, vector_docs: List[Document], keyword_docs: List[Document],
                     query: str) -> List[Document]:
        """åˆå¹¶æ£€ç´¢ç»“æœ"""
        # åˆ›å»ºæ–‡æ¡£IDåˆ°æ–‡æ¡£çš„æ˜ å°„
        all_docs = {}

        # æ·»åŠ å‘é‡æ£€ç´¢ç»“æœ
        for doc in vector_docs:
            if doc.id not in all_docs:
                all_docs[doc.id] = {
                    'doc': doc,
                    'vector_score': doc.score,
                    'keyword_score': 0.0
                }
            else:
                all_docs[doc.id]['vector_score'] = max(
                    all_docs[doc.id]['vector_score'],
                    doc.score
                )

        # æ·»åŠ å…³é”®è¯æ£€ç´¢ç»“æœ
        for doc in keyword_docs:
            if doc.id not in all_docs:
                all_docs[doc.id] = {
                    'doc': doc,
                    'vector_score': 0.0,
                    'keyword_score': doc.score
                }
            else:
                all_docs[doc.id]['keyword_score'] = max(
                    all_docs[doc.id]['keyword_score'],
                    doc.score
                )

        # è®¡ç®—æ··åˆåˆ†æ•°
        for doc_info in all_docs.values():
            combined_score = (
                self.vector_weight * doc_info['vector_score'] +
                self.keyword_weight * doc_info['keyword_score']
            )
            doc_info['doc'].score = combined_score

        # æ’åºå¹¶è¿”å›
        merged_docs = [info['doc'] for info in all_docs.values()]
        merged_docs.sort(key=lambda x: x.score, reverse=True)

        return merged_docs

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ··åˆæ£€ç´¢å™¨ç»Ÿè®¡"""
        return {
            'vector_retriever_stats': self.vector_retriever.get_stats(),
            'keyword_weight': self.keyword_weight,
            'vector_weight': self.vector_weight
        }
```

## ğŸ“„ æ–‡æ¡£ç´¢å¼•å™¨

### FAISSç´¢å¼•å®ç°
```python
import faiss
import numpy as np
import pickle
import os
from pathlib import Path

class FAISSIndexer(BaseRAGComponent):
    """FAISSç´¢å¼•å™¨"""

    def __init__(self, index_path: str = "faiss_index.index",
                 documents_path: str = "documents.pkl",
                 index_type: str = "flat"):
        super().__init__("faiss_indexer")
        self.index_path = Path(index_path)
        self.documents_path = Path(documents_path)
        self.index_type = index_type
        self.index = None
        self.documents = []
        self.dimension = None

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–ç´¢å¼•å™¨"""
        try:
            # å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
            if await self._load_index():
                self.logger.info("Loaded existing index")
            else:
                self.logger.info("No existing index found, creating new one")
                self._create_empty_index()

            return True

        except Exception as e:
            self.logger.error(f"Failed to initialize indexer: {e}")
            return False

    def _create_empty_index(self):
        """åˆ›å»ºç©ºç´¢å¼•"""
        if self.dimension is None:
            self.dimension = 384  # é»˜è®¤ç»´åº¦

        if self.index_type == "flat":
            self.index = faiss.IndexFlatIP(self.dimension)
        elif self.index_type == "ivf":
            nlist = 100
            quantizer = faiss.IndexFlatIP(self.dimension)
            self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)
        elif self.index_type == "hnsw":
            self.index = faiss.IndexHNSWFlat(self.dimension, 32)
        else:
            self.index = faiss.IndexFlatIP(self.dimension)

    async def _load_index(self) -> bool:
        """åŠ è½½ç´¢å¼•å’Œæ–‡æ¡£"""
        # åŠ è½½ç´¢å¼•
        if self.index_path.exists():
            self.index = faiss.read_index(str(self.index_path))
            self.dimension = self.index.d
        else:
            return False

        # åŠ è½½æ–‡æ¡£
        if self.documents_path.exists():
            with open(self.documents_path, 'rb') as f:
                self.documents = pickle.load(f)
        else:
            return False

        return True

    async def _save_index(self):
        """ä¿å­˜ç´¢å¼•å’Œæ–‡æ¡£"""
        try:
            # ä¿å­˜ç´¢å¼•
            if self.index:
                faiss.write_index(self.index, str(self.index_path))

            # ä¿å­˜æ–‡æ¡£
            with open(self.documents_path, 'wb') as f:
                pickle.dump(self.documents, f)

            self.logger.info("Index and documents saved successfully")

        except Exception as e:
            self.logger.error(f"Failed to save index: {e}")

    async def add_document(self, document: Document) -> bool:
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        if document.embedding is None:
            self.logger.warning(f"Document {document.id} has no embedding")
            return False

        try:
            # æ·»åŠ åˆ°æ–‡æ¡£åˆ—è¡¨
            self.documents.append(document)

            # æ·»åŠ åˆ°ç´¢å¼•
            if self.index_type == "ivf":
                # IVFç´¢å¼•éœ€è¦è®­ç»ƒ
                if len(self.documents) % 1000 == 0:  # æ¯1000ä¸ªæ–‡æ¡£é‡æ–°è®­ç»ƒ
                    await self._retrain_index()
                else:
                    # ç›´æ¥æ·»åŠ 
                    embedding = np.array([document.embedding], dtype='float32')
                    self.index.add(embedding)
            else:
                embedding = np.array([document.embedding], dtype='float32')
                self.index.add(embedding)

            return True

        except Exception as e:
            self.logger.error(f"Failed to add document: {e}")
            return False

    async def add_documents(self, documents: List[Document]) -> int:
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        added_count = 0
        embeddings = []

        # æ”¶é›†æœ‰æ•ˆåµŒå…¥
        for doc in documents:
            if doc.embedding:
                embeddings.append(doc.embedding)
                self.documents.append(doc)
                added_count += 1

        if embeddings:
            try:
                embeddings_array = np.array(embeddings, dtype='float32')

                if self.index_type == "ivf":
                    await self._retrain_index()
                else:
                    self.index.add(embeddings_array)

                self.logger.info(f"Added {added_count} documents to index")

            except Exception as e:
                self.logger.error(f"Failed to add embeddings: {e}")
                added_count = 0

        return added_count

    async def _retrain_index(self):
        """é‡æ–°è®­ç»ƒç´¢å¼•ï¼ˆä¸»è¦ç”¨äºIVFï¼‰"""
        if len(self.documents) < 1000:
            return  # æ–‡æ¡£å¤ªå°‘ï¼Œä¸éœ€è¦é‡æ–°è®­ç»ƒ

        try:
            # æå–æ‰€æœ‰åµŒå…¥
            embeddings = np.array(
                [doc.embedding for doc in self.documents if doc.embedding],
                dtype='float32'
            )

            # é‡æ–°åˆ›å»ºç´¢å¼•
            if self.index_type == "ivf":
                nlist = min(100, len(self.documents) // 10)
                quantizer = faiss.IndexFlatIP(self.dimension)
                index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist)

                # è®­ç»ƒç´¢å¼•
                index.train(embeddings)
                index.add(embeddings)
                self.index = index

            self.logger.info("Index retrained successfully")

        except Exception as e:
            self.logger.error(f"Failed to retrain index: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•å™¨ç»Ÿè®¡"""
        return {
            'index_type': self.index_type,
            'dimension': self.dimension,
            'total_documents': len(self.documents),
            'index_ntotal': self.index.ntotal if self.index else 0,
            'is_trained': hasattr(self.index, 'is_trained') and self.index.is_trained if self.index else False
        }

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            await self._save_index()
        except Exception as e:
            self.logger.error(f"Failed to save index during cleanup: {e}")

class DocumentChunker:
    """æ–‡æ¡£åˆ†å—å™¨"""

    def __init__(self, chunk_size: int = 500, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def chunk_document(self, document: Document) -> List[Document]:
        """å°†æ–‡æ¡£åˆ†å—"""
        if not document.content:
            return [document]

        content = document.content
        chunks = []

        # æŒ‰å¥å­åˆ†å—
        sentences = self._split_sentences(content)
        current_chunk = ""
        chunk_id = 0

        for sentence in sentences:
            # æ£€æŸ¥æ·»åŠ è¿™ä¸ªå¥å­æ˜¯å¦ä¼šè¶…è¿‡å—å¤§å°
            if len(current_chunk + sentence) > self.chunk_size and current_chunk:
                # åˆ›å»ºæ–‡æ¡£å—
                chunk_doc = self._create_chunk_document(
                    document, current_chunk, chunk_id
                )
                chunks.append(chunk_doc)

                current_chunk = sentence  # é‡ç½®ï¼Œä¿ç•™é‡å éƒ¨åˆ†
                chunk_id += 1
            else:
                current_chunk += sentence

        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunk_doc = self._create_chunk_document(
                document, current_chunk, chunk_id
            )
            chunks.append(chunk_doc)

        return chunks

    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        import re

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­
        sentence_endings = r'[.!?]+(?=\s|$)'
        sentences = re.split(sentence_endings, text)

        # è¿‡æ»¤ç©ºå¥å­
        sentences = [s.strip() for s in sentences if s.strip()]

        return sentences

    def _create_chunk_document(self, original_doc: Document,
                           content: str, chunk_id: int) -> Document:
        """åˆ›å»ºæ–‡æ¡£å—"""
        return Document(
            id=f"{original_doc.id}_chunk_{chunk_id}",
            content=content,
            title=f"{original_doc.title} (Chunk {chunk_id + 1})",
            url=original_doc.url,
            metadata={
                **original_doc.metadata,
                'original_doc_id': original_doc.id,
                'chunk_id': chunk_id,
                'chunk_size': len(content)
            },
            source=original_doc.source,
            chunk_id=str(chunk_id)
        )
```

## ğŸ”„ æ–‡æ¡£é‡æ’åºå™¨

### è¯­ä¹‰é‡æ’åºå®ç°
```python
import numpy as np
from sentence_transformers import SentenceTransformer, util
from typing import List, Tuple

class SemanticReranker(BaseRAGComponent):
    """è¯­ä¹‰é‡æ’åºå™¨"""

    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                 top_k: int = 20):
        super().__init__("semantic_reranker")
        self.model_name = model_name
        self.top_k = top_k
        self.model = None
        self.is_initialized = False

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–é‡æ’åºå™¨"""
        try:
            self.model = SentenceTransformer(self.model_name)
            self.is_initialized = True
            self.logger.info(f"Loaded reranker model: {self.model_name}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to initialize reranker: {e}")
            return False

    async def rerank(self, query: str, documents: List[Document]) -> List[Document]:
        """é‡æ’åºæ–‡æ¡£"""
        if not self.is_initialized:
            self.logger.warning("Reranker not initialized")
            return documents

        if not documents:
            return documents

        try:
            # é™åˆ¶å€™é€‰æ–‡æ¡£æ•°é‡
            candidates = documents[:self.top_k]

            # ç¼–ç æŸ¥è¯¢å’Œæ–‡æ¡£
            query_embedding = self.model.encode([query], convert_to_tensor=True)
            doc_embeddings = self.model.encode(
                [doc.content for doc in candidates],
                convert_to_tensor=True
            )

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            cosine_scores = util.cos_sim(query_embedding, doc_embeddings)[0]

            # è®¡ç®—é‡æ’åºåˆ†æ•°
            # ç»“åˆåŸå§‹æ£€ç´¢åˆ†æ•°å’Œè¯­ä¹‰åˆ†æ•°
            reranked_docs = []
            for i, (doc, cosine_score) in enumerate(zip(candidates, cosine_scores)):
                # åŠ æƒç»„åˆåˆ†æ•°
                semantic_weight = 0.7
                original_weight = 0.3

                # å½’ä¸€åŒ–åŸå§‹åˆ†æ•°
                original_score = max(0.0, min(1.0, doc.score))

                combined_score = (
                    semantic_weight * cosine_score.item() +
                    original_weight * original_score
                )

                # æ›´æ–°æ–‡æ¡£åˆ†æ•°
                doc.score = combined_score
                reranked_docs.append(doc)

            # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
            reranked_docs.sort(key=lambda x: x.score, reverse=True)

            self.logger.info(f"Reranked {len(documents)} documents")
            return reranked_docs

        except Exception as e:
            self.logger.error(f"Reranking failed: {e}")
            return documents

    def get_stats(self) -> Dict[str, Any]:
        """è·å–é‡æ’åºå™¨ç»Ÿè®¡"""
        return {
            'model_name': self.model_name,
            'is_initialized': self.is_initialized,
            'top_k': self.top_k
        }

class CrossEncoderReranker(BaseRAGComponent):
    """è·¨ç¼–ç å™¨é‡æ’åºå™¨"""

    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        super().__init__("cross_encoder_reranker")
        self.model_name = model_name
        self.model = None
        self.is_initialized = False

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–è·¨ç¼–ç å™¨é‡æ’åºå™¨"""
        try:
            from sentence_transformers import CrossEncoder
            self.model = CrossEncoder(self.model_name)
            self.is_initialized = True
            self.logger.info(f"Loaded cross-encoder model: {self.model_name}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to initialize cross-encoder reranker: {e}")
            return False

    async def rerank(self, query: str, documents: List[Document]) -> List[Document]:
        """ä½¿ç”¨è·¨ç¼–ç å™¨é‡æ’åº"""
        if not self.is_initialized:
            self.logger.warning("Cross-encoder reranker not initialized")
            return documents

        if not documents:
            return documents

        try:
            # æ„å»ºæŸ¥è¯¢-æ–‡æ¡£å¯¹
            query_doc_pairs = [
                (query, doc.content)
                for doc in documents[:50]  # é™åˆ¶å€™é€‰æ•°é‡
            ]

            # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
            scores = self.model.predict(query_doc_pairs)

            # æ›´æ–°æ–‡æ¡£åˆ†æ•°
            for i, (doc, score) in enumerate(zip(documents, scores)):
                doc.score = float(score)

            # æŒ‰åˆ†æ•°æ’åº
            documents.sort(key=lambda x: x.score, reverse=True)

            self.logger.info(f"Cross-encoder reranked {len(documents)} documents")
            return documents

        except Exception as e:
            self.logger.error(f"Cross-encoder reranking failed: {e}")
            return documents

    def get_stats(self) -> Dict[str, Any]:
        """è·å–è·¨ç¼–ç å™¨é‡æ’åºå™¨ç»Ÿè®¡"""
        return {
            'model_name': self.model_name,
            'is_initialized': self.is_initialized,
            'model_type': 'cross_encoder'
        }
```

## ğŸ¤– å›ç­”ç”Ÿæˆå™¨

### ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”Ÿæˆå™¨
```python
class ContextAwareGenerator(BaseRAGComponent):
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç”Ÿæˆå™¨"""

    def __init__(self, llm_client, template: str = None):
        super().__init__("context_aware_generator")
        self.llm_client = llm_client
        self.template = template or self._get_default_template()

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        try:
            # æµ‹è¯•LLMè¿æ¥
            test_response = await self.llm_client.generate("æµ‹è¯•")
            self.is_initialized = True
            self.logger.info("Generator initialized successfully")
            return True
        except Exception as e:
            self.logger.error(f"Failed to initialize generator: {e}")
            return False

    async def generate(self, query: str, context: str) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”"""
        if not self.is_initialized:
            return {
                'answer': "ç”Ÿæˆå™¨æœªåˆå§‹åŒ–",
                'success': False
            }

        try:
            # æ„å»ºæç¤º
            prompt = self._build_prompt(query, context)

            # ç”Ÿæˆå›ç­”
            start_time = time.time()
            response = await self.llm_client.generate(prompt)
            generation_time = time.time() - start_time

            # è§£æå“åº”
            answer = self._parse_response(response)

            return {
                'answer': answer,
                'success': True,
                'generation_time': generation_time,
                'prompt_length': len(prompt),
                'context_length': len(context)
            }

        except Exception as e:
            self.logger.error(f"Generation failed: {e}")
            return {
                'answer': f"ç”Ÿæˆå¤±è´¥: {str(e)}",
                'success': False,
                'error': str(e)
            }

    def _build_prompt(self, query: str, context: str) -> str:
        """æ„å»ºæç¤º"""
        return self.template.format(
            query=query,
            context=context
        )

    def _parse_response(self, response: str) -> str:
        """è§£æå“åº”"""
        # ç®€åŒ–çš„å“åº”è§£æ
        if response.startswith("å›ç­”ï¼š"):
            return response[3:].strip()
        elif "æ— æ³•å›ç­”" in response:
            return "æŠ±æ­‰ï¼ŒåŸºäºæä¾›çš„ä¿¡æ¯æˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
        else:
            return response.strip()

    def _get_default_template(self) -> str:
        """è·å–é»˜è®¤æ¨¡æ¿"""
        return """è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆå™¨ç»Ÿè®¡"""
        return {
            'template_length': len(self.template),
            'is_initialized': hasattr(self, 'is_initialized') and self.is_initialized
        }

class StreamingGenerator(ContextAwareGenerator):
    """æµå¼ç”Ÿæˆå™¨"""

    async def generate_stream(self, query: str, context: str):
        """æµå¼ç”Ÿæˆå›ç­”"""
        if not self.is_initialized:
            yield "ç”Ÿæˆå™¨æœªåˆå§‹åŒ–"
            return

        try:
            # æ„å»ºæç¤º
            prompt = self._build_prompt(query, context)

            # æµå¼ç”Ÿæˆ
            async for chunk in self.llm_client.generate_stream(prompt):
                yield chunk

        except Exception as e:
            yield f"ç”Ÿæˆå¤±è´¥: {str(e)}"
```

## ğŸ“Š RAGç³»ç»Ÿæ•´åˆ

### å®Œæ•´çš„RAGå®ç°
```python
async def create_rag_system():
    """åˆ›å»ºå®Œæ•´çš„RAGç³»ç»Ÿ"""

    # é…ç½®
    config = {
        'retriever': {
            'type': 'hybrid',
            'model_name': 'sentence-transformers/all-MiniLM-L6-v2',
            'dimension': 384
        },
        'indexer': {
            'type': 'faiss',
            'index_path': 'rag_index.faiss',
            'documents_path': 'rag_documents.pkl'
        },
        'reranker': {
            'type': 'semantic',
            'model_name': 'sentence-transformers/all-MiniLM-L6-v2'
        },
        'generator': {
            'type': 'context_aware',
            'template': None  # ä½¿ç”¨é»˜è®¤æ¨¡æ¿
        }
    }

    # åˆ›å»ºRAGç³»ç»Ÿ
    rag_system = RAGSystem(config)

    # åˆå§‹åŒ–ç³»ç»Ÿ
    if await rag_system.initialize():
        print("RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
    else:
        print("RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        return None

    return rag_system

async def add_sample_documents(rag_system):
    """æ·»åŠ ç¤ºä¾‹æ–‡æ¡£"""
    documents = [
        Document(
            id="doc1",
            title="äººå·¥æ™ºèƒ½åŸºç¡€",
            content="äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚AIåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªå­é¢†åŸŸã€‚",
            url="https://example.com/ai-basics",
            metadata={'category': 'technology', 'difficulty': 'beginner'}
        ),
        Document(
            id="doc2",
            title="æœºå™¨å­¦ä¹ ç®—æ³•",
            content="æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚å¸¸è§ç®—æ³•æœ‰çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€æ”¯æŒå‘é‡æœºã€ç¥ç»ç½‘ç»œç­‰ã€‚",
            url="https://example.com/ml-algorithms",
            metadata={'category': 'technology', 'difficulty': 'intermediate'}
        ),
        Document(
            id="doc3",
            title="æ·±åº¦å­¦ä¹ åŸç†",
            content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚å¸¸è§çš„æ·±åº¦å­¦ä¹ æ¶æ„åŒ…æ‹¬CNNã€RNNã€Transformerç­‰ã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚",
            url="https://example.com/deep-learning",
            metadata={'category': 'technology', 'difficulty': 'advanced'}
        )
    ]

    added_count = await rag_system.add_documents(documents)
    print(f"æˆåŠŸæ·»åŠ  {added_count} ä¸ªæ–‡æ¡£åˆ°RAGç³»ç»Ÿ")
    return added_count

async def test_rag_queries(rag_system):
    """æµ‹è¯•RAGæŸ¥è¯¢"""
    test_queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç®—æ³•ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•å¼€å§‹å­¦ä¹ AIï¼Ÿ"
    ]

    for query in test_queries:
        print(f"\né—®é¢˜: {query}")
        print("-" * 50)

        result = await rag_system.query(query, top_k=3, rerank=True)

        if result['success']:
            retrieval = result['retrieval']
            generation = result['generation']

            print(f"æ£€ç´¢åˆ° {len(retrieval.documents)} ä¸ªç›¸å…³æ–‡æ¡£:")
            for i, doc in enumerate(retrieval.documents, 1):
                print(f"{i}. {doc.title} (åˆ†æ•°: {doc.score:.4f})")
                print(f"   å†…å®¹: {doc.content[:100]}...")
                print(f"   æ¥æº: {doc.url}")

            print(f"\nç”Ÿæˆçš„å›ç­”:")
            print(generation['answer'])
            print(f"ç”Ÿæˆæ—¶é—´: {generation.get('generation_time', 0):.2f}ç§’")
        else:
            print(f"æŸ¥è¯¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        print("=" * 60)

async def rag_system_demo():
    """RAGç³»ç»Ÿæ¼”ç¤º"""
    print("ğŸ” RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)

    # åˆ›å»ºRAGç³»ç»Ÿ
    rag_system = await create_rag_system()
    if not rag_system:
        return

    # æ·»åŠ ç¤ºä¾‹æ–‡æ¡£
    await add_sample_documents(rag_system)

    # æµ‹è¯•æŸ¥è¯¢
    await test_rag_queries(rag_system)

    # æ˜¾ç¤ºç³»ç»Ÿç»Ÿè®¡
    stats = rag_system.get_stats()
    print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡:")
    print(f"æ€»æŸ¥è¯¢æ•°: {stats['performance_metrics']['total_queries']}")
    print(f"å¹³å‡æ£€ç´¢æ—¶é—´: {stats['performance_metrics']['avg_retrieval_time']:.4f}ç§’")

    for component_name, component_stats in stats['component_stats'].items():
        print(f"{component_name}: {component_stats}")

# è¿è¡Œæ¼”ç¤º
# asyncio.run(rag_system_demo())
```

## ğŸ“ æ€»ç»“

RAGç³»ç»Ÿæ˜¯å¢å¼ºAgentçŸ¥è¯†èƒ½åŠ›çš„é‡è¦æŠ€æœ¯ï¼Œæœ¬æ–‡æ¡£ä»‹ç»äº†ä»æ£€ç´¢åˆ°ç”Ÿæˆçš„å®Œæ•´å®ç°ã€‚

### ğŸ¯ å…³é”®è¦ç‚¹
- **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„ç»„ä»¶åˆ†ç¦»å’Œæ¥å£å®šä¹‰
- **å¤šç­–ç•¥æ£€ç´¢**: å‘é‡ã€å…³é”®è¯ã€æ··åˆæ£€ç´¢æ–¹æ³•
- **æ™ºèƒ½é‡æ’åº**: è¯­ä¹‰å’Œè·¨ç¼–ç å™¨é‡æ’åºæŠ€æœ¯
- **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**: åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡çš„ç”Ÿæˆ
- **æ€§èƒ½ä¼˜åŒ–**: ç´¢å¼•ä¼˜åŒ–å’Œç¼“å­˜æœºåˆ¶

### ğŸš€ å®ç°ç‰¹è‰²
- **æ··åˆæ£€ç´¢**: ç»“åˆå¤šç§æ£€ç´¢ç­–ç•¥æé«˜å¬å›ç‡
- **åŠ¨æ€ç´¢å¼•**: æ”¯æŒæ–‡æ¡£çš„åŠ¨æ€æ·»åŠ å’Œç´¢å¼•æ›´æ–°
- **å®æ—¶é‡æ’åº**: ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œç²¾æ’åº
- **æµå¼ç”Ÿæˆ**: æ”¯æŒå¤§æ®µæ–‡æœ¬çš„æµå¼ç”Ÿæˆ
- **æ€§èƒ½ç›‘æ§**: å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œç›‘æ§

### ğŸ”„ ä¸‹ä¸€æ­¥
- å­¦ä¹ [ä¸Šä¸‹æ–‡ç®¡ç†](05-ä¸Šä¸‹æ–‡ç®¡ç†.md)
- æŒæ¡[å†³ç­–è§„åˆ’](06-å†³ç­–è§„åˆ’.md)
- æ¢ç´¢[å¤šæ¨¡æ€å­¦ä¹ ](../multimodal/01-åŸºç¡€æ¦‚å¿µ.md)
- äº†è§£[æ¨¡å‹éƒ¨ç½²](../deployment/01-æ¨¡å‹é‡åŒ–.md)

# ğŸ¤– æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ

## ğŸ“š æ¦‚è¿°

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚æœ¬æ–‡æ¡£æ¶µç›–æœºå™¨å­¦ä¹ çš„åŸºç¡€æ¦‚å¿µå’Œæ ¸å¿ƒæŠ€æœ¯ã€‚

## ğŸ¯ å­¦ä¹ ç±»å‹

### ç›‘ç£å­¦ä¹  (Supervised Learning)

ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å…³ç³»ã€‚

#### åˆ†ç±»ä»»åŠ¡
```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
import xgboost as xgb
import lightgbm as lgb

# å¤šåˆ†ç±»é—®é¢˜å¤„ç†
from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# ä¸å¹³è¡¡æ•°æ®å¤„ç†
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler
from imblearn.ensemble import BalancedRandomForestClassifier

# ç¤ºä¾‹ï¼šåˆ›å»ºåˆ†ç±»å™¨
classifiers = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(probability=True, random_state=42),
    'LogisticRegression': LogisticRegression(random_state=42),
    'XGBoost': xgb.XGBClassifier(random_state=42),
    'LightGBM': lgb.LGBMClassifier(random_state=42)
}

# å¤šåˆ†ç±»ç­–ç•¥
ovo_classifier = OneVsOneClassifier(LogisticRegression(random_state=42))
orest_classifier = OneVsRestClassifier(LogisticRegression(random_state=42))

# å¤„ç†ä¸å¹³è¡¡æ•°æ®
smote = SMOTE(random_state=42)
rus = RandomUnderSampler(random_state=42)
```

#### å›å½’ä»»åŠ¡
```python
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor

# æ­£åˆ™åŒ–æŠ€æœ¯è¯´æ˜
"""
L1æ­£åˆ™åŒ– (Lasso):
- ç‰¹å¾é€‰æ‹©ï¼šå¯ä»¥å°†ä¸é‡è¦ç‰¹å¾çš„ç³»æ•°å‹ç¼©ä¸º0
- é€‚ç”¨äºé«˜ç»´ç¨€ç–æ•°æ®
- å¯èƒ½äº§ç”Ÿç¨€ç–æ¨¡å‹

L2æ­£åˆ™åŒ– (Ridge):
- é˜²æ­¢è¿‡æ‹Ÿåˆï¼šçº¦æŸæƒé‡çš„å¤§å°
- é€‚ç”¨äºæ‰€æœ‰ç‰¹å¾éƒ½ç›¸å…³çš„æƒ…å†µ
- äº§ç”Ÿå¯†é›†æ¨¡å‹

ElasticNet:
- L1 + L2ç»„åˆ
- åŒæ—¶å…·æœ‰ç‰¹å¾é€‰æ‹©å’Œé˜²è¿‡æ‹Ÿåˆçš„ä¼˜ç‚¹
- éœ€è¦è°ƒèŠ‚ä¸¤ä¸ªæ­£åˆ™åŒ–å‚æ•°
"""

# ç¤ºä¾‹ï¼šåˆ›å»ºå›å½’å™¨
regressors = {
    'LinearRegression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0, random_state=42),
    'Lasso': Lasso(alpha=1.0, random_state=42),
    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),
    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'SVR': SVR(kernel='rbf'),
    'DecisionTree': DecisionTreeRegressor(random_state=42),
    'MLP': MLPRegressor(random_state=42, max_iter=1000)
}
```

#### åºåˆ—æ ‡æ³¨
```python
# ä½¿ç”¨CRFè¿›è¡Œåºåˆ—æ ‡æ³¨
import sklearn_crfsuite
from sklearn_crfsuite import metrics

class CRFTagger:
    def __init__(self):
        self.model = sklearn_crfsuite.CRF(
            algorithm='lbfgs',
            c1=0.1,
            c2=0.1,
            max_iterations=100,
            all_possible_transitions=True
        )

    def train(self, X, y):
        self.model.fit(X, y)

    def predict(self, X):
        return self.model.predict(X)

    def evaluate(self, X, y):
        y_pred = self.predict(X)
        return metrics.flat_f1_score(y, y_pred, average='weighted')

# BiLSTM-CRFç”¨äºNLPåºåˆ—æ ‡æ³¨
import tensorflow as tf
from tensorflow.keras import layers

class BiLSTMCRF:
    def __init__(self, vocab_size, tag_count, embedding_dim=100, hidden_dim=128):
        self.vocab_size = vocab_size
        self.tag_count = tag_count
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        self.model = self._build_model()

    def _build_model(self):
        model = tf.keras.Sequential([
            layers.Embedding(self.vocab_size, self.embedding_dim, mask_zero=True),
            layers.Bidirectional(layers.LSTM(self.hidden_dim, return_sequences=True)),
            layers.TimeDistributed(layers.Dense(self.tag_count))
        ])

        return model

    def train(self, X, y, epochs=10):
        # è¿™é‡Œéœ€è¦å®ç°CRFå±‚æˆ–ä½¿ç”¨TensorFlow Addons
        # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨softmaxåˆ†ç±»
        self.model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        return self.model.fit(X, y, epochs=epochs, validation_split=0.2)
```

### æ— ç›‘ç£å­¦ä¹  (Unsupervised Learning)

æ— ç›‘ç£å­¦ä¹ ä»æœªæ ‡è®°çš„æ•°æ®ä¸­å‘ç°æ¨¡å¼ã€‚

#### èšç±»ç®—æ³•
```python
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
import hdbscan
import numpy as np

# èšç±»è¯„ä¼°æŒ‡æ ‡
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

class ClusteringAnalyzer:
    def __init__(self, X):
        self.X = X
        self.labels = None
        self.cluster_centers = None

    def kmeans_clustering(self, n_clusters=3, random_state=42):
        """Kå‡å€¼èšç±»"""
        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
        self.labels = kmeans.fit_predict(self.X)
        self.cluster_centers = kmeans.cluster_centers_
        return self.labels

    def dbscan_clustering(self, eps=0.5, min_samples=5):
        """DBSCANå¯†åº¦èšç±»"""
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        self.labels = dbscan.fit_predict(self.X)
        return self.labels

    def hierarchical_clustering(self, n_clusters=3):
        """å±‚æ¬¡èšç±»"""
        agg = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
        self.labels = agg.fit_predict(self.X)
        return self.labels

    def spectral_clustering(self, n_clusters=3, random_state=42):
        """è°±èšç±»"""
        spectral = SpectralClustering(n_clusters=n_clusters, random_state=random_state)
        self.labels = spectral.fit_predict(self.X)
        return self.labels

    def gmm_clustering(self, n_components=3, random_state=42):
        """é«˜æ–¯æ··åˆæ¨¡å‹"""
        gmm = GaussianMixture(n_components=n_components, random_state=random_state)
        self.labels = gmm.fit_predict(self.X)
        return self.labels

    def evaluate_clustering(self):
        """è¯„ä¼°èšç±»è´¨é‡"""
        if self.labels is None:
            return {}

        # è¿‡æ»¤å™ªå£°ç‚¹ï¼ˆDBSCANä¸­çš„-1æ ‡ç­¾ï¼‰
        valid_mask = self.labels != -1
        valid_labels = self.labels[valid_mask]
        valid_X = self.X[valid_mask]

        if len(np.unique(valid_labels)) < 2:
            return {"error": "éœ€è¦è‡³å°‘2ä¸ªæœ‰æ•ˆèšç±»"}

        return {
            'silhouette_score': silhouette_score(valid_X, valid_labels),
            'calinski_harabasz_score': calinski_harabasz_score(valid_X, valid_labels),
            'davies_bouldin_score': davies_bouldin_score(valid_X, valid_labels),
            'n_clusters': len(np.unique(valid_labels))
        }

# å¯†åº¦èšç±»ç¤ºä¾‹
dbscan = DBSCAN(eps=0.5, min_samples=5)
# å±‚æ¬¡èšç±»ç¤ºä¾‹
agg = AgglomerativeClustering(n_clusters=3, linkage='ward')

# HDBSCAN - é€‚åˆå¯å˜å¯†åº¦èšç±»
hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5)
```

#### é™ç»´æŠ€æœ¯
```python
from sklearn.decomposition import PCA, ICA, NMF, TruncatedSVD
from sklearn.manifold import TSNE, MDS, Isomap
import umap
import matplotlib.pyplot as plt

class DimensionalityReducer:
    def __init__(self, X):
        self.X = X
        self.X_reduced = None
        self.reducer = None

    def pca_reduction(self, n_components=2, variance_ratio=0.95):
        """ä¸»æˆåˆ†åˆ†æ"""
        if isinstance(variance_ratio, float):
            pca = PCA(n_components=variance_ratio)
            print(f"ä¿ç•™ {variance_ratio*100}% æ–¹å·®éœ€è¦çš„ç»„ä»¶æ•°: {pca.n_components_}")
        else:
            pca = PCA(n_components=n_components)

        self.X_reduced = pca.fit_transform(self.X)
        self.reducer = pca
        return self.X_reduced

    def tsne_visualization(self, n_components=2, perplexity=30, random_state=42):
        """t-SNEç”¨äºå¯è§†åŒ–"""
        tsne = TSNE(n_components=n_components, perplexity=perplexity,
                    random_state=random_state)
        self.X_reduced = tsne.fit_transform(self.X)
        return self.X_reduced

    def umap_visualization(self, n_components=2, n_neighbors=15, min_dist=0.1):
        """UMAPç”¨äºé«˜ç»´æ•°æ®å¯è§†åŒ–"""
        reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist,
                             n_components=n_components)
        self.X_reduced = reducer.fit_transform(self.X)
        return self.X_reduced

    def plot_reduction(self, title="é™ç»´ç»“æœ"):
        """å¯è§†åŒ–é™ç»´ç»“æœ"""
        if self.X_reduced is None:
            print("è¯·å…ˆè¿›è¡Œé™ç»´æ“ä½œ")
            return

        plt.figure(figsize=(10, 8))
        if self.X_reduced.shape[1] >= 2:
            plt.scatter(self.X_reduced[:, 0], self.X_reduced[:, 1], alpha=0.6)
            plt.xlabel(f'ç»„ä»¶ 1 ({self.reducer.explained_variance_ratio_[0]:.2%})')
            plt.ylabel(f'ç»„ä»¶ 2 ({self.reducer.explained_variance_ratio_[1]:.2%})')
        else:
            plt.scatter(range(len(self.X_reduced)), self.X_reduced[:, 0], alpha=0.6)
            plt.xlabel('æ ·æœ¬ç´¢å¼•')
            plt.ylabel(f'ç»„ä»¶ 1 ({self.reducer.explained_variance_ratio_[0]:.2%})')

        plt.title(title)
        plt.grid(True, alpha=0.3)
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
# pca = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
# tsne = TSNE(n_components=2, perplexity=30)
# reducer = umap.UMAP(n_neighbors=15, min_dist=0.1)
```

#### å¼‚å¸¸æ£€æµ‹
```python
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.covariance import EllipticEnvelope
import pyod  # Python Outlier Detection

class AnomalyDetector:
    def __init__(self, contamination=0.1, random_state=42):
        self.contamination = contamination
        self.random_state = random_state
        self.detectors = {}
        self._initialize_detectors()

    def _initialize_detectors(self):
        """åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨"""
        self.detectors = {
            'isolation_forest': IsolationForest(
                contamination=self.contamination,
                random_state=self.random_state
            ),
            'one_class_svm': OneClassSVM(nu=self.contamination),
            'elliptic_envelope': EllipticEnvelope(contamination=self.contamination)
        }

    def fit_detect(self, X, method='isolation_forest'):
        """æ‹Ÿåˆæ£€æµ‹å™¨å¹¶æ£€æµ‹å¼‚å¸¸"""
        if method not in self.detectors:
            raise ValueError(f"ä¸æ”¯æŒçš„æ£€æµ‹æ–¹æ³•: {method}")

        detector = self.detectors[method]
        predictions = detector.fit_predict(X)

        # -1è¡¨ç¤ºå¼‚å¸¸ï¼Œ1è¡¨ç¤ºæ­£å¸¸
        anomalies = X[predictions == -1]
        normal_points = X[predictions == 1]

        return {
            'anomaly_indices': np.where(predictions == -1)[0],
            'anomalies': anomalies,
            'normal_points': normal_points,
            'anomaly_ratio': len(anomalies) / len(X)
        }

    def ensemble_detection(self, X):
        """é›†æˆå¤šä¸ªæ£€æµ‹å™¨çš„ç»“æœ"""
        results = {}
        for method in self.detectors.keys():
            results[method] = self.fit_detect(X, method)

        # æŠ•ç¥¨æœºåˆ¶ï¼šå¦‚æœå¤šä¸ªæ£€æµ‹å™¨éƒ½è®¤ä¸ºæ˜¯å¼‚å¸¸ï¼Œåˆ™æ ‡è®°ä¸ºå¼‚å¸¸
        anomaly_votes = np.zeros(len(X))
        for result in results.values():
            anomaly_votes[result['anomaly_indices']] += 1

        consensus_anomalies = np.where(anomaly_votes >= 2)[0]

        return {
            'individual_results': results,
            'consensus_anomalies': consensus_anomalies,
            'consensus_ratio': len(consensus_anomalies) / len(X)
        }

# ä½¿ç”¨ç¤ºä¾‹
# iso_forest = IsolationForest(contamination=0.1, random_state=42)
# oc_svm = OneClassSVM(nu=0.1)
```

#### åŠç›‘ç£å­¦ä¹ 
```python
# æ ‡ç­¾ä¼ æ’­
from sklearn.semi_supervised import LabelPropagation, LabelSpreading

# è‡ªè®­ç»ƒ
from sklearn.base import BaseEstimator, ClassifierMixin

class SelfTrainingModel(BaseEstimator, ClassifierMixin):
    def __init__(self, base_classifier, threshold=0.8, max_iter=10):
        self.base_classifier = base_classifier
        self.threshold = threshold
        self.max_iter = max_iter

    def fit(self, X, y):
        """è‡ªè®­ç»ƒæ‹Ÿåˆ"""
        labeled_mask = y != -1
        unlabeled_mask = ~labeled_mask

        for iteration in range(self.max_iter):
            # åœ¨å·²æ ‡è®°æ•°æ®ä¸Šè®­ç»ƒ
            self.base_classifier.fit(X[labeled_mask], y[labeled_mask])

            # é¢„æµ‹æœªæ ‡è®°æ•°æ®
            if not np.any(unlabeled_mask):
                break

            proba = self.base_classifier.predict_proba(X[unlabeled_mask])
            max_proba = np.max(proba, axis=1)
            confident_mask = max_proba >= self.threshold

            # é€‰æ‹©é«˜ç½®ä¿¡åº¦é¢„æµ‹åŠ å…¥è®­ç»ƒé›†
            if np.any(confident_mask):
                new_labeled_indices = np.where(unlabeled_mask)[0][confident_mask]
                new_labels = np.argmax(proba[confident_mask], axis=1)

                y[new_labeled_indices] = new_labels
                labeled_mask[new_labeled_indices] = True
                unlabeled_mask[new_labeled_indices] = False

        self.classes_ = np.unique(y[labeled_mask])
        return self

    def predict(self, X):
        return self.base_classifier.predict(X)

    def predict_proba(self, X):
        return self.base_classifier.predict_proba(X)

# å›¾ç¥ç»ç½‘ç»œç”¨äºåŠç›‘ç£å­¦ä¹ 
import torch_geometric
from torch_geometric.nn import GCNConv

class GraphSemiSupervised:
    def __init__(self, input_dim, hidden_dim, output_dim):
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, x, edge_index):
        x = torch.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)
```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### åˆ†ç±»æŒ‡æ ‡
```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, precision_recall_curve,
    confusion_matrix, classification_report,
    log_loss, matthews_corrcoef, cohen_kappa_score
)

class ClassificationMetrics:
    def __init__(self, y_true, y_pred, y_pred_proba=None):
        self.y_true = y_true
        self.y_pred = y_pred
        self.y_pred_proba = y_pred_proba

    def basic_metrics(self):
        """åŸºç¡€åˆ†ç±»æŒ‡æ ‡"""
        return {
            'accuracy': accuracy_score(self.y_true, self.y_pred),
            'precision': precision_score(self.y_true, self.y_pred, average='weighted'),
            'recall': recall_score(self.y_true, self.y_pred, average='weighted'),
            'f1_score': f1_score(self.y_true, self.y_pred, average='weighted')
        }

    def multiclass_metrics(self):
        """å¤šåˆ†ç±»æŒ‡æ ‡"""
        metrics = {}

        # å®å¹³å‡
        metrics.update({
            'precision_macro': precision_score(self.y_true, self.y_pred, average='macro'),
            'recall_macro': recall_score(self.y_true, self.y_pred, average='macro'),
            'f1_macro': f1_score(self.y_true, self.y_pred, average='macro')
        })

        # å¾®å¹³å‡
        metrics.update({
            'precision_micro': precision_score(self.y_true, self.y_pred, average='micro'),
            'recall_micro': recall_score(self.y_true, self.y_pred, average='micro'),
            'f1_micro': f1_score(self.y_true, self.y_pred, average='micro')
        })

        # åŠ æƒå¹³å‡
        metrics.update({
            'precision_weighted': precision_score(self.y_true, self.y_pred, average='weighted'),
            'recall_weighted': recall_score(self.y_true, self.y_pred, average='weighted'),
            'f1_weighted': f1_score(self.y_true, self.y_pred, average='weighted')
        })

        return metrics

    def auc_metrics(self):
        """AUCç›¸å…³æŒ‡æ ‡"""
        if self.y_pred_proba is None:
            return {"error": "éœ€è¦æ¦‚ç‡é¢„æµ‹"}

        # äºŒåˆ†ç±»AUC
        if len(np.unique(self.y_true)) == 2:
            return {
                'roc_auc': roc_auc_score(self.y_true, self.y_pred_proba[:, 1])
            }

        # å¤šåˆ†ç±»AUC
        from sklearn.preprocessing import label_binarize
        classes = np.unique(self.y_true)
        y_true_bin = label_binarize(self.y_true, classes=classes)

        return {
            'roc_auc_macro': roc_auc_score(y_true_bin, self.y_pred_proba, average='macro', multi_class='ovr'),
            'roc_auc_weighted': roc_auc_score(y_true_bin, self.y_pred_proba, average='weighted', multi_class='ovr')
        }

    def detailed_report(self):
        """è¯¦ç»†åˆ†ç±»æŠ¥å‘Š"""
        return classification_report(self.y_true, self.y_pred)
```

### å›å½’æŒ‡æ ‡
```python
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    mean_absolute_percentage_error, explained_variance_score
)

class RegressionMetrics:
    def __init__(self, y_true, y_pred):
        self.y_true = y_true
        self.y_pred = y_pred

    def basic_metrics(self):
        """åŸºç¡€å›å½’æŒ‡æ ‡"""
        return {
            'mse': mean_squared_error(self.y_true, self.y_pred),
            'rmse': np.sqrt(mean_squared_error(self.y_true, self.y_pred)),
            'mae': mean_absolute_error(self.y_true, self.y_pred),
            'r2_score': r2_score(self.y_true, self.y_pred),
            'explained_variance': explained_variance_score(self.y_true, self.y_pred)
        }

    def percentage_metrics(self):
        """ç™¾åˆ†æ¯”æŒ‡æ ‡"""
        try:
            mape = mean_absolute_percentage_error(self.y_true, self.y_pred)
        except ValueError:
            mape = float('inf')

        return {
            'mape': mape,
            'è‡ªå®šä¹‰_smape': self.symmetric_mape()
        }

    def symmetric_mape(self):
        """å¯¹ç§°å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®"""
        return np.mean(2 * np.abs(self.y_pred - self.y_true) /
                        (np.abs(self.y_true) + np.abs(self.y_pred)))
```

## ğŸ”§ å·¥å…·å’ŒæŠ€å·§

### äº¤å‰éªŒè¯ç­–ç•¥
```python
from sklearn.model_selection import (
    cross_val_score, cross_validate, KFold, StratifiedKFold,
    TimeSeriesSplit, GroupKFold, LeaveOneOut, ShuffleSplit
)

class CrossValidationStrategies:
    def __init__(self, model, X, y):
        self.model = model
        self.X = X
        self.y = y

    def stratified_kfold(self, n_splits=5, shuffle=True, random_state=42):
        """åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯"""
        skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)
        scores = cross_val_score(self.model, self.X, self.y, cv=skf)
        return {
            'scores': scores,
            'mean_score': scores.mean(),
            'std_score': scores.std()
        }

    def time_series_split(self, n_splits=5):
        """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯"""
        tscv = TimeSeriesSplit(n_splits=n_splits)
        scores = cross_val_score(self.model, self.X, self.y, cv=tscv)
        return {
            'scores': scores,
            'mean_score': scores.mean(),
            'std_score': scores.std()
        }

    def group_kfold(self, groups, n_splits=5):
        """åˆ†ç»„äº¤å‰éªŒè¯"""
        gkf = GroupKFold(n_splits=n_splits)
        scores = cross_val_score(self.model, self.X, self.y, cv=gkf, groups=groups)
        return {
            'scores': scores,
            'mean_score': scores.mean(),
            'std_score': scores.std()
        }

    def nested_cv(self, param_grid, outer_cv=5, inner_cv=3):
        """åµŒå¥—äº¤å‰éªŒè¯"""
        from sklearn.model_selection import GridSearchCV

        grid_search = GridSearchCV(
            self.model, param_grid, cv=inner_cv, scoring='accuracy'
        )

        outer_scores = cross_val_score(grid_search, self.X, self.y, cv=outer_cv)

        return {
            'outer_scores': outer_scores,
            'mean_score': outer_scores.mean(),
            'std_score': outer_scores.std(),
            'best_params': grid_search.best_params_
        }
```

---

## ğŸ“ æ€»ç»“

æœºå™¨å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µæ¶µç›–äº†ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹ ç­‰å¤šä¸ªæ–¹é¢ã€‚ç†è§£è¿™äº›åŸºç¡€æ¦‚å¿µå¯¹äºæ·±å…¥å­¦ä¹ AIæŠ€æœ¯è‡³å…³é‡è¦ã€‚

### ğŸ¯ å…³é”®è¦ç‚¹
- **ç›‘ç£å­¦ä¹ **é€‚ç”¨äºæœ‰æ ‡è®°æ•°æ®çš„é¢„æµ‹ä»»åŠ¡
- **æ— ç›‘ç£å­¦ä¹ **é€‚ç”¨äºæ•°æ®ä¸­å‘ç°éšè—æ¨¡å¼
- **è¯„ä¼°æŒ‡æ ‡**éœ€è¦æ ¹æ®ä»»åŠ¡ç±»å‹åˆç†é€‰æ‹©
- **äº¤å‰éªŒè¯**æ˜¯é¿å…è¿‡æ‹Ÿåˆçš„é‡è¦æŠ€æœ¯
- **ç‰¹å¾å·¥ç¨‹**å¯¹æ¨¡å‹æ€§èƒ½å½±å“å·¨å¤§

### ğŸš€ ä¸‹ä¸€æ­¥
- æ·±å…¥å­¦ä¹ [ç¥ç»ç½‘ç»œåŸºç¡€](../deep-learning/01-ç¥ç»ç½‘ç»œåŸºç¡€.md)
- äº†è§£[ç‰¹å¾å·¥ç¨‹è¿›é˜¶](02-ç‰¹å¾å·¥ç¨‹.md)
- æŒæ¡[æ¨¡å‹è¯„ä¼°ä¸é€‰æ‹©](05-æ¨¡å‹è¯„ä¼°.md)

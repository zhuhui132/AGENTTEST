# ğŸ–¼ï¸ å¤šæ¨¡æ€å­¦ä¹ åŸºç¡€æ¦‚å¿µ

## ğŸ“š æ¦‚è¿°

å¤šæ¨¡æ€å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºå¤„ç†å’Œèåˆæ¥è‡ªå¤šä¸ªæ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰ï¼‰çš„ä¿¡æ¯ã€‚æœ¬æ–‡æ¡£ä»‹ç»å¤šæ¨¡æ€å­¦ä¹ çš„åŸºç¡€æ¦‚å¿µã€æ ¸å¿ƒæŠ€æœ¯å’Œåº”ç”¨åœºæ™¯ã€‚

## ğŸ¯ å¤šæ¨¡æ€å­¦ä¹ æ ¸å¿ƒ

### åŸºæœ¬æ¦‚å¿µ
```python
from typing import List, Dict, Any, Union, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
import torch
import torch.nn as nn
from abc import ABC, abstractmethod

class ModalityType(Enum):
    """æ¨¡æ€ç±»å‹"""
    TEXT = "text"
    IMAGE = "image"
    AUDIO = "audio"
    VIDEO = "video"
    TABULAR = "tabular"
    GRAPH = "graph"
    POINT_CLOUD = "point_cloud"
    TIME_SERIES = "time_series"

@dataclass
class ModalityData:
    """æ¨¡æ€æ•°æ®å¯¹è±¡"""
    data: Any
    modality: ModalityType
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: float = field(default_factory=lambda: time.time())
    quality_score: float = 1.0

@dataclass
class MultiModalExample:
    """å¤šæ¨¡æ€ç¤ºä¾‹"""
    modalities: Dict[ModalityType, ModalityData]
    label: Optional[Any] = None
    task_type: str = "classification"
    relationships: Dict[str, List[str]] = field(default_factory=dict)

class MultiModalProcessor(ABC):
    """å¤šæ¨¡æ€å¤„ç†å™¨åŸºç¡€ç±»"""

    def __init__(self, modality_types: List[ModalityType]):
        self.modality_types = modality_types
        self.modality_processors = {}
        self.fusion_strategy = None

    @abstractmethod
    def extract_features(self, modal_data: ModalityData) -> torch.Tensor:
        """æå–å•ä¸ªæ¨¡æ€ç‰¹å¾"""
        pass

    @abstractmethod
    def fuse_features(self, features: Dict[ModalityType, torch.Tensor]) -> torch.Tensor:
        """èåˆå¤šæ¨¡æ€ç‰¹å¾"""
        pass

    def process_example(self, example: MultiModalExample) -> Tuple[torch.Tensor, Any]:
        """å¤„ç†å¤šæ¨¡æ€ç¤ºä¾‹"""
        # æå–å„æ¨¡æ€ç‰¹å¾
        features = {}
        for modality, data in example.modalities.items():
            if modality in self.modality_types:
                features[modality] = self.extract_features(data)

        # èåˆç‰¹å¾
        if len(features) > 1:
            fused_features = self.fuse_features(features)
        elif len(features) == 1:
            fused_features = list(features.values())[0]
        else:
            raise ValueError("æ²¡æœ‰å¯å¤„ç†çš„æ¨¡æ€æ•°æ®")

        return fused_features, example.label

class MultiModalDataset:
    """å¤šæ¨¡æ€æ•°æ®é›†"""

    def __init__(self, examples: List[MultiModalExample]):
        self.examples = examples
        self.modality_types = self._get_available_modalities()

    def _get_available_modalities(self) -> List[ModalityType]:
        """è·å–å¯ç”¨çš„æ¨¡æ€ç±»å‹"""
        modalities = set()
        for example in self.examples:
            modalities.update(example.modalities.keys())
        return list(modalities)

    def filter_by_modalities(self, required_modalities: List[ModalityType]) -> 'MultiModalDataset':
        """æŒ‰æ¨¡æ€è¿‡æ»¤æ•°æ®é›†"""
        filtered_examples = []

        for example in self.examples:
            if all(mod in example.modalities for mod in required_modalities):
                filtered_examples.append(example)

        return MultiModalDataset(filtered_examples)

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        modality_counts = {modality: 0 for modality in ModalityType}
        task_types = {}

        for example in self.examples:
            for modality in example.modalities:
                modality_counts[modality] += 1

            task_type = example.task_type
            task_types[task_type] = task_types.get(task_type, 0) + 1

        return {
            'total_examples': len(self.examples),
            'modality_distribution': dict(modality_counts),
            'task_type_distribution': task_types,
            'available_modalities': self.modality_types
        }

# ä½¿ç”¨ç¤ºä¾‹
def multimodal_basics_demo():
    """å¤šæ¨¡æ€åŸºç¡€æ¼”ç¤º"""

    # åˆ›å»ºå¤šæ¨¡æ€ç¤ºä¾‹
    examples = [
        MultiModalExample(
            modalities={
                ModalityType.TEXT: ModalityData("A cat sitting on a table", ModalityType.TEXT),
                ModalityType.IMAGE: ModalityData(np.random.rand(3, 224, 224), ModalityType.IMAGE)
            },
            label="cat",
            task_type="classification"
        ),
        MultiModalExample(
            modalities={
                ModalityType.TEXT: ModalityData("A dog playing in the park", ModalityType.TEXT),
                ModalityType.AUDIO: ModalityData(np.random.rand(1, 16000), ModalityType.AUDIO)
            },
            label="dog",
            task_type="classification"
        )
    ]

    # åˆ›å»ºæ•°æ®é›†
    dataset = MultiModalDataset(examples)

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = dataset.get_statistics()
    print(f"æ•°æ®é›†ç»Ÿè®¡: {stats}")

    # æŒ‰æ¨¡æ€è¿‡æ»¤
    text_image_dataset = dataset.filter_by_modalities([ModalityType.TEXT, ModalityType.IMAGE])
    print(f"æ–‡æœ¬-å›¾åƒæ•°æ®é›†å¤§å°: {len(text_image_dataset.examples)}")

# multimodal_basics_demo()
```

## ğŸ” ç‰¹å¾æå–

### å„æ¨¡æ€ç‰¹å¾æå–å™¨
```python
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import torchvision.models as models
from PIL import Image
import librosa
import cv2

class TextFeatureExtractor(nn.Module):
    """æ–‡æœ¬ç‰¹å¾æå–å™¨"""

    def __init__(self, model_name: str = "bert-base-uncased",
                 hidden_size: int = 768, max_length: int = 512):
        super().__init__()
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.bert = AutoModel.from_pretrained(model_name)
        self.hidden_size = hidden_size
        self.max_length = max_length

        # æŠ•å½±å±‚
        self.projection = nn.Linear(self.bert.config.hidden_size, hidden_size)

    def forward(self, text: str) -> torch.Tensor:
        """æå–æ–‡æœ¬ç‰¹å¾"""
        # ç¼–ç æ–‡æœ¬
        inputs = self.tokenizer(
            text,
            max_length=self.max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )

        # è·å–BERTè¾“å‡º
        with torch.no_grad():
            outputs = self.bert(**inputs)

        # ä½¿ç”¨[CLS]å‘é‡
        cls_features = outputs.last_hidden_state[:, 0, :]
        features = self.projection(cls_features)

        return F.normalize(features, p=2, dim=-1)

class ImageFeatureExtractor(nn.Module):
    """å›¾åƒç‰¹å¾æå–å™¨"""

    def __init__(self, model_name: str = "resnet50",
                 feature_size: int = 512):
        super().__init__()

        if model_name == "resnet50":
            self.backbone = models.resnet50(pretrained=True)
        elif model_name == "resnet101":
            self.backbone = models.resnet101(pretrained=True)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")

        # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])

        # æŠ•å½±å±‚
        self.feature_size = feature_size
        self.projection = nn.Linear(2048, feature_size)

    def forward(self, image: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:
        """æå–å›¾åƒç‰¹å¾"""
        if isinstance(image, np.ndarray):
            # ä»numpyè½¬æ¢
            if image.ndim == 3:
                image = torch.from_numpy(image).permute(2, 0, 1).float()
            else:
                image = torch.from_numpy(image).float()

            # æ·»åŠ batchç»´åº¦
            image = image.unsqueeze(0)

        # è°ƒæ•´å°ºå¯¸
        if image.shape[-2:] != (224, 224):
            image = F.interpolate(image, size=(224, 224), mode='bilinear', align_corners=False)

        # æ ‡å‡†åŒ–
        image = image / 255.0
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        image = (image - mean) / std

        # æå–ç‰¹å¾
        with torch.no_grad():
            features = self.backbone(image)

        # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        features = self.projection(features.squeeze())

        return F.normalize(features, p=2, dim=-1)

class AudioFeatureExtractor(nn.Module):
    """éŸ³é¢‘ç‰¹å¾æå–å™¨"""

    def __init__(self, sample_rate: int = 16000, n_mels: int = 128,
                 feature_size: int = 512):
        super().__init__()
        self.sample_rate = sample_rate
        self.n_mels = n_mels
        self.feature_size = feature_size

        # MFCCç‰¹å¾æå–å™¨
        self.mfcc_extractor = self._build_mfcc_extractor()

        # æŠ•å½±å±‚
        self.projection = nn.Linear(n_mels, feature_size)

    def _build_mfcc_extractor(self) -> nn.Module:
        """æ„å»ºMFCCç‰¹å¾æå–å™¨"""
        # ä½¿ç”¨ç®€å•çš„å·ç§¯ç½‘ç»œæå–éŸ³é¢‘ç‰¹å¾
        return nn.Sequential(
            nn.Conv1d(1, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(self.n_mels)
        )

    def forward(self, audio: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:
        """æå–éŸ³é¢‘ç‰¹å¾"""
        if isinstance(audio, np.ndarray):
            # ä»numpyè½¬æ¢
            if len(audio.shape) == 1:
                audio = torch.from_numpy(audio).float()
            else:
                audio = torch.from_numpy(audio).float()

            # æ·»åŠ batchå’Œchannelç»´åº¦
            audio = audio.unsqueeze(0).unsqueeze(0)
        elif audio.dim() == 1:
            audio = audio.unsqueeze(0).unsqueeze(0)
        elif audio.dim() == 2:
            audio = audio.unsqueeze(0)

        # ç¡®ä¿æ˜¯å•å£°é“
        if audio.shape[1] > 1:
            audio = torch.mean(audio, dim=1, keepdim=True)

        # ç‰¹å¾æå–
        with torch.no_grad():
            features = self.mfcc_extractor(audio)

        # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        features = self.projection(features.squeeze())

        return F.normalize(features, p=2, dim=-1)

class VideoFeatureExtractor(nn.Module):
    """è§†é¢‘ç‰¹å¾æå–å™¨"""

    def __init__(self, model_name: str = "resnet50",
                 feature_size: int = 512,
                 num_frames: int = 8):
        super().__init__()
        self.num_frames = num_frames
        self.feature_size = feature_size

        # å›¾åƒç‰¹å¾æå–å™¨
        self.image_extractor = ImageFeatureExtractor(model_name, 2048)

        # æ—¶åºå»ºæ¨¡
        self.temporal_model = nn.LSTM(2048, 512, batch_first=True)

        # æŠ•å½±å±‚
        self.projection = nn.Linear(512, feature_size)

    def forward(self, video: Union[np.ndarray, torch.Tensor]) -> torch.Tensor:
        """æå–è§†é¢‘ç‰¹å¾"""
        if isinstance(video, np.ndarray):
            # ä»numpyè½¬æ¢ (T, H, W, C) æˆ– (T, C, H, W)
            if len(video.shape) == 4:
                if video.shape[-1] <= 4:  # (T, H, W, C)
                    video = torch.from_numpy(video).float()
                else:  # (T, C, H, W)
                    video = torch.from_numpy(video).float().permute(0, 2, 3, 1)
            else:
                video = torch.from_numpy(video).float()

            # æ·»åŠ batchç»´åº¦
            video = video.unsqueeze(0)
        elif video.dim() == 3:  # (C, T, H, W)
            video = video.unsqueeze(0)

        # ç¡®ä¿ç»´åº¦æ­£ç¡®
        if video.dim() == 4:  # (B, C, T, H, W)
            video = video.permute(0, 2, 1, 3, 4)  # (B, T, C, H, W)
        elif video.dim() == 5:  # (B, T, C, H, W)
            pass

        # é‡‡æ ·å¸§
        total_frames = video.shape[1]
        if total_frames > self.num_frames:
            indices = torch.linspace(0, total_frames - 1, self.num_frames).long()
            video = video[:, indices]

        # æå–æ¯å¸§ç‰¹å¾
        frame_features = []
        for t in range(video.shape[1]):
            frame = video[:, t]  # (B, C, H, W)
            frame_feature = self.image_extractor(frame)  # (B, 2048)
            frame_features.append(frame_feature)

        # å †å ä¸ºæ—¶åºåºåˆ—
        frame_features = torch.stack(frame_features, dim=1)  # (B, T, 2048)

        # æ—¶åºå»ºæ¨¡
        with torch.no_grad():
            temporal_features, _ = self.temporal_model(frame_features)

        # ä½¿ç”¨æœ€åæ—¶é—´æ­¥
        features = temporal_features[:, -1, :]

        # æŠ•å½±åˆ°ç›®æ ‡ç»´åº¦
        features = self.projection(features)

        return F.normalize(features, p=2, dim=-1)

# ç‰¹å¾æå–å™¨å·¥å‚
class FeatureExtractorFactory:
    """ç‰¹å¾æå–å™¨å·¥å‚"""

    @staticmethod
    def create_extractor(modality: ModalityType, **kwargs) -> nn.Module:
        """åˆ›å»ºç‰¹å¾æå–å™¨"""
        if modality == ModalityType.TEXT:
            return TextFeatureExtractor(**kwargs)
        elif modality == ModalityType.IMAGE:
            return ImageFeatureExtractor(**kwargs)
        elif modality == ModalityType.AUDIO:
            return AudioFeatureExtractor(**kwargs)
        elif modality == ModalityType.VIDEO:
            return VideoFeatureExtractor(**kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡æ€ç±»å‹: {modality}")

# ä½¿ç”¨ç¤ºä¾‹
async def feature_extraction_demo():
    """ç‰¹å¾æå–æ¼”ç¤º"""

    # åˆ›å»ºç‰¹å¾æå–å™¨
    text_extractor = FeatureExtractorFactory.create_extractor(
        ModalityType.TEXT, model_name="bert-base-uncased"
    )

    image_extractor = FeatureExtractorFactory.create_extractor(
        ModalityType.IMAGE, model_name="resnet50"
    )

    audio_extractor = FeatureExtractorFactory.create_extractor(
        ModalityType.AUDIO, sample_rate=16000
    )

    # æå–ç‰¹å¾
    text_feature = text_extractor("This is a sample text.")
    print(f"æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {text_feature.shape}")

    # æ¨¡æ‹Ÿå›¾åƒæ•°æ®
    dummy_image = np.random.rand(224, 224, 3)
    image_feature = image_extractor(dummy_image)
    print(f"å›¾åƒç‰¹å¾å½¢çŠ¶: {image_feature.shape}")

    # æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®
    dummy_audio = np.random.rand(16000)  # 1ç§’éŸ³é¢‘
    audio_feature = audio_extractor(dummy_audio)
    print(f"éŸ³é¢‘ç‰¹å¾å½¢çŠ¶: {audio_feature.shape}")

# import asyncio
# asyncio.run(feature_extraction_demo())
```

## ğŸ”€ ç‰¹å¾èåˆ

### èåˆç­–ç•¥
```python
from abc import ABC, abstractmethod
from enum import Enum

class FusionStrategy(Enum):
    """èåˆç­–ç•¥"""
    CONCAT = "concatenation"
    ATTENTION = "attention"
    CROSS_MODAL = "cross_modal"
    GATED = "gated"
    BILINEAR = "bilinear"
    COMPOUND = "compound"

class MultiModalFusion(nn.Module):
    """å¤šæ¨¡æ€èåˆæ¨¡å—"""

    def __init__(self, modality_configs: Dict[ModalityType, int],
                 fusion_strategy: FusionStrategy = FusionStrategy.CONCAT,
                 output_dim: int = 512):
        super().__init__()

        self.modality_configs = modality_configs
        self.fusion_strategy = fusion_strategy
        self.output_dim = output_dim

        # æ„å»ºèåˆæ¨¡å—
        if fusion_strategy == FusionStrategy.CONCAT:
            self.fusion = ConcatenationFusion(modality_configs, output_dim)
        elif fusion_strategy == FusionStrategy.ATTENTION:
            self.fusion = AttentionFusion(modality_configs, output_dim)
        elif fusion_strategy == FusionStrategy.CROSS_MODAL:
            self.fusion = CrossModalFusion(modality_configs, output_dim)
        elif fusion_strategy == FusionStrategy.GATED:
            self.fusion = GatedFusion(modality_configs, output_dim)
        elif fusion_strategy == FusionStrategy.BILINEAR:
            self.fusion = BilinearFusion(modality_configs, output_dim)
        elif fusion_strategy == FusionStrategy.COMPOUND:
            self.fusion = CompoundFusion(modality_configs, output_dim)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆç­–ç•¥: {fusion_strategy}")

    def forward(self, features: Dict[ModalityType, torch.Tensor]) -> torch.Tensor:
        """èåˆå¤šæ¨¡æ€ç‰¹å¾"""
        return self.fusion(features)

class ConcatenationFusion(nn.Module):
    """æ‹¼æ¥èåˆ"""

    def __init__(self, modality_configs: Dict[ModalityType, int], output_dim: int):
        super().__init__()

        # è®¡ç®—æ€»ç‰¹å¾ç»´åº¦
        total_dim = sum(modality_configs.values())

        self.projection = nn.Linear(total_dim, output_dim)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(0.1)

    def forward(self, features: Dict[ModalityType, torch.Tensor]) -> torch.Tensor:
        """æ‹¼æ¥èåˆ"""
        # å°†ç‰¹å¾æ‹¼æ¥
        concat_features = torch.cat(list(features.values()), dim=-1)

        # æŠ•å½±å’Œæ¿€æ´»
        output = self.projection(concat_features)
        output = self.activation(output)
        output = self.dropout(output)

        return output

class AttentionFusion(nn.Module):
    """æ³¨æ„åŠ›èåˆ"""

    def __init__(self, modality_configs: Dict[ModalityType, int], output_dim: int):
        super().__init__()

        self.modality_types = list(modality_configs.keys())
        self.num_modalities = len(self.modality_types)
        self.output_dim = output_dim

        # æ³¨æ„åŠ›å‚æ•°
        self.attention_weights = nn.Parameter(torch.ones(self.num_modalities))

        # æŠ•å½±å±‚
        self.projections = nn.ModuleDict({
            modality.value: nn.Linear(dim, output_dim)
            for modality, dim in modality_configs.items()
        })

        # èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(output_dim, output_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

    def forward(self, features: Dict[ModalityType, torch.Tensor]) -> torch.Tensor:
        """æ³¨æ„åŠ›èåˆ"""
        # æŠ•å½±å„æ¨¡æ€ç‰¹å¾
        projected_features = []
        for modality in self.modality_types:
            if modality in features:
                feature = self.projections[modality.value](features[modality])
                projected_features.append(feature)

        if not projected_features:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾è¿›è¡Œèåˆ")

        # å †å ç‰¹å¾
        stacked_features = torch.stack(projected_features, dim=1)  # (B, M, D)

        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attention_weights = F.softmax(self.attention_weights, dim=0)
        weighted_features = (stacked_features * attention_weights.unsqueeze(0).unsqueeze(-1))

        # æ± å’Œ
        fused_features = torch.sum(weighted_features, dim=1)

        # æœ€ç»ˆèåˆ
        output = self.fusion_layer(fused_features)

        return output

class CrossModalFusion(nn.Module):
    """è·¨æ¨¡æ€èåˆ"""

    def __init__(self, modality_configs: Dict[ModalityType, int], output_dim: int):
        super().__init__()

        self.modality_types = list(modality_configs.keys())
        self.output_dim = output_dim

        # è·¨æ¨¡æ€äº¤äº’å±‚
        self.cross_modal_layers = nn.ModuleDict()

        # ä¸ºæ¯å¯¹æ¨¡æ€åˆ›å»ºäº¤äº’
        for i, mod1 in enumerate(self.modality_types):
            for j, mod2 in enumerate(self.modality_types):
                if i != j:
                    self.cross_modal_layers[f"{mod1.value}_to_{mod2.value}"] = nn.Linear(
                        modality_configs[mod1], modality_configs[mod2]
                    )

        # èåˆå±‚
        total_input_dim = sum(modality_configs.values())
        self.fusion_layer = nn.Sequential(
            nn.Linear(total_input_dim, output_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

    def forward(self, features: Dict[ModalityType, torch.Tensor]) -> torch.Tensor:
        """è·¨æ¨¡æ€èåˆ"""
        enhanced_features = {}

        # è·¨æ¨¡æ€ä¿¡æ¯ä¼ é€’
        for mod1 in self.modality_types:
            if mod1 not in features:
                continue

            # åŸå§‹ç‰¹å¾
            mod1_features = [features[mod1]]

            # æ¥è‡ªå…¶ä»–æ¨¡æ€çš„ä¿¡æ¯
            for mod2 in self.modality_types:
                if mod1 != mod2 and mod2 in features:
                    layer_name = f"{mod2.value}_to_{mod1.value}"
                    if layer_name in self.cross_modal_layers:
                        transferred = self.cross_modal_layers[layer_name](features[mod2])
                        mod1_features.append(transferred)

            # èåˆå¢å¼ºç‰¹å¾
            enhanced_features[mod1] = torch.mean(torch.stack(mod1_features), dim=0)

        # æœ€ç»ˆèåˆ
        all_features = torch.cat(list(enhanced_features.values()), dim=-1)
        output = self.fusion_layer(all_features)

        return output

class GatedFusion(nn.Module):
    """é—¨æ§èåˆ"""

    def __init__(self, modality_configs: Dict[ModalityType, int], output_dim: int):
        super().__init__()

        self.modality_types = list(modality_configs.keys())
        self.output_dim = output_dim

        # é—¨æ§ç½‘ç»œ
        total_input_dim = sum(modality_configs.values())
        self.gate_network = nn.Sequential(
            nn.Linear(total_input_dim, len(self.modality_types)),
            nn.Sigmoid()
        )

        # ç‰¹å¾æŠ•å½±
        self.projections = nn.ModuleDict({
            modality.value: nn.Linear(dim, output_dim)
            for modality, dim in modality_configs.items()
        })

    def forward(self, features: Dict[ModalityType, torch.Tensor]) -> torch.Tensor:
        """é—¨æ§èåˆ"""
        # æŠ•å½±å„æ¨¡æ€ç‰¹å¾
        projected_features = []
        for modality in self.modality_types:
            if modality in features:
                feature = self.projections[modality.value](features[modality])
                projected_features.append(feature)

        if not projected_features:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾è¿›è¡Œèåˆ")

        # è®¡ç®—é—¨æ§æƒé‡
        all_features = torch.cat(list(features.values()), dim=-1)
        gates = self.gate_network(all_features)

        # åº”ç”¨é—¨æ§
        weighted_features = []
        for i, feature in enumerate(projected_features):
            if i < gates.size(1):
                weighted_feature = feature * gates[:, i:i+1]
                weighted_features.append(weighted_feature)

        # æ±‚å’Œä½œä¸ºæœ€ç»ˆç‰¹å¾
        if weighted_features:
            output = torch.sum(torch.stack(weighted_features, dim=0), dim=0)
        else:
            output = torch.zeros_like(projected_features[0])

        return output

class BilinearFusion(nn.Module):
    """åŒçº¿æ€§èåˆ"""

    def __init__(self, modality_configs: Dict[ModalityType, int], output_dim: int):
        super().__init__()

        self.modality_types = list(modality_configs.keys())
        self.output_dim = output_dim

        # åŒçº¿æ€§çŸ©é˜µ
        self.bilinear_matrices = nn.ModuleDict()

        # ä¸ºæ¯å¯¹æ¨¡æ€åˆ›å»ºåŒçº¿æ€§å˜æ¢
        for i, mod1 in enumerate(self.modality_types):
            for j, mod2 in enumerate(self.modality_types):
                if i <= j:  # é¿å…é‡å¤
                    self.bilinear_matrices[f"{mod1.value}_{mod2.value}"] = nn.Bilinear(
                        modality_configs[mod1], modality_configs[mod2], bias=False
                    )

        # è¾“å‡ºæŠ•å½±
        total_bilinear_features = len(self.bilinear_matrices)
        self.output_projection = nn.Sequential(
            nn.Linear(total_bilinear_features, output_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

    def forward(self, features: Dict[ModalityType, torch.Tensor]) -> torch.Tensor:
        """åŒçº¿æ€§èåˆ"""
        bilinear_features = []

        # è®¡ç®—æ‰€æœ‰åŒçº¿æ€§äº¤äº’
        for i, mod1 in enumerate(self.modality_types):
            if mod1 not in features:
                continue

            for j, mod2 in enumerate(self.modality_types):
                if i <= j and mod2 in features:
                    matrix_name = f"{mod1.value}_{mod2.value}"
                    if matrix_name in self.bilinear_matrices:
                        bilinear = self.bilinear_matrices[matrix_name]
                        if i == j:
                            # è‡ªäº¤äº’
                            feature = bilinear(features[mod1])
                        else:
                            # äº¤å‰äº¤äº’
                            feature = bilinear(features[mod1], features[mod2])
                        bilinear_features.append(feature.squeeze())

        if not bilinear_features:
            raise ValueError("æ²¡æœ‰å¯è®¡ç®—çš„åŒçº¿æ€§ç‰¹å¾")

        # æ‹¼æ¥å¹¶æŠ•å½±
        concatenated = torch.cat(bilinear_features, dim=-1)
        output = self.output_projection(concatenated)

        return output

class CompoundFusion(nn.Module):
    """å¤åˆèåˆ"""

    def __init__(self, modality_configs: Dict[ModalityType, int], output_dim: int):
        super().__init__()

        # å¤šç§èåˆç­–ç•¥çš„ç»„åˆ
        self.concat_fusion = ConcatenationFusion(modality_configs, output_dim // 2)
        self.attention_fusion = AttentionFusion(modality_configs, output_dim // 2)

        # æœ€ç»ˆèåˆ
        self.final_fusion = nn.Sequential(
            nn.Linear(output_dim, output_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

    def forward(self, features: Dict[ModalityType, torch.Tensor]) -> torch.Tensor:
        """å¤åˆèåˆ"""
        # å¤šç§èåˆç»“æœ
        concat_result = self.concat_fusion(features)
        attention_result = self.attention_fusion(features)

        # ç»„åˆç»“æœ
        combined = torch.cat([concat_result, attention_result], dim=-1)

        # æœ€ç»ˆèåˆ
        output = self.final_fusion(combined)

        return output

# èåˆå™¨å·¥å‚
class FusionFactory:
    """èåˆå™¨å·¥å‚"""

    @staticmethod
    def create_fusion(modality_configs: Dict[ModalityType, int],
                     strategy: FusionStrategy,
                     output_dim: int = 512) -> MultiModalFusion:
        """åˆ›å»ºèåˆå™¨"""
        return MultiModalFusion(modality_configs, strategy, output_dim)

# ä½¿ç”¨ç¤ºä¾‹
async def fusion_demo():
    """èåˆç­–ç•¥æ¼”ç¤º"""

    # æ¨¡æ€é…ç½®
    modality_configs = {
        ModalityType.TEXT: 768,
        ModalityType.IMAGE: 512,
        ModalityType.AUDIO: 128
    }

    # åˆ›å»ºä¸åŒèåˆç­–ç•¥
    concat_fusion = FusionFactory.create_fusion(
        modality_configs, FusionStrategy.CONCAT, 512
    )

    attention_fusion = FusionFactory.create_fusion(
        modality_configs, FusionStrategy.ATTENTION, 512
    )

    gated_fusion = FusionFactory.create_fusion(
        modality_configs, FusionStrategy.GATED, 512
    )

    # æ¨¡æ‹Ÿç‰¹å¾
    features = {
        ModalityType.TEXT: torch.randn(4, 768),
        ModalityType.IMAGE: torch.randn(4, 512),
        ModalityType.AUDIO: torch.randn(4, 128)
    }

    # æµ‹è¯•èåˆ
    concat_result = concat_fusion(features)
    attention_result = attention_fusion(features)
    gated_result = gated_fusion(features)

    print(f"æ‹¼æ¥èåˆç»“æœå½¢çŠ¶: {concat_result.shape}")
    print(f"æ³¨æ„åŠ›èåˆç»“æœå½¢çŠ¶: {attention_result.shape}")
    print(f"é—¨æ§èåˆç»“æœå½¢çŠ¶: {gated_result.shape}")

# import asyncio
# asyncio.run(fusion_demo())
```

## ğŸ‹ï¸ å¤šæ¨¡æ€åˆ†ç±»å™¨

### å®Œæ•´çš„å¤šæ¨¡æ€å­¦ä¹ ç³»ç»Ÿ
```python
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import List, Tuple
import time

class MultiModalClassifier(nn.Module):
    """å¤šæ¨¡æ€åˆ†ç±»å™¨"""

    def __init__(self, modality_configs: Dict[ModalityType, int],
                 num_classes: int,
                 fusion_strategy: FusionStrategy = FusionStrategy.ATTENTION,
                 hidden_dim: int = 512):
        super().__init__()

        self.modality_types = list(modality_configs.keys())
        self.num_classes = num_classes
        self.hidden_dim = hidden_dim

        # ç‰¹å¾æå–å™¨
        self.feature_extractors = nn.ModuleDict({
            modality.value: FeatureExtractorFactory.create_extractor(modality)
            for modality in self.modality_types
        })

        # å¤šæ¨¡æ€èåˆ
        self.fusion = FusionFactory.create_fusion(
            modality_configs, fusion_strategy, hidden_dim
        )

        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, batch_data: Dict[ModalityType, Any]) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # æå–å„æ¨¡æ€ç‰¹å¾
        features = {}
        for modality in self.modality_types:
            if modality in batch_data:
                data = batch_data[modality]
                if hasattr(data, 'data'):
                    # ModalityDataå¯¹è±¡
                    extractor = self.feature_extractors[modality.value]
                    features[modality] = extractor(data.data)
                else:
                    # åŸå§‹æ•°æ®
                    extractor = self.feature_extractors[modality.value]
                    features[modality] = extractor(data)

        # èåˆç‰¹å¾
        fused_features = self.fusion(features)

        # åˆ†ç±»
        logits = self.classifier(fused_features)

        return logits

class MultiModalTrainer:
    """å¤šæ¨¡æ€è®­ç»ƒå™¨"""

    def __init__(self, model: MultiModalClassifier, device: str = "cuda"):
        self.model = model
        self.device = device
        self.model.to(device)

        self.training_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }

    def train_epoch(self, train_loader: DataLoader,
                   optimizer: torch.optim.Optimizer,
                   criterion: nn.Module) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, batch in enumerate(train_loader):
            # å‡†å¤‡æ•°æ®
            data, labels = batch
            labels = labels.to(self.device)

            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            batch_data = {}
            for modality, mod_data in data.items():
                if hasattr(mod_data, 'data'):
                    batch_data[modality] = mod_data.data.to(self.device)
                else:
                    batch_data[modality] = mod_data.to(self.device)

            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            outputs = self.model(batch_data)
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total

        return avg_loss, accuracy

    def validate_epoch(self, val_loader: DataLoader,
                       criterion: nn.Module) -> Tuple[float, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                data, labels = batch
                labels = labels.to(self.device)

                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                batch_data = {}
                for modality, mod_data in data.items():
                    if hasattr(mod_data, 'data'):
                        batch_data[modality] = mod_data.data.to(self.device)
                    else:
                        batch_data[modality] = mod_data.to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = self.model(batch_data)
                loss = criterion(outputs, labels)

                # ç»Ÿè®¡
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        avg_loss = total_loss / len(val_loader)
        accuracy = correct / total

        return avg_loss, accuracy

    def train(self, train_loader: DataLoader, val_loader: DataLoader,
              num_epochs: int = 10, learning_rate: float = 0.001) -> Dict[str, List[float]]:
        """è®­ç»ƒæ¨¡å‹"""

        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()

        print(f"å¼€å§‹è®­ç»ƒï¼Œå…±{num_epochs}ä¸ªepoch")
        print("-" * 50)

        for epoch in range(num_epochs):
            start_time = time.time()

            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader, optimizer, criterion)

            # éªŒè¯
            val_loss, val_acc = self.validate_epoch(val_loader, criterion)

            # è®°å½•å†å²
            self.training_history['train_loss'].append(train_loss)
            self.training_history['train_acc'].append(train_acc)
            self.training_history['val_loss'].append(val_loss)
            self.training_history['val_acc'].append(val_acc)

            epoch_time = time.time() - start_time

            print(f"Epoch {epoch + 1:2d}/{num_epochs} | "
                  f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
                  f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | "
                  f"Time: {epoch_time:.2f}s")

        print("-" * 50)
        print("è®­ç»ƒå®Œæˆï¼")

        return self.training_history

# ä½¿ç”¨ç¤ºä¾‹
async def multimodal_classification_demo():
    """å¤šæ¨¡æ€åˆ†ç±»æ¼”ç¤º"""

    # æ¨¡æ€é…ç½®
    modality_configs = {
        ModalityType.TEXT: 768,
        ModalityType.IMAGE: 512
    }

    # åˆ›å»ºæ¨¡å‹
    model = MultiModalClassifier(
        modality_configs=modality_configs,
        num_classes=10,
        fusion_strategy=FusionStrategy.ATTENTION,
        hidden_dim=512
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiModalTrainer(model, device="cuda" if torch.cuda.is_available() else "cpu")

    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")

    # æ¨¡æ‹Ÿè®­ç»ƒ
    print("\nå¤šæ¨¡æ€åˆ†ç±»å™¨åˆ›å»ºæˆåŠŸï¼")
    print(f"æ”¯æŒçš„æ¨¡æ€: {modality_configs.keys()}")
    print(f"åˆ†ç±»ç±»åˆ«æ•°: 10")
    print(f"èåˆç­–ç•¥: {FusionStrategy.ATTENTION.value}")

# import asyncio
# asyncio.run(multimodal_classification_demo())
```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### å¤šæ¨¡æ€è¯„ä¼°æŒ‡æ ‡
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

class MultiModalMetrics:
    """å¤šæ¨¡æ€è¯„ä¼°æŒ‡æ ‡"""

    def __init__(self, num_classes: int):
        self.num_classes = num_classes
        self.reset_metrics()

    def reset_metrics(self):
        """é‡ç½®æŒ‡æ ‡"""
        self.predictions = []
        self.targets = []
        self.confidence_scores = []

    def update(self, predictions: np.ndarray, targets: np.ndarray,
                confidence_scores: np.ndarray = None):
        """æ›´æ–°æŒ‡æ ‡"""
        self.predictions.extend(predictions)
        self.targets.extend(targets)
        if confidence_scores is not None:
            self.confidence_scores.extend(confidence_scores)

    def calculate_basic_metrics(self) -> Dict[str, float]:
        """è®¡ç®—åŸºç¡€æŒ‡æ ‡"""
        predictions = np.array(self.predictions)
        targets = np.array(self.targets)

        metrics = {
            'accuracy': accuracy_score(targets, predictions),
            'macro_precision': precision_score(targets, predictions, average='macro'),
            'macro_recall': recall_score(targets, predictions, average='macro'),
            'macro_f1': f1_score(targets, predictions, average='macro'),
            'weighted_precision': precision_score(targets, predictions, average='weighted'),
            'weighted_recall': recall_score(targets, predictions, average='weighted'),
            'weighted_f1': f1_score(targets, predictions, average='weighted')
        }

        return metrics

    def calculate_per_class_metrics(self) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æ¯ç±»æŒ‡æ ‡"""
        predictions = np.array(self.predictions)
        targets = np.array(self.targets)

        # è®¡ç®—æ¯ç±»çš„precision, recall, f1
        per_class_metrics = {}

        for class_idx in range(self.num_classes):
            # å°†å¤šç±»é—®é¢˜è½¬ä¸ºäºŒç±»é—®é¢˜
            binary_predictions = (predictions == class_idx).astype(int)
            binary_targets = (targets == class_idx).astype(int)

            per_class_metrics[f'class_{class_idx}'] = {
                'precision': precision_score(binary_targets, binary_predictions, zero_division=0),
                'recall': recall_score(binary_targets, binary_predictions, zero_division=0),
                'f1': f1_score(binary_targets, binary_predictions, zero_division=0),
                'support': int((targets == class_idx).sum())
            }

        return per_class_metrics

    def calculate_confidence_metrics(self) -> Dict[str, float]:
        """è®¡ç®—ç½®ä¿¡åº¦æŒ‡æ ‡"""
        if not self.confidence_scores:
            return {}

        confidence_scores = np.array(self.confidence_scores)
        predictions = np.array(self.predictions)
        targets = np.array(self.targets)

        # è®¡ç®—é¢„æµ‹æ­£ç¡®çš„ç½®ä¿¡åº¦å‡å€¼
        correct_mask = (predictions == targets)
        if correct_mask.any():
            avg_confidence_correct = confidence_scores[correct_mask].mean()
        else:
            avg_confidence_correct = 0.0

        if (~correct_mask).any():
            avg_confidence_incorrect = confidence_scores[~correct_mask].mean()
        else:
            avg_confidence_incorrect = 0.0

        return {
            'avg_confidence_correct': float(avg_confidence_correct),
            'avg_confidence_incorrect': float(avg_confidence_incorrect),
            'confidence_gap': float(avg_confidence_correct - avg_confidence_incorrect)
        }

    def get_confusion_matrix(self) -> np.ndarray:
        """è·å–æ··æ·†çŸ©é˜µ"""
        return confusion_matrix(self.targets, self.predictions)

    def get_classification_report(self) -> str:
        """è·å–åˆ†ç±»æŠ¥å‘Š"""
        return classification_report(self.targets, self.predictions)

    def get_detailed_report(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†æŠ¥å‘Š"""
        return {
            'basic_metrics': self.calculate_basic_metrics(),
            'per_class_metrics': self.calculate_per_class_metrics(),
            'confidence_metrics': self.calculate_confidence_metrics(),
            'confusion_matrix': self.get_confusion_matrix().tolist(),
            'classification_report': self.get_classification_report()
        }

# ä½¿ç”¨ç¤ºä¾‹
def multimodal_metrics_demo():
    """å¤šæ¨¡æ€è¯„ä¼°æ¼”ç¤º"""

    # åˆ›å»ºè¯„ä¼°å™¨
    metrics_calculator = MultiModalMetrics(num_classes=3)

    # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
    predictions = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 1])
    targets = np.array([0, 1, 1, 0, 2, 2, 1, 1, 2, 0])
    confidence_scores = np.array([0.9, 0.8, 0.7, 0.95, 0.6, 0.85, 0.75, 0.88, 0.92, 0.65])

    # æ›´æ–°æŒ‡æ ‡
    metrics_calculator.update(predictions, targets, confidence_scores)

    # è®¡ç®—æŒ‡æ ‡
    basic_metrics = metrics_calculator.calculate_basic_metrics()
    per_class_metrics = metrics_calculator.calculate_per_class_metrics()
    confidence_metrics = metrics_calculator.calculate_confidence_metrics()

    print("åŸºç¡€æŒ‡æ ‡:")
    for metric, value in basic_metrics.items():
        print(f"  {metric}: {value:.4f}")

    print("\nç½®ä¿¡åº¦æŒ‡æ ‡:")
    for metric, value in confidence_metrics.items():
        print(f"  {metric}: {value:.4f}")

    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(metrics_calculator.get_classification_report())

# multimodal_metrics_demo()
```

## ğŸ“ æ€»ç»“

å¤šæ¨¡æ€å­¦ä¹ æ˜¯AIé¢†åŸŸçš„é‡è¦æŠ€æœ¯ï¼Œæœ¬æ–‡æ¡£ä»‹ç»äº†å¤šæ¨¡æ€å­¦ä¹ çš„åŸºç¡€æ¦‚å¿µå’Œå®ç°æ–¹æ³•ã€‚

### ğŸ¯ å…³é”®è¦ç‚¹
- **å¤šæ¨¡æ€æ•°æ®**: æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰å¤šç§æ•°æ®ç±»å‹
- **ç‰¹å¾æå–**: å„æ¨¡æ€çš„ç‰¹å¾æå–æ–¹æ³•
- **èåˆç­–ç•¥**: æ‹¼æ¥ã€æ³¨æ„åŠ›ã€é—¨æ§ã€åŒçº¿æ€§ç­‰èåˆæ–¹æ³•
- **è¯„ä¼°æŒ‡æ ‡**: å¤šæ¨¡æ€å­¦ä¹ çš„ä¸“ç”¨è¯„ä¼°æŒ‡æ ‡

### ğŸš€ å®ç°ç‰¹è‰²
- **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„ç»„ä»¶åˆ†ç¦»å’Œæ¥å£å®šä¹‰
- **å¤šç§èåˆ**: ä¸°å¯Œçš„èåˆç­–ç•¥å®ç°
- **å®Œæ•´pipeline**: ä»æ•°æ®åˆ°è®­ç»ƒçš„å®Œæ•´æµç¨‹
- **å¯æ‰©å±•æ€§**: æ”¯æŒæ–°æ¨¡æ€å’Œèåˆç­–ç•¥çš„æ‰©å±•

### ğŸ”„ ä¸‹ä¸€æ­¥
- å­¦ä¹ [è§†è§‰è¯­è¨€æ¨¡å‹](02-è§†è§‰è¯­è¨€æ¨¡å‹.md)
- äº†è§£[å¤šæ¨¡æ€èåˆ](03-å¤šæ¨¡æ€èåˆ.md)
- æŒæ¡[è·¨æ¨¡æ€æ£€ç´¢](04-è·¨æ¨¡æ€æ£€ç´¢.md)
- æ¢ç´¢[Agentå¤šæ¨¡æ€](../agents/05-ä¸Šä¸‹æ–‡ç®¡ç†.md)

"""
RAGç³»ç»Ÿç»¼åˆæµ‹è¯•
æ·±åº¦æµ‹è¯•æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿçš„å„ç§åœºæ™¯å’Œè¾¹ç•Œæ¡ä»¶
"""

import pytest
import asyncio
import numpy as np
from unittest.mock import Mock, AsyncMock
from src.rag.retriever import Retriever
from src.rag.embeddings import Embeddings
from src.rag.vector_store import VectorStore
from src.rag.knowledge_base import KnowledgeBase
from src.core.types import RetrievalConfig


class TestRAGComprehensive:
    """RAGç³»ç»Ÿç»¼åˆæµ‹è¯•ç±»"""

    @pytest.fixture
    def config(self):
        """RAGé…ç½®fixture"""
        return RetrievalConfig(
            top_k=5,
            threshold=0.7,
            include_metadata=True,
            similarity_metric="cosine",
            max_context_length=4000
        )

    @pytest.fixture
    def mock_embeddings(self):
        """æ¨¡æ‹ŸåµŒå…¥æœåŠ¡"""
        embeddings = Mock()

        def embed_batch(texts):
            return [np.random.randn(512) for _ in texts]

        embeddings.encode = Mock(side_effect=embed_batch)
        embeddings.encode_batch = Mock(side_effect=embed_batch)
        return embeddings

    @pytest.fixture
    def mock_vector_store(self):
        """æ¨¡æ‹Ÿå‘é‡å­˜å‚¨"""
        store = Mock()

        # æ¨¡æ‹Ÿå‘é‡å­˜å‚¨
        vectors = {}
        for i in range(100):
            doc_id = f"doc_{i}"
            vector = np.random.randn(512)
            vectors[doc_id] = vector

        store.vectors = vectors
        store.get_vector = Mock(side_effect=lambda doc_id: vectors.get(doc_id))

        # æ¨¡æ‹Ÿæœç´¢åŠŸèƒ½
        def search(query_vector, top_k=5, threshold=0.7):
            scores = {}
            for doc_id, vector in vectors.items():
                similarity = np.dot(query_vector, vector) / (
                    np.linalg.norm(query_vector) * np.linalg.norm(vector)
                )
                scores[doc_id] = similarity

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            return [
                {"doc_id": doc_id, "score": score}
                for doc_id, score in sorted_docs[:top_k]
                if score >= threshold
            ]

        store.search = Mock(side_effect=search)
        return store

    @pytest.fixture
    def mock_knowledge_base(self):
        """æ¨¡æ‹ŸçŸ¥è¯†åº“"""
        kb = Mock()

        # æ¨¡æ‹Ÿæ–‡æ¡£æ•°æ®
        documents = {}
        for i in range(100):
            doc_id = f"doc_{i}"
            doc_content = f"è¿™æ˜¯æ–‡æ¡£{doc_id}çš„å†…å®¹ï¼ŒåŒ…å«å„ç§ä¸»é¢˜å’Œå…³é”®è¯"
            doc_metadata = {
                "source": f"source_{i%10}",
                "category": f"category_{i%5}",
                "author": f"author_{i%20}",
                "created_date": f"2023-{(i%12)+1:02d}-{(i%28)+1:02d}",
                "tags": [f"tag_{j}" for j in range(3)]
            }
            documents[doc_id] = {
                "id": doc_id,
                "content": doc_content,
                "metadata": doc_metadata
            }

        kb.documents = documents
        kb.get_document = Mock(side_effect=lambda doc_id: documents.get(doc_id))
        kb.get_documents = Mock(side_effect=lambda doc_ids: [
            documents[doc_id] for doc_id in doc_ids if doc_id in documents
        ])

        return kb

    @pytest.fixture
    def rag_system(self, config, mock_embeddings, mock_vector_store, mock_knowledge_base):
        """RAGç³»ç»Ÿfixture"""
        return Retriever(
            embeddings=mock_embeddings,
            vector_store=mock_vector_store,
            knowledge_base=mock_knowledge_base,
            config=config
        )

    @pytest.mark.asyncio
    async def test_hierarchical_retrieval(self, rag_system):
        """æµ‹è¯•åˆ†å±‚æ£€ç´¢"""
        # æ„å»ºåˆ†å±‚æ•°æ®
        queries = [
            "æŸ¥è¯¢æ ¸å¿ƒæ¦‚å¿µ",
            "æŸ¥è¯¢è¯¦ç»†æŠ€æœ¯",
            "æŸ¥è¯¢åŸºç¡€çŸ¥è¯†",
            "æŸ¥è¯¢é«˜çº§åº”ç”¨"
        ]

        # æµ‹è¯•åˆ†å±‚æ£€ç´¢ç­–ç•¥
        for query in queries:
            # è®¾ç½®åˆ†å±‚æ£€ç´¢
            rag_system.enable_hierarchical_retrieval(
                layers=["core", "detailed", "basic", "advanced"]
            )

            results = await rag_system.hierarchical_retrieve(query)

            # éªŒè¯åˆ†å±‚ç»“æœ
            assert len(results) > 0
            assert 'layer_scores' in results
            assert 'final_results' in results

            # éªŒè¯åˆ†å±‚æƒé‡
            total_weight = sum(results['layer_scores'].values())
            assert abs(total_weight - 1.0) < 0.01  # æƒé‡æ€»å’Œåº”è¯¥æ¥è¿‘1

    @pytest.mark.asyncio
    async def test_adaptive_retrieval(self, rag_system):
        """æµ‹è¯•è‡ªé€‚åº”æ£€ç´¢"""
        # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„æŸ¥è¯¢
        adaptive_queries = [
            ("ç®€å•æŸ¥è¯¢", "ç®€å•"),
            ("ä¸­ç­‰å¤æ‚åº¦çš„æŸ¥è¯¢å†…å®¹", "ä¸­ç­‰"),
            ("è¿™æ˜¯ä¸€ä¸ªéå¸¸å¤æ‚çš„æŸ¥è¯¢ï¼ŒåŒ…å«å¤šä¸ªå…³é”®è¯å’Œå¤æ‚çš„æ¦‚å¿µå…³ç³»", "å¤æ‚"),
            ("è¶…å¤æ‚çš„æŸ¥è¯¢ï¼Œæ¶‰åŠæ·±åº¦æŠ€æœ¯ç»†èŠ‚å’Œä¸“ä¸šçŸ¥è¯†", "è¶…å¤æ‚")
        ]

        for query, complexity in adaptive_queries:
            # è®¾ç½®è‡ªé€‚åº”æ£€ç´¢
            rag_system.enable_adaptive_retrieval()

            results = await rag_system.retrieve_with_adaptation(query)

            # éªŒè¯è‡ªé€‚åº”ç»“æœ
            assert 'complexity_score' in results
            assert 'adapted_config' in results
            assert 'retrieval_results' in results

            # éªŒè¯å¤æ‚åº¦è¯„ä¼°
            complexity_score = results['complexity_score']
            if complexity == "ç®€å•":
                assert complexity_score < 0.3
            elif complexity == "å¤æ‚":
                assert complexity_score > 0.7
            elif complexity == "è¶…å¤æ‚":
                assert complexity_score > 0.9

            # éªŒè¯é…ç½®è‡ªé€‚åº”
            adapted_config = results['adapted_config']
            if complexity == "ç®€å•":
                assert adapted_config['top_k'] <= 5
                assert adapted_config['threshold'] >= 0.7
            elif complexity == "å¤æ‚":
                assert adapted_config['top_k'] >= 10
                assert adapted_config['threshold'] <= 0.5

    @pytest.mark.asyncio
    async def test_context_aware_retrieval(self, rag_system):
        """æµ‹è¯•ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€ç´¢"""
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        context_history = [
            {"role": "user", "content": "æˆ‘å¯¹æœºå™¨å­¦ä¹ æ„Ÿå…´è¶£"},
            {"role": "assistant", "content": "æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é¢†åŸŸ"},
            {"role": "user", "content": "ç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ "},
            {"role": "assistant", "content": "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ"}
        ]

        # ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸ¥è¯¢
        contextual_query = "è¯·è¯¦ç»†ä»‹ç»"

        # è®¾ç½®ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€ç´¢
        rag_system.set_context_history(context_history)

        results = await rag_system.context_aware_retrieve(contextual_query)

        # éªŒè¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç»“æœ
        assert len(results) > 0
        assert 'context_scores' in results
        assert 'context_boosted_results' in results

        # éªŒè¯ä¸Šä¸‹æ–‡ç›¸å…³æ€§
        for result in results['context_boosted_results']:
            assert 'original_score' in result
            assert 'context_boost' in result
            assert result['context_boost'] >= 1.0  # ä¸Šä¸‹æ–‡å¢å¼ºåº”è¯¥æå‡åˆ†æ•°

        # æ£€æŸ¥æ˜¯å¦è¯†åˆ«å‡ºæ·±åº¦å­¦ä¹ ç›¸å…³å†…å®¹
        dl_related = any(
            "æ·±åº¦å­¦ä¹ " in result.get('document', {}).get('content', '')
            or "ç¥ç»ç½‘ç»œ" in result.get('document', {}).get('content', '')
            for result in results['context_boosted_results']
        )
        assert dl_related

    @pytest.mark.asyncio
    async def test_multi_modal_retrieval(self, rag_system):
        """æµ‹è¯•å¤šæ¨¡æ€æ£€ç´¢"""
        # æ„å»ºå¤šæ¨¡æ€æŸ¥è¯¢
        multimodal_queries = [
            {
                "text": "å›¾ç‰‡ä¸­çš„åŠ¨ç‰©",
                "image_embedding": np.random.randn(512),  # æ¨¡æ‹Ÿå›¾åƒåµŒå…¥
                "modality": "text+image"
            },
            {
                "text": "éŸ³é¢‘å†…å®¹çš„æè¿°",
                "audio_embedding": np.random.randn(256),  # æ¨¡æ‹ŸéŸ³é¢‘åµŒå…¥
                "modality": "text+audio"
            },
            {
                "text": "è§†é¢‘åˆ†æ",
                "video_embedding": np.random.randn(1024),  # æ¨¡æ‹Ÿè§†é¢‘åµŒå…¥
                "modality": "text+video"
            }
        ]

        for query in multimodal_queries:
            results = await rag_system.multimodal_retrieve(query)

            # éªŒè¯å¤šæ¨¡æ€ç»“æœ
            assert len(results) > 0
            assert 'modality_scores' in results
            assert 'cross_modal_results' in results

            # éªŒè¯è·¨æ¨¡æ€åŒ¹é…
            for result in results['cross_modal_results']:
                assert 'modality_type' in result
                assert 'cross_similarity' in result
                assert result['cross_similarity'] >= 0.0
                assert result['cross_similarity'] <= 1.0

    @pytest.mark.asyncio
    async def test_temporal_retrieval(self, rag_system):
        """æµ‹è¯•æ—¶é—´æ„ŸçŸ¥æ£€ç´¢"""
        # æ„å»ºæ—¶é—´ç›¸å…³çš„æŸ¥è¯¢
        temporal_queries = [
            "æœ€è¿‘çš„å‘ç°",
            "å»å¹´çš„è¶‹åŠ¿",
            "æœªæ¥çš„é¢„æµ‹",
            "å†å²æ•°æ®"
        ]

        # è®¾ç½®æ—¶é—´æ„ŸçŸ¥æ£€ç´¢
        rag_system.enable_temporal_retrieval(
            time_weight=0.7,
            decay_factor=0.9
        )

        for query in temporal_queries:
            # æ¨¡æ‹Ÿå½“å‰æ—¶é—´
            current_time = "2023-11-01"
            results = await rag_system.temporal_retrieve(query, current_time)

            # éªŒè¯æ—¶é—´æ„ŸçŸ¥ç»“æœ
            assert len(results) > 0
            assert 'temporal_scores' in results
            assert 'time_decayed_results' in results

            # éªŒè¯æ—¶é—´æƒé‡åº”ç”¨
            for result in results['time_decayed_results']:
                assert 'base_score' in result
                assert 'temporal_weight' in result
                assert 'final_score' in result

                # æ—¶é—´æƒé‡åº”è¯¥åŸºäºæ—¶é—´å·®è®¡ç®—
                final_score = result['base_score'] * result['temporal_weight']
                assert abs(result['final_score'] - final_score) < 0.001

    @pytest.mark.asyncio
    async def test_domain_specific_retrieval(self, rag_system):
        """æµ‹è¯•é¢†åŸŸç‰¹å®šæ£€ç´¢"""
        # å®šä¹‰ä¸åŒé¢†åŸŸçš„æŸ¥è¯¢
        domain_queries = [
            ("åŒ»å­¦ç ”ç©¶", "medical"),
            ("æ³•å¾‹æ¡æ–‡", "legal"),
            ("é‡‘èåˆ†æ", "financial"),
            ("æŠ€æœ¯æ–‡æ¡£", "technical"),
            ("ç§‘å­¦è®ºæ–‡", "academic")
        ]

        for query, domain in domain_queries:
            # è®¾ç½®é¢†åŸŸç‰¹å®šæ£€ç´¢
            rag_system.enable_domain_specific_retrieval(domain)

            results = await rag_system.domain_retrieve(query)

            # éªŒè¯é¢†åŸŸç‰¹å®šç»“æœ
            assert len(results) > 0
            assert 'domain_relevance' in results
            assert results['domain_relevance'] >= 0.0
            assert results['domain_relevance'] <= 1.0

            # éªŒè¯é¢†åŸŸé€‚é…
            domain_config = rag_system.get_domain_config(domain)
            assert domain_config is not None
            assert domain_config['domain'] == domain

    @pytest.mark.asyncio
    async def test_query_expansion_and_refinement(self, rag_system):
        """æµ‹è¯•æŸ¥è¯¢æ‰©å±•å’Œä¼˜åŒ–"""
        # æµ‹è¯•æŸ¥è¯¢æ‰©å±•
        expansion_queries = [
            "ML",  # ç¼©å†™æ‰©å±•
            "æœºå™¨å­¦ä¹ ",  # åŒä¹‰è¯æ‰©å±•
            "AI",   # ç›¸å…³æ¦‚å¿µæ‰©å±•
            "ç®—æ³•æ¨¡å‹"  # ä¸Šä¸‹æ–‡æ‰©å±•
        ]

        for original_query in expansion_queries:
            results = await rag_system.retrieve_with_expansion(original_query)

            # éªŒè¯æ‰©å±•ç»“æœ
            assert 'original_query' in results
            assert 'expanded_queries' in results
            assert 'expanded_results' in results

            # éªŒè¯æ‰©å±•æŸ¥è¯¢æ•°é‡
            assert len(results['expanded_queries']) >= 1
            assert len(results['expanded_results']) >= len(results['expanded_queries'])

            # éªŒè¯æ‰©å±•è´¨é‡
            for expanded_result in results['expanded_results']:
                assert 'expansion_type' in expanded_result
                assert 'expansion_confidence' in expanded_result
                assert expanded_result['expansion_confidence'] >= 0.0
                assert expanded_result['expansion_confidence'] <= 1.0

    @pytest.mark.asyncio
    async def test_relevance_feedback_loop(self, rag_system):
        """æµ‹è¯•ç›¸å…³æ€§åé¦ˆå¾ªç¯"""
        # åˆå§‹åŒ–åé¦ˆç³»ç»Ÿ
        rag_system.enable_relevance_feedback()

        # ç¬¬ä¸€è½®æ£€ç´¢
        query = "æµ‹è¯•æŸ¥è¯¢"
        results1 = await rag_system.retrieve(query)

        # æ¨¡æ‹Ÿç”¨æˆ·åé¦ˆ
        feedback_data = []
        for i, result in enumerate(results1[:3]):
            if i == 0:
                feedback = {"doc_id": result['doc_id'], "relevance": 5}  # é«˜ç›¸å…³
            elif i == 1:
                feedback = {"doc_id": result['doc_id'], "relevance": 3}  # ä¸­ç­‰ç›¸å…³
            else:
                feedback = {"doc_id": result['doc_id'], "relevance": 1}  # ä½ç›¸å…³
            feedback_data.append(feedback)

            # æäº¤åé¦ˆ
            await rag_system.submit_relevance_feedback(feedback)

        # ç¬¬äºŒè½®æ£€ç´¢ï¼ˆåº”è¯¥è€ƒè™‘åé¦ˆï¼‰
        results2 = await rag_system.retrieve(query)

        # éªŒè¯åé¦ˆå­¦ä¹ æ•ˆæœ
        assert len(results2) > 0

        # éªŒè¯åé¦ˆå½±å“äº†æ’å
        feedback_scores = {item['doc_id']: item['relevance'] for item in feedback_data}

        for result in results2:
            doc_id = result['doc_id']
            if doc_id in feedback_scores:
                # é«˜åé¦ˆçš„æ–‡æ¡£åº”è¯¥æ’åæ›´é«˜
                result_position = results2.index(result)
                high_feedback_docs = [item for item in feedback_data if item['relevance'] >= 4]
                if doc_id in [item['doc_id'] for item in high_feedback_docs]:
                    assert result_position <= len(high_feedback_docs)

    @pytest.mark.asyncio
    async def test_reranking_strategies(self, rag_system):
        """æµ‹è¯•é‡æ’åºç­–ç•¥"""
        # æµ‹è¯•æŸ¥è¯¢
        rerank_query = "å¤æ‚çš„æŠ€æœ¯æŸ¥è¯¢"

        # è·å–åˆå§‹æ£€ç´¢ç»“æœ
        initial_results = await rag_system.basic_retrieve(rerank_query)

        # æµ‹è¯•ä¸åŒé‡æ’åºç­–ç•¥
        reranking_strategies = [
            "cross_encoder",
            "learning_to_rank",
            "neural_rerank",
            "semantic_similarity",
            "diversity_aware"
        ]

        for strategy in reranking_strategies:
            rag_system.set_reranking_strategy(strategy)
            reranked_results = await rag_system.rerank_results(initial_results)

            # éªŒè¯é‡æ’åºç»“æœ
            assert len(reranked_results) > 0
            assert 'rerank_scores' in reranked_results[0]
            assert 'original_scores' in reranked_results[0]

            # éªŒè¯é‡æ’åºæ”¹å˜äº†æ’åº
            original_scores = [r['original_score'] for r in reranked_results]
            rerank_scores = [r['rerank_scores'] for r in reranked_results]

            # æ’åºåº”è¯¥ä¸åŒï¼ˆé™¤éç­–ç•¥ä¸ç”Ÿæ•ˆï¼‰
            if strategy != "no_rerank":
                # é‡æ’åºåçš„åˆ†æ•°åº”è¯¥ç”¨äºæ’åº
                assert all(rerank_scores[i] >= rerank_scores[i+1]
                       for i in range(len(rerank_scores)-1))

    @pytest.mark.asyncio
    async def test_cache_aware_retrieval(self, rag_system):
        """æµ‹è¯•ç¼“å­˜æ„ŸçŸ¥æ£€ç´¢"""
        # å¯ç”¨ç¼“å­˜
        rag_system.enable_caching(cache_size=1000, ttl_seconds=3600)

        # ç›¸åŒæŸ¥è¯¢çš„å¤šæ¬¡æ£€ç´¢
        query = "ç¼“å­˜æµ‹è¯•æŸ¥è¯¢"

        # ç¬¬ä¸€æ¬¡æ£€ç´¢
        start_time = asyncio.get_event_loop().time()
        results1 = await rag_system.retrieve(query)
        first_time = asyncio.get_event_loop().time() - start_time

        # ç¬¬äºŒæ¬¡æ£€ç´¢ï¼ˆåº”è¯¥ä»ç¼“å­˜è·å–ï¼‰
        start_time = asyncio.get_event_loop().time()
        results2 = await rag_system.retrieve(query)
        second_time = asyncio.get_event_loop().time() - start_time

        # éªŒè¯ç¼“å­˜æ•ˆæœ
        assert len(results1) == len(results2)
        for r1, r2 in zip(results1, results2):
            assert r1['doc_id'] == r2['doc_id']
            assert abs(r1['score'] - r2['score']) < 0.001

        # ç¼“å­˜æŸ¥è¯¢åº”è¯¥æ›´å¿«
        assert second_time < first_time * 0.5  # è‡³å°‘å¿«50%

        # éªŒè¯ç¼“å­˜ç»Ÿè®¡
        cache_stats = rag_system.get_cache_stats()
        assert cache_stats['hit_rate'] > 0.0
        assert cache_stats['total_queries'] == 2
        assert cache_stats['cache_hits'] == 1

    @pytest.mark.asyncio
    async def test_explainable_retrieval(self, rag_system):
        """æµ‹è¯•å¯è§£é‡Šçš„æ£€ç´¢"""
        query = "éœ€è¦è§£é‡Šçš„å¤æ‚æŸ¥è¯¢"

        # å¯ç”¨å¯è§£é‡Šæ£€ç´¢
        rag_system.enable_explainability()

        results = await rag_system.explainable_retrieve(query)

        # éªŒè¯å¯è§£é‡Šæ€§
        for result in results:
            assert 'explanation' in result
            assert 'retrieval_factors' in result
            assert 'confidence_score' in result
            assert 'matching_terms' in result

            # éªŒè¯è§£é‡Šè´¨é‡
            explanation = result['explanation']
            assert len(explanation) > 0
            assert any(factor['factor'] in explanation.lower()
                   for factor in result['retrieval_factors'])

    @pytest.mark.asyncio
    async def test_robust_retrieval(self, rag_system):
        """æµ‹è¯•é²æ£’æ€§æ£€ç´¢"""
        # æµ‹è¯•å„ç§æŒ‘æˆ˜æ€§æŸ¥è¯¢
        robust_queries = [
            "",  # ç©ºæŸ¥è¯¢
            "x" * 1000,  # è¶…é•¿æŸ¥è¯¢
            "ã€ç‰¹æ®Šå­—ç¬¦ã€‘@#$%^&*()",  # ç‰¹æ®Šå­—ç¬¦
            "ğŸš€ğŸŒŸâœ¨ğŸ¯",  # è¡¨æƒ…ç¬¦å·
            "ä¸­ æ–‡ æ·· åˆ",  # ä¸­è‹±æ–‡æ··åˆ
            "æŸ¥è¯¢\n\n\næŸ¥è¯¢",  # å¤šè¡Œ
            "   æŸ¥è¯¢   ",  # é¦–å°¾ç©ºæ ¼
        ]

        for query in robust_queries:
            results = await rag_system.robust_retrieve(query)

            # éªŒè¯é²æ£’æ€§å¤„ç†
            if not query:  # ç©ºæŸ¥è¯¢
                assert len(results) == 0
            else:
                # éç©ºæŸ¥è¯¢åº”è¯¥è¿”å›ç»“æœæˆ–ä¼˜é›…é™çº§
                assert isinstance(results, list)

                # éªŒè¯é”™è¯¯å¤„ç†
                if 'error' in results[0]:
                    assert 'error_type' in results[0]
                    assert 'fallback_results' in results[0]

    @pytest.mark.asyncio
    async def test_integrated_rag_pipeline(self, rag_system):
        """æµ‹è¯•é›†æˆçš„RAGæµæ°´çº¿"""
        # æµ‹è¯•å®Œæ•´çš„RAGæµæ°´çº¿
        complex_query = "éœ€è¦æ·±åº¦æ£€ç´¢å’Œç”Ÿæˆçš„å¤æ‚é—®é¢˜"

        # æ‰§è¡Œå®Œæ•´çš„RAGæµæ°´çº¿
        pipeline_result = await rag_system.full_rag_pipeline(complex_query)

        # éªŒè¯æµæ°´çº¿ç»“æœ
        assert 'retrieval_results' in pipeline_result
        assert 'context_construction' in pipeline_result
        assert 'generation_input' in pipeline_result
        assert 'final_response' in pipeline_result
        assert 'pipeline_metadata' in pipeline_result

        # éªŒè¯ä¸Šä¸‹æ–‡æ„å»º
        context = pipeline_result['context_construction']
        assert 'total_context_length' in context
        assert 'used_documents' in context
        assert 'truncation_applied' in context

        # éªŒè¯æµæ°´çº¿æŒ‡æ ‡
        metadata = pipeline_result['pipeline_metadata']
        assert 'retrieval_time' in metadata
        assert 'context_building_time' in metadata
        assert 'generation_time' in metadata
        assert 'total_pipeline_time' in metadata

        # éªŒè¯æ—¶é—´åˆç†æ€§
        total_time = metadata['total_pipeline_time']
        retrieval_time = metadata['retrieval_time']
        context_time = metadata['context_building_time']
        generation_time = metadata['generation_time']

        assert total_time >= retrieval_time + context_time + generation_time
        assert total_time > 0  # åº”è¯¥æœ‰æ—¶é—´æ¶ˆè€—
        assert total_time < 10.0  # åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆ

    def test_rag_system_configuration(self, rag_system):
        """æµ‹è¯•RAGç³»ç»Ÿé…ç½®"""
        # æµ‹è¯•å„ç§é…ç½®ç»„åˆ
        config_combinations = [
            {
                'top_k': 5,
                'threshold': 0.7,
                'similarity_metric': 'cosine'
            },
            {
                'top_k': 10,
                'threshold': 0.5,
                'similarity_metric': 'euclidean'
            },
            {
                'top_k': 3,
                'threshold': 0.9,
                'similarity_metric': 'dot_product'
            }
        ]

        for config in config_combinations:
            rag_system.update_config(config)
            current_config = rag_system.get_config()

            # éªŒè¯é…ç½®æ›´æ–°
            assert current_config['top_k'] == config['top_k']
            assert current_config['threshold'] == config['threshold']
            assert current_config['similarity_metric'] == config['similarity_metric']

            # éªŒè¯é…ç½®éªŒè¯
            assert rag_system.validate_config() is True

    def test_rag_system_metrics(self, rag_system):
        """æµ‹è¯•RAGç³»ç»ŸæŒ‡æ ‡"""
        # ç”Ÿæˆä¸€äº›æ£€ç´¢æ´»åŠ¨
        metrics_data = [
            {'query': 'æŸ¥è¯¢1', 'results_count': 5, 'latency': 0.1},
            {'query': 'æŸ¥è¯¢2', 'results_count': 3, 'latency': 0.15},
            {'query': 'æŸ¥è¯¢3', 'results_count': 7, 'latency': 0.08}
        ]

        # è®°å½•æŒ‡æ ‡
        for data in metrics_data:
            rag_system.record_metrics(data)

        # è·å–ç³»ç»ŸæŒ‡æ ‡
        metrics = rag_system.get_system_metrics()

        # éªŒè¯æŒ‡æ ‡å®Œæ•´æ€§
        assert 'total_queries' in metrics
        assert 'average_latency' in metrics
        assert 'average_results_count' in metrics
        assert 'cache_hit_rate' in metrics
        assert 'error_rate' in metrics
        assert 'throughput' in metrics

        # éªŒè¯æŒ‡æ ‡è®¡ç®—
        assert metrics['total_queries'] == 3
        assert abs(metrics['average_latency'] - 0.11) < 0.01
        assert abs(metrics['average_results_count'] - 5.0) < 0.01


if __name__ == "__main__":
    pytest.main([__file__])
